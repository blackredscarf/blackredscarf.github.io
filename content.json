{"pages":[],"posts":[{"title":"Transformer双向编码器","text":"BERT，完整描述是使用双向编码器的Transformer (Bidirectional Encoder Representations from Transformers)， 是Google在2018年发布的一个神经网络模型 [1]。该模型与以往大多数NLP模型不同，它本身便是一个用于迁移训练的模型。 对于迁移训练，在计算机视觉中迁移训练已经是个常态了，一般都是在经过ImageNet数据训练的预训练模型上进行微调。如今，研究人员在处理NLP问题上也希望能够通过预训练来提高模型能力。而BERT的该论文主要阐述如何在BERT模型上进行预训练，然后基于预训练模型，对于不同任务(task)进行微调。 预训练与微调方法 早于BERT，OpenAI GPT提出了在transformer上进行预训练和微调的方法 [2]，不过与BERT不同的是，GPT采用的是单向transformer以及使用无监督数据来学习语言模型。BERT采用的方法是基于无监督数据采用有监督方法进行预训练，详细方法后面会讲到。 模型结构 bert-1.png 上图是BERT与几个相似模型的对比，只有BERT在所有层级上都加入了左右上下文的信息。 输入表示 bert-2.png 输入向量由三层组合 1. Token Embeddings 即词向量。[CLS] 表示开始标志，[SEP] 用于分割多个句子的标志 2. Segment Embeddings 段落向量。上图中的\\(\\large E_A, E_B\\) 表示两个学习过的句子A和B。为什么这么做后面会讲到。 3. Position Embeddings 位置向量。这里的位置向量不使用余弦函数，而是需要进行训练。 预训练任务 下面会介绍几个用于预训练的任务。 Masked LM BERT 最重要的特点就是双向，能够同时掌握上下文的信息。但是这里就出现了一个问题，convS2S和transformer都在致力于使用masked去清除“过去”的影响，BERT论文中提到这么做是因为在双向多层结构中词汇会在第二层以后看到“自己”，所以要清除这种干扰。 而为了使得BERT这样的双向结构能够进行良好的训练，作者提出了使用Masked LM方法 [3]，该方法就是对于输入的句子，随机遮住(mask)掉一些词汇(token)，然后让模型去预测这个词汇。明显，token被mask掉以后就不会出现自己干扰自己的情况。mask的时候使用[MASK]标记即可。 但是这么做会产生一个弊端，这个[MASK]标记在微调(fine-tuning)中是不会出现的，为了减轻该影响，作者提供了方案： 1. 80% 的情况下，替换为[MASK]，比如 my dog is hairy -&gt; my dog is [MASK] 2. 10% 的情况下，替换为随机词，比如 my dog is hairy -&gt; my dog is apple 3. 10% 的情况下，不替换 下一句预测 就是将句子A和句子B通过[SEP]拼接起来，问你句子B是不是句子A的下一句。 举两个例子： Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] Label = IsNext Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label = NotNext 微调 提取最后一层中 [CLS]标记所在隐藏状态 \\(\\large C \\in \\mathbb{R}^H\\)，以及你自己加上去的权重 \\(\\large W \\in \\mathbb{R}^{K \\times H}\\)，\\(\\large K\\) 即你的分类数。通过Softmax进行预测： \\[ \\large P = softmax(CW^T) \\] 对于超参数的调节，视不同任务而定，作者建议的参数有： Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Number of epochs: 3, 4 为了方便理解，作者提供对于不同任务的微调的图示： bert-3.png bert-4.png 实验结果 bert-5.png 从上图可以看出，BERT在各项任务上的表现基本是毫无敌手。(以上指标均为GLUE [4]) 总结 BERT的实验结果表明预训练模型的优越性，尽管BERT本身有些瑕疵，但论文本身阐述了诸多预训练的方法和理论，并且表现出强劲的实力，可以说是NLP史上的一座重要的里程碑了。 尽管BERT能够适用于诸多任务，但也有一些任务无法胜任，比如机器翻译…… 参考文献 [1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018 [2] Improving Language Understandingby Generative Pre-Training. 2018 [3] “Cloze Procedure”: A New Tool for Measuring Readability. 1953 [4] GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. 2018","link":"/post/bi-transformer/"},{"title":"博客折腾记录","text":"hexo博客的一些坑和技巧写在这里。而对于常规操作可以看官方文档。 使用pandoc作为markdown渲染引擎 默认的 hexo-renderer-marked 与 mathjax 和 katex 有很多冲突的问题，所以建议换用 hexo-renderer-pandoc。 确保你已经安装了 pandoc 。然后npm安装即可。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-pandoc --save 有些主题可能会有js代码检查依赖，把对 hexo-renderer-marked 的检查去掉即可。 使用自定义域名 在github项目的 settings 中设置 custom domain 为域名设置DNS，设置以下A记录： 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 设置CNAME记录，www.YOURDOMAIN.com 指向 xxx.github.io 在source目录下添加文件CNAME，一行一个写下 www.YOURDOMAIN.com 和 YOURDOMAIN.com 自动部署与持续集成 Github提供了永久域名xxx.github.io使得博客能被永久访问，考虑到没有必要使这个永久域名失效，所以决定把博客部署到自己服务器上，使用自己的域名去访问服务器，和Github pages服务区分开来，这样就算我自己的域名失效了，也能通过xxx.github.io域名访问挂在github上的页面。 总体方案是在服务器通过docker部署nginx和drone，每次博客仓库发生更新，drone就会自动把源码拉下来然后复制到nginx的静态目录下。","link":"/post/blog-log/"},{"title":"布隆过滤器","text":"布隆过滤器是一种空间高效的数据结构，用于判断一个元素是否位于集合中，但空间高效是有代价的。它基本实现方式是把元素计算成一个小序列存到数据结构中，多个序列有一定几率会在数据结构中重叠，所以有一定几率会判断错误，而且数据转换成序列后是不可逆的。所以，该数据结构一般用于容错高的场景，作为一个额外的数据结构来判断元素是否在集合中。 基本思路 设置一个长度为\\(w\\)的位数组\\(A\\)，只存0和1，初始都是0。当想插入\\(x\\)值，先对\\(x\\)做\\(k\\)次哈希，得到\\(k\\)个哈希值(\\({h}_1, {h}_2, {h}_3, ... {h}_k\\))，并依次与容器长度取余\\(pos={h}_1\\%w\\)，并将对应位置置为1，即\\(A_{pos}=1\\)。 判断元素是否存在时，也是类似，当想判断\\(x\\)值，对\\(x\\)做\\(k\\)次哈希，得到\\(k\\)个哈希值(\\({h}_1, {h}_2, {h}_3, ... {h}_k\\))，并依次与容器长度取余 \\(pos={h}_1\\%w\\)，若计算所得位置上皆为1，即\\(A_{pos}==1\\)，则说明该元素可能存在容器中，否则可能不存在。 公式 插入元素：一个长度为\\(w\\)的位数组\\(A\\)，当想插入\\(x\\)值时， \\(H = { f_1(x), f_2(x) ..., f_k(x) }\\)，\\(H\\)为哈希值集合，\\(f_i\\)为哈希函数 \\(P = H \\% w\\)，\\(P\\) 为取余集合 对于\\(P\\)中每个元素\\(j\\)，都有\\(A_j=1\\) 判断元素是否存在：一个长度为\\(w\\)的位数组\\(A\\)，当想判断\\(x\\)值时， \\(H = { f_1(x), f_2(x) ..., f_k(x) }\\)，\\(H\\)为哈希值集合，\\(f_i\\)为哈希函数 \\(P = H \\% w\\)，\\(P\\) 为取余集合 当\\(\\forall j \\in P, A_j==1\\)成立，则认为\\(x \\in A\\) 数学结论 当哈希函数个数为\\(k\\)，数组容量为\\(w\\)，插入数据数量为\\(n\\)，则当 \\[ k = (w/n) * \\ln 2 \\] 时，布隆过滤器获得最优的准确性。 另外，在哈希函数的个数取到最优时，要让错误率不超过\\(\\epsilon\\)，\\(w\\)至少需要是\\(n\\)的1.44倍。 实现 尝试使用Rust语言实现布隆过滤器。 用例： 12345678let mut bf = BloomFilter::new(10);let b1 = Bytes::from(&amp;b\"hello\"[..]);let b2 = Bytes::from(&amp;b\"world\"[..]);bf.add(&amp;b1);let filter = bf.generate();println!(\"&#123;&#125;\", bf.contains(&amp;filter, &amp;b1)); // trueprintln!(\"&#123;&#125;\", bf.contains(&amp;filter, &amp;b2)); // false Hash 此处的hash函数为MurmurHash，一般用于检索操作，随机分布特性表现好，广泛运用于数据库。 12345678910111213141516171819202122232425262728293031323334353637383940pub mod codec &#123; pub fn get_u32_little_end(arr: &amp;[u8]) -&gt; u32 &#123; ((arr[0] as u32) &lt;&lt; 0) + ((arr[1] as u32) &lt;&lt; 8) + ((arr[2] as u32) &lt;&lt; 16) + ((arr[3] as u32) &lt;&lt; 24) &#125;&#125;fn hash(data: &amp;Bytes, seed: u32) -&gt; u32 &#123; let m = 0xc6a4a793 as u32; let r = 24 as u32; let mut h = seed ^ (data.len() as u64 * m as u64) as u32; let n = data.len() - data.len() % 4; let mut i = 0; while i &lt; n &#123; h += codec::get_u32_little_end(&amp;data[i..]) as u32; h = (h as u64 * m as u64) as u32; h ^= (h &gt;&gt; 16); i += 4; &#125; let flag = data.len() - i; if flag == 3 &#123; h += (data.len() as u32) &lt;&lt; 16; &#125; else if flag == 2 &#123; h += (data.len() as u32) &lt;&lt; 8; &#125; else if flag == 1 &#123; h = (h as u64 + get_u32_little_end(&amp;data[i..]) as u64) as u32; h = (h as u64 * m as u64) as u32; h ^= (h &gt;&gt; r); &#125; return h&#125;fn bloom_hash(data: &amp;Bytes) -&gt; u32 &#123; hash(data, 0xbc9f1d34)&#125; BloomFilter 成员变量 bits_per_key 表示平均每个key占用的位，即数学结论中的\\(w/n\\) k 表示k个哈希函数，通过数学结论中的公式计算可得 key_hashes 用于存储key的哈希值 12345678910111213141516struct BloomFilter &#123; bits_per_key: usize, k: u8, key_hashes: Vec&lt;u32&gt;,&#125;impl BloomFilter &#123; fn new(bits_per_key: usize) -&gt; Self &#123; let mut k = (bits_per_key as f64 * 0.69) as u8; // 0.69 = ln2 if k &lt; 1 &#123; k = 1; &#125; if k &gt; 30 &#123; k = 30; &#125; BloomFilter &#123; bits_per_key, k, key_hashes: vec![] &#125; &#125; // ... 插入Key 插入Key时算一次哈希，理论上是要算k次的，但其实可以通过对一个哈希值进行位移来得到下一个哈希值，提高效率。 123fn add(&amp;mut self, key: &amp;Bytes) &#123; self.key_hashes.push(bloom_hash(key))&#125; 生成Filter 这里的写法有点特别，首先我们用字节数组替代我们认知中的位数组，比如一个长度为72的char类型数组，可以用长度为9的uint8类型数组替代。另外，可以通过对一个哈希值进行位移来得到下一个哈希值，提高效率。 123456789101112131415161718192021222324252627282930313233fn generate(&amp;mut self) -&gt; Bytes &#123; let mut n_bits = (self.key_hashes.len() * self.bits_per_key) as u32; if n_bits &lt; 64 &#123; n_bits = 64; &#125; // 向上取整 let n_bytes = (n_bits + 7) / 8; n_bits = n_bytes * 8; let mut dest = BytesMut::new(); // 初始化n_bytes大小的数组，每个元素是一个u8 // 这里用字节数组表示位数组 dest.resize(n_bytes as usize + 1, 0); // 最后一个位置存哈希值的数量 dest[n_bytes as usize] = self.k; for v in &amp;self.key_hashes &#123; let mut kh = v.clone(); // 哈希值的位移量 let delta = (kh &gt;&gt; 17) | (kh &lt;&lt; 15); for _ in 0..self.k &#123; let bitpos = (kh % n_bits) as usize; dest[bitpos/8] |= (1 &lt;&lt; (bitpos % 8)); // 通过位移来得到另一个哈希值，而不是继续用哈希函数计算 kh = (kh as u64 + delta as u64) as u32; &#125; &#125; self.key_hashes.clear(); dest.freeze()&#125; 判断是否存在 123456789101112131415161718192021222324252627fn contains(&amp;self, filter: &amp;Bytes, key: &amp;Bytes) -&gt; bool &#123; let n_bytes = filter.len() - 1; if n_bytes &lt; 1 &#123; return false &#125; let n_bits = (n_bytes * 8) as u32; // 通过最后一个位置拿到哈希值的数量 let k = filter[n_bytes]; if k &gt; 30 &#123; return true &#125; let mut kh = bloom_hash(key); let delta = (kh &gt;&gt; 17) | (kh &lt;&lt; 15); for _ in 0..k &#123; let bitpos = (kh % n_bits) as usize; // 如果filter对应位不是1就认为不存在 if filter[bitpos/8] as u32 &amp; (1 &lt;&lt; (bitpos % 8)) == 0 &#123; return false &#125; kh = (kh as u64 + delta as u64) as u32; &#125; return true&#125; 测试 测试分为两个指标，一个是集合存该元素在而过滤器判断为存在，另一个是集合不存在该元素而过滤器判断为存在。而第二个指标则称之为假阳性(False positive)。在\\(w/n\\)为10时，向过滤器插入1万个key，第一个指标是要求100%判断正确，对于第二个指标，让过滤器判断1万个不存在的key，判断错误的几率即为假阳性率。假阳性率不超过万分之二为可接受范围。 参考 Network Applications of Bloom Filters: A Survey Bloom Filter概念和原理 goleveldb/leveldb/filter/bloom.go An Improved Construction for Counting Bloom Filters","link":"/post/bloom-filter/"},{"title":"CentOS6升级glibc2.17","text":"前段时间在CentOS6集群上跑TensorFlow的时候遇到了glibc和gcc版本过低的问题。本篇先讲升级glibc。 注意，glibc是linux的核心底层库，一旦升级失败，系统基本就完蛋了。虽然这个教程我有九成把握能升级成功，但是为了避免突发情况，建议你在实际环境上升级之前，先跑一台虚拟机去模拟升级。 检测你的glibc版本 1strings /lib64/libc.so.6 | grep GLIBC 或 1ldd --version 安装必要依赖 1yum install -y wget gcc glibc kernel-devel 方法一：rpm无痛升级 可以直接用社区制作好的rpm包来轻松升级。缺点是社区维护的版本不高，目前centos6最高维护到2.17，你想装2.18的话就没有了。 下载rpm包 1234wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-2.17-55.el6.x86_64.rpmwget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-common-2.17-55.el6.x86_64.rpmwget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-devel-2.17-55.el6.x86_64.rpmwget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-headers-2.17-55.el6.x86_64.rpm ### 安装rpm包 1234rpm -Uvh glibc-2.17-55.el6.x86_64.rpm \\glibc-common-2.17-55.el6.x86_64.rpm \\glibc-devel-2.17-55.el6.x86_64.rpm \\glibc-headers-2.17-55.el6.x86_64.rpm 安装完后查看一下当前glibc的版本，如果最高到2.17的话，说明安装成功。 方法二：源码安装glibc 源码安装的缺点就是有些麻烦，要花时间下载以及编译安装。优点就是你想要的所有版本都能安装上。 下载和解压 1234cd /usr/local/srcwget https://mirrors.tuna.tsinghua.edu.cn/gnu/glibc/glibc-2.17.tar.gztar -xf glibc-2.17.tar.gz 配置编译参数 1234mkdir glibc-buildcd glibc-build../glibc-2.17/configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin 编译与安装 12makemake install 安装完后查看一下当前glibc的版本，如果最高到2.17的话，说明安装成功。 问题 你会注意到，配置安装目录的路径是/usr，这么做的目的是在安装的时候直接覆盖旧版的glibc。 网上的一些博客的教程会在配置安装目录的时候，把路径配置到/usr之外的地方，比如 1../configure --prefix=/opt/glibc-2.17 然后把旧的软连接/lib64/libc.so.6删掉，再创建新的软连接。 事实上，这种做法是有风险的，一旦你把软连接删掉，系统在那一瞬间，一些核心的命令和功能就暂时废掉了。一开始我也这么做的，当我马上再创建新的软连接时就报了各种错。为了实验可行性，还导致我重装了好几次系统。","link":"/post/centos-upgrade-glibc/"},{"title":"卷积网络的反向传播推导","text":"以前写的一篇文章，补个档。本文主要推导了一下卷积神经网络的反向传播过程。 卷积层的前向传播 在讲反向传播之前，我先来温习一下卷积层的前向传播。 \\[ \\begin{pmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix} * \\begin{pmatrix} w_{11} &amp; w_{12} \\\\ w_{21} &amp; w_{22} \\end{pmatrix} + \\begin{pmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\end{pmatrix} = \\begin{pmatrix} z_{11} &amp; z_{12} \\\\ z_{21} &amp; z_{22} \\end{pmatrix} \\] 注意，参照维基百科，这里的运算符 \\(\\large*\\) 是卷积的意思，不是乘法。 如果我们展开来写就是这样： \\[ z_{11} = x_{11}w_{11} + x_{12}w_{12} + x_{21}w_{21} + x_{22}w_{22} + b_{11} \\] \\[ z_{12} = x_{12}w_{11} + x_{13}w_{12} + x_{22}w_{21} + x_{23}w_{22} + b_{12} \\] \\[ z_{21} = x_{21}w_{11} + x_{22}w_{12} + x_{31}w_{21} + x_{32}w_{22} + b_{21} \\] \\[ z_{22} = x_{22}w_{11} + x_{23}w_{12} + x_{32}w_{21} + x_{33}w_{22} + b_{22} \\] 卷积层的反向传播 以该图两个层卷积层为例 cnn-bp.png 设损失函数为 \\(J\\) ，第 \\(l\\) 层的 \\(a\\) 的梯度值就是 \\[ da^l = \\frac{d J}{d a^{l}} \\] 根据链式法则，\\(z^l\\) 的梯度值就为 \\[ dz^l = \\frac{\\partial J}{\\partial z^{l}} = \\frac{d J}{d a^l} \\, \\frac{da^l}{dz^l} = da^l \\, g&#39;(z^l) \\] 已经知道 \\(z^l\\) 后，我们就要去计算梯度值 \\(dx^l\\), 即上一层的 $da^{l-1} $。但也许你会注意到，上面的卷积操作是一个矩阵操作，但是矩阵要怎么求导呀？这里，我们先不考虑矩阵的求导，我们先尝试对展开式去求导，为了简洁，我们省略参数的 \\(l\\)。 由上面我们提到的 \\[ z_{11} = x_{11}w_{11} + x_{12}w_{12} + x_{21}w_{21} + x_{22}w_{22} + b_{11} \\] 以 \\(x_{11}\\) 为自变量求导得 \\[ dx_{11} = \\frac{\\partial J}{\\partial z_{11}} \\, \\frac{dz_{11}}{dx_{11}} + \\frac{\\partial J}{\\partial z_{12}} \\, \\frac{dz_{12}}{dx_{11}} + \\frac{\\partial J}{\\partial z_{21}} \\, \\frac{dz_{21}}{dx_{11}} + \\frac{\\partial J}{\\partial z_{22}} \\, \\frac{dz_{22}}{dx_{11}} \\] 即等于 \\[ dx_{11} = dz_{11}w_{11} + 0 + 0 + 0 \\] 对于 \\(dx_{12}\\) ，那么同理 \\[ dx_{12} = \\frac{\\partial J}{\\partial z_{11}} \\, \\frac{dz_{11}}{dx_{12}} + \\frac{\\partial J}{\\partial z_{12}} \\, \\frac{dz_{12}}{dx_{12}} + \\frac{\\partial J}{\\partial z_{21}} \\, \\frac{dz_{21}}{dx_{12}} + \\frac{\\partial J}{\\partial z_{22}} \\, \\frac{dz_{22}}{dx_{12}} \\] \\[ dx_{12} = dz_{11}w_{11} + dz_{12}w_{11} + 0 + 0 \\] 那么其他 \\(x_{ij}\\) 同理。都写出来后，就会发现，我们可以写成一个矩阵的卷积操作： \\[ \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; dz_{11} &amp; dz_{12} &amp; 0 \\\\ 0 &amp; dz_{21} &amp; dz_{22} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} * \\begin{pmatrix} w_{22} &amp; w_{21} \\\\ w_{12} &amp; w_{11} \\end{pmatrix} = \\begin{pmatrix} dx_{11} &amp; dx_{12} &amp; dx_{13} \\\\ dx_{21} &amp; dx_{22} &amp; dx_{23} \\\\ dx_{31} &amp; dx_{32} &amp; dx_{33} \\end{pmatrix} \\] 这样的话，我们就可以总结出公式： \\[ dx^{l} = da^{l-1} = pad(dz^{l}) * rot180(W^l) \\] \\(pad(dz^{l})\\) 表示将 \\(dz^l\\) 填充 \\(0\\) 来扩大到合适的维度，\\(rot180(W^l)\\) 表示将卷积核 \\(W^l\\) 旋转180度。 那么当我们计算\\(dw\\)的时候，也可以得出一个卷积公式： \\[ \\begin{pmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix} * \\begin{pmatrix} dz_{11} &amp; dz_{12} \\\\ dz_{21} &amp; dz_{22} \\end{pmatrix} = \\begin{pmatrix} dw_{11} &amp; dw_{12} \\\\ dw_{21} &amp; dw_{22} \\end{pmatrix} \\] \\[ dw^{l} = \\frac{dz^l}{dw^l} = x^l * dz^{l} \\] 而偏置项的梯度值 \\(db\\) 为 \\[ db^l = \\frac{dz^l}{db^l} = dz^{l} \\] 池化层的前向传播 温习一下池化层的前向传播时是怎么算的。 如果是平均池化（AvgPooling）, 假设池化层大小是一个 \\(2×2\\)，即每次的池化范围是\\(m = 4\\)，那么每一个池化区域可得 \\[ avg(x) = \\frac{1}{m} \\sum_{k=1}^{m}x_k \\] 如果是最大池化（MaxPooling），假设池化层大小是一个 \\(2×2\\)，即每次的池化范围是\\(m = 4\\)，那么每一个池化区域可得 \\[ max(x) = max(x_1, x_2, ..., x_m) \\] 池化层的反向传播 池化层和激活函数一样，相当于卷积层中的一个子层，没有学习参数，他们会将反向传播得到的梯度值向后传递。 cnn-pool.png 比如说，一个 \\(2×2\\) 池化层, 即每一个池化区域大小为\\(m = 4\\)，从上一层反向传递过来的是这样一个矩阵 \\[ dout = \\begin{pmatrix} dx_1 &amp; dx_2 \\\\ dx_3 &amp; dx_4 \\end{pmatrix} \\] 当计算梯度值时，需要先将参数还原到输入时的维度 \\[ dout = \\begin{pmatrix} dx_1 &amp; dx_1 &amp; dx_2 &amp; dx_2 \\\\ dx_1 &amp; dx_1 &amp; dx_2 &amp; dx_2 \\\\ dx_3 &amp; dx_3 &amp; dx_4 &amp; dx_4 \\\\ dx_3 &amp; dx_3 &amp; dx_4 &amp; dx_4 \\end{pmatrix} \\] 如果是AvgPooling，我们定义\\(dout\\) 中每一个池化区域的梯度值为 \\(davg(x)\\) ，该区域中每一个值的导数为 \\[ dx_{k} = \\frac{d}{dx_{k}}avg(x) = \\frac{1}{m} \\] 那么整个\\(x\\) 矩阵往后传递的梯度值为 \\[ dx = da^l = \\frac{dout}{dx} = \\begin{pmatrix} \\frac{1}{m} &amp; \\frac{1}{m} &amp; \\frac{1}{m} &amp; \\frac{1}{m} \\\\ \\frac{1}{m} &amp; \\frac{1}{m} &amp; \\frac{1}{m} &amp; \\frac{1}{m} \\\\ \\frac{1}{m} &amp; \\frac{1}{m} &amp; \\frac{1}{m} &amp; \\frac{1}{m} \\\\ \\frac{1}{m} &amp; \\frac{1}{m} &amp; \\frac{1}{m} &amp; \\frac{1}{m} \\end{pmatrix} \\; dout \\] 如果是MaxPooling，我们定义\\(dout\\) 中每一个池化区域的梯度值为 \\(dmax(x)\\) ，该区域中每一个值的导数为 \\[ dx_{k} = \\frac{d}{dx_{k}}max(x) = \\begin{cases} 1, &amp; \\text{if $x_k$ = max(x)} \\\\[2ex] 0, &amp; \\text{otherwise} \\end{cases} \\] 那么整个\\(x\\) 矩阵往后传递的梯度值为 \\[ dx = da^l = \\frac{dout}{dx} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\end{pmatrix} \\; dout \\]","link":"/post/conv-bp/"},{"title":"CentOS6升级gcc4.9","text":"前段时间在CentOS6集群上跑TensorFlow的时候遇到了glibc和gcc版本过低的问题。本篇讲升级gcc。 问题 如果你的gcc版本过低，在跑一些比较新的软件的时候，可能会报这样的错误： 1libstdc++.so.6: version CXXABI_1.3.7’ not found 那么你就需要更新gcc了，因为CXXABI_x是包含在gcc的libstdc++中的。 查看你的gcc版本 1gcc -v 查看libstdc++版本 1find / -name &quot;libstdc++.so.*&quot; 安装必要依赖包 1yum install gcc gcc-c++ automake autoconf libtool make 下载和解压 12wget ftp://gcc.gnu.org/pub/gcc/releases/gcc-4.9.2/gcc-4.9.2.tar.bz2tar xf gcc-4.9.2.tar.bz2 如果直接从官网下载的话，因为国内网络问题，会慢到怀疑人生。这里提供一个网盘地址。 链接：https://pan.baidu.com/s/1X3cv2eUBh1ViUWOeUQtrCg 密码：ya2y 下载内置依赖 12cd gcc-4.9.2./contrib/download_prerequisites 同样的，下载速度依然龟速。依赖文件包含在上面的网盘地址里了，你可以一并下载下来，再上传到服务器。 打开download_prerequisites这个脚本，发现里面的工作就是下载了几个依赖包到gcc目录外，然后解压，并构建软连接到当前gcc目录。 既然如此，我们就照猫画虎的执行就好了。 12345678tar xf ../mpfr-2.4.2.tar.bz2ln -sf mpfr-2.4.2 mpfrtar xf ../gmp-4.3.2.tar.bz2ln -sf gmp-4.3.2 gmptar xf ../mpc-0.8.1.tar.gzln -sf mpc-0.8.1 mpc 编译安装 1./configure --prefix=/usr --enable-languages=c,c++ --enable--long-long --enable-threads=posix --disable-checking --disable-multilib -j 4 表示开4个线程去编译，一般编译时间大概是20分钟左右。请确保你的系统剩余空间大于6G。如果期间出了错误，一般是缺少依赖和必要库，把依赖安装完后再次执行即可。 12make -j4make install 安装完后，去检查版本是否为4.9.2。 发现 12345678910111213141516Libraries have been installed in: /usr/lib/../lib64If you ever happen to want to link against installed librariesin a given directory, LIBDIR, you must either use libtool, andspecify the full pathname of the library, or use the `-LLIBDIR&apos;flag during linking and do at least one of the following: - add LIBDIR to the `LD_LIBRARY_PATH&apos; environment variable during execution - add LIBDIR to the `LD_RUN_PATH&apos; environment variable during linking - use the `-Wl,-rpath -Wl,LIBDIR&apos; linker flag - have your system administrator add LIBDIR to `/etc/ld.so.conf&apos;See any operating system documentation about shared libraries formore information, such as the ld(1) and ld.so(8) manual pages. 安装完后，会打印这样一段话。这段话的大概意思是linux不会自动去你自己指定的安装目录寻找gcc，你需要在环境变量中设置gcc的安装目录，比如： 1LD_LIBRARY_PATH=/your_gcc_path/ 但是，我们不需要这么干，因为我们安装的时候，路径配置到了gcc的缺省路径上了。","link":"/post/centos-upgrade-gcc/"},{"title":"双深度Q网络","text":"本文主要讲一下 DQN 和 DDQN，以及它们的实验对比。 深度Q网络 (DQN) 回顾一下深度Q网络的过程。 从经验池 \\(\\large R\\) 中获取 \\(\\large n\\) 个转移，\\(\\large (s_i, a_i, r_{i+1}, t_i, s_{i+1})\\)，其中 \\(\\large t_i=1\\) 表示 \\(\\large s_i\\) 是结束状态 计算状态 \\(\\large s_i\\) 的预测Q值，\\(\\large y_{pred}^i = Q(s_i)\\)，执行动作 \\(\\large a_i\\) 时的值为 \\(\\large y_{pred}^{a_i}\\) 计算状态 \\(\\large s_{i+1}\\) 的目标Q值，\\(y_{target}^i=Q&#39;(s_i)\\)，取最优动作对应的值，即最大Q值 \\(\\large y_{target}^{max}\\) 计算预期Q值 \\(\\large y_{expected}^i = r_{i+1} + (1-t_i) * \\gamma * y_{target}^{max}\\) \\(\\large n\\) 个转移的均方差损失函数 \\(\\large loss=\\frac{1}{n}\\sum_i (y_{pred}^{a_i} - y_{expected}^i)\\) 使用梯度下降最小化损失函数，更新 \\(\\large Q\\) 网络的梯度值，注意这里不对目标网络 \\(\\large Q&#39;\\) 更新 相隔一定步数后，更新目标网络的参数 \\(\\large Q&#39; = Q\\) 其中，最重要的部分是对于期望Q值的计算，我们把这部分提取出来， \\[ Y_t^{DQN}=R_{t+1} + \\gamma \\max_{a} Q&#39;(S_{t+1}, a; w^-) \\] 使用经验池进行经验重新（experience replay）早在1992年就已经提出，而使用目标网络是在2015年由Mnih等人提出。 双深度Q网络（Double DQN） 不同于DQN，我们的期望Q值的计算为， \\[ Y_t^{DDQN} = R_{t+1} + \\gamma \\max_{a} Q&#39;(S_{t+1}, \\mathop{argmax}\\limits_a Q(S_{t+1}, a; w); w^-) \\] 我们的目标网络输入的不再是指定的 \\(\\large a\\)，而是原本的Q网络的最大Q值对应的动作。 深度Q网络的过度估计 论文中提出，DQN存在普遍的过度估计（overestimations）问题。 假设预测Q值为 \\(\\large Q_t(s, a)\\)，最优价值为 \\(\\large V_*(s)\\) ，每个状态有 \\(\\large m\\) 个动作，则平均误差为 \\[ \\frac{1}{m}\\sum_a(Q_t(s, a) - V_*(s))^2 = C \\] 从误差公式上看，环境的噪声，近似函数等因素都是Q函数参数学习的一部分，意味着任何这些因素都可能导致估值偏差。 对于最大值的估计， \\[ \\max_aQ_t(s,a) - V_*(s) \\ge \\sqrt{\\frac{C}{m+1}} \\] 随着 \\(\\large m\\) 的增大，下界越来越小，而实验结果的表现是随着 \\(\\large m\\) 的增大，估计误差会越来越大，即 \\(\\large Q_t(s,a)\\) 越来越大，产生了所谓的过分乐观（Overoptimism）。 过度乐观的实验 论文中选取的真值函数和近似函数分别为： True Value \\(\\large Q_*(s,a)\\) Approx Function \\(\\large Q_t(s,a)\\) \\(\\large sin(s)\\) \\(\\large W^TX\\) (d=6) \\(\\large 2exp(-s^2)\\) \\(\\large W^TX\\) (d=6) \\(\\large 2exp(-s^2)\\) \\(\\large W^TX\\) (d=9) 测试结果如下，其中每一行是一种真值函数， 第一行第一幅图，描绘了其中一个动作，紫色是真值函数，绿色是拟合函数，可以看出6维的线性回归可以很好的拟合真值。 第一行第二幅图，描绘了10个动作的拟合函数随着的Q值曲线，用黑色描绘的是对应的最大Q值，你会发现最大值普遍大于真值。 第一行第三幅图，描绘了最大Q值与真值的误差，其中橙色曲线是DQN，蓝色曲线是DDQN。明显的，DQN的平均误差要远大于DDQN。 第三行第三幅图，你会发现9维的线性回归的拟合效果远不如6维（第二行第三幅图）。所以说明，有时候复杂的近似函数不一定取得好的效果。 实际效果 在atari游戏上运行对比。 红色是DQN，蓝色是DDQN，其中横线表示无偏估计值。会发现DQN的估计值普遍大于DDQN，而且DQN的估计值偏离很严重。 从上图发现，使用DQN的估计值（浅色区域）非常的不稳地，所以过高的估计值会影响学习效率。 总结 注意一个问题，上面我们一直都在讨论估计值的问题，DDQN能够降低估计值，使得他更接近真值。在实际训练中得到的好处就是训练过程不会太过动荡，也就提高了训练的效率。强化学习与深度学习不同，深度学习会设法得到一个高的准确度。而强化学习中无论是DQN还是DDQN，一直训练下去都能收敛到一个值上，算法的区别更多体现在训练的过程和效率上。 代码实现 我用pytorch分别实现了DQN，DDQN以及用DDQN算法运行atari游戏。详见Github。 测试结果 CartPole-v0 针对CartPole-v0问题，小车上面放着一个杆子，我们的任务是左右移动车子尽量使得杆子在车子上屹立得更久，每一步动作奖励为1，屹立时间越长得到的奖励越多，默认200步结束一个episode，gym认为一个最近100个episode的平均奖励大于195就认为解决。 我分别是用DQN和DDQN训练它，结果如下（蓝色是DQN，橙色是DDQN） PongNoFrameskip-v4 Atari Pong 游戏就是两方在玩类似于乒乓球的游戏，当对方不能打回来，你就得分。目前，这是未解决问题，即没有一个奖励区间能说明解决该问题。 我用DDQN训练2百万帧，情况如下： 训练该环境时遇到了一些问题，一开始发现奖励曲线一直在-20左右徘徊，没有增长的趋势。后来我仔细对比了一下参数，发现Adam的学习率应该使用0.0001，而不是0.001。这也证明了调参的重要性，一个小数点就可能导致模型出现严重的偏差。 参考文献 Deep Reinforcement Learning with Double Q-learning Playing Atari with Deep Reinforcement Learning","link":"/post/ddqn/"},{"title":"卷积序列模型","text":"使用长短时期记忆（LSTM）用于序列模型取得了很好的效果，在2017年由Facebook提出了使用卷积神经网络构建Seq2Seq模型 [1]。循环神经网络通过窗口移动方式输入数据进行训练，当句子有 \\(\\large n\\) 个窗口时，获得对应的特征表示的时间复杂度为 \\(\\large \\mathcal{O}(n)\\)。而使用卷积神经网络进行并行化计算，当卷积核宽度为 \\(\\large k\\) ，其时间复杂度为 \\(\\large \\mathcal{O}(\\frac{n}{k})\\)。 Encoder结构 我们采用一维卷积对序列进行处理，假设输入序列长度为 \\(\\large T\\)，卷积核宽度为 \\(\\large f\\)，边缘为 \\(\\large p\\)，则输出宽度为 \\(\\large T + 2p - f + 1\\)。 源句子的最大序列长度为 \\(\\large T\\), 目标句子的最大序列长度为 \\(\\large T&#39;\\)，批次大小为 \\(\\large B\\)，词向量大小为 \\(\\large E\\)， 源语言词汇数量为 \\(\\large V\\)，目标语言词汇数量为 \\(\\large V&#39;\\)，第一层卷积的通道数为 \\(\\large C\\)。 整个句子的词向量为 \\(\\large X \\leftarrow emb \\in \\mathbb{R}^{T \\times B \\times E}\\) 输入卷积层之前，要先通过一个线性层将维度规范到 $$\\(\\large X \\in \\mathbb{R}^{T \\times B \\times C}\\)，同时对维度做一个转置 \\(\\large X \\in \\mathbb{R}^{B \\times C \\times T}\\) 卷积块带有残差结构 (residual)，所以计算下一个卷积块之前保留一份残差 \\(\\large R \\leftarrow X\\) 在卷积计算时，我们不希望边缘向量产生影响，所以构造一个遮罩矩阵，将 \\(\\large \\langle pad \\rangle\\) 位置值设为0， \\[ mask \\in \\mathbb{R}^{T \\times B} \\begin{cases} 0 &amp; if \\; x = pad \\\\ 1 &amp; otherwise \\end{cases} \\] 转置和扩充维度 \\(\\large mask \\in \\mathbb{R}^{B \\times C \\times T}\\)，把边缘值的通道设为0，\\(\\large X \\leftarrow X \\circ mask\\) 然后把 \\(\\large X\\) 输入到一维卷积中 \\(\\large X \\overset{conv}{\\rightarrow} X \\in \\mathbb{R}^{B \\times T \\times 2C}\\)，这里指定输出通道是输入通道的2倍，而采用Same Convolution，所以序列长度不变 我们使用门控线性单元（Gated Linear Units）对卷积块的输出进行计算 [2]，输出后通道数减半， \\[ \\begin{align} &amp; X \\overset{split}{\\rightarrow}A,B \\in \\mathbb{R}^{B \\times C \\times T} \\\\ &amp; X = A \\circ \\sigma(B) \\end{align} \\] 加上残差，\\(\\large X \\leftarrow X + R\\)；更新残差 \\(\\large R \\leftarrow X\\) 把 \\(\\large X\\) 输入到下一卷积块，即下一层 从最后一个卷积块输出后，再经过线性层得到 \\(\\large O \\in \\mathbb{R}^{B \\times T \\times E}\\) 除了 \\(\\large O\\) 外，还要输出一个向量用于注意力计算 \\(\\large U \\leftarrow O + emb^T\\) 卷积块的计算过程如图所示：（图是我自己画的，如有错误请指出） Decoder结构 Decdoer前面的结构基本与encoder一致，不同处有三个， decoder的卷积层边缘 padding 设置为 kernel_size - 1。当输出时，我们去掉序列最后的 padding 个，以保持输入输出序列相同。这么做是为了确保当前信息不会受未来信息的影响 上述 (5) 计算完GLU后，还要输入到注意力层进行计算，注意力层输出后接上 (6) 上述 (8) 后再经过一个线性层得到对所有词汇的得分矩阵 \\(\\large X \\in \\mathbb{R}^{T&#39; \\times B \\times V&#39;}\\) 注意力层结构 注意力层接收decoder的隐藏状态，目标序列词向量，encoder的输出。 保留一个残差 \\(\\large R \\leftarrow X\\) ，结合decoder的隐藏状态和目标序列词向量，\\(\\large X \\leftarrow W^TX + emb_t ,\\; X \\in \\mathbb{R}^{B \\times T&#39; \\times E}\\) 再与encoder的输出计算注意力得分矩阵 \\(\\large A \\leftarrow X * O,\\; A \\in \\mathbb{R}^{B \\times T&#39; \\times T}\\) 我们不需要 \\(\\large \\langle pad \\rangle\\) 的产生注意，所以把 \\(\\large \\langle pad \\rangle\\) 位置的得分都设置为负无穷 通过softmax对 \\(\\large A\\) 计算注意力分布，得到对齐矩阵 \\(\\large A \\leftarrow softmax(A)\\) 论文中提到，对于注意力的输出，我们还需要计算一个conditional vector，\\(\\large Co \\leftarrow A * U,\\;Co \\in \\mathbb{R}^{B \\times T&#39; \\times E}\\) 最后加上残差后输出 \\(\\large X \\leftarrow W^TX + R,\\; X \\in \\mathbb{R}^{B \\times C \\times T&#39;}\\) 不同于RNN中所有时间步共享一个注意力层，这里的每一个卷积层后面都是一个独立的注意力层，当你有10层卷积层，那么就有10个独立的注意力层。 位置向量（Position Embeddings ） 为了让卷积网络在处理序列时有一种空间感，我们要对词向量加上一个位置向量 \\(\\large emb \\leftarrow emb + pos\\_emb\\)。其中 \\(\\large pos\\_emb\\) 表示对于词汇在该句子的索引编号。 初始化策略 为了抑制加上残差导致数值持续变大而导致高方差，所以每次加上残差或者加上词向量后，都乘以 \\(\\large \\sqrt{0.5}\\) 。 在获得词向量后和输入卷积块之前进行 \\(\\large p=0.1\\) 的dropout正则化 对于卷积块的参数初始化，我们指定其正态分布 \\(\\large \\mathcal{N}(0, \\sqrt{4p/C})\\)，其中乘以 \\(\\large p\\) 是为了抵消dropout时乘以的 \\(\\large 1/p\\)。 线性层初始化参数符合 \\(\\large \\mathcal{N}(0, \\sqrt{(1-p)/N})\\) 词向量和位置向量在 \\(\\large [0, 0.1]\\) 之间均匀分布，并且 \\(\\large \\langle pad \\rangle\\) 对应的词向量设为0 生成（Generation） 在训练时，我们可以一次性把整个目标句子输入到CNN中并行计算，不用像RNN中一步一步的输入，理论上的训练速度会有所提升，实际跑起来后会因为其中大量的注意力层会把训练速度拖慢。 在decoder生成预测序列时，我们需要以递进式的输入到CNN，需要输入 \\(\\large \\frac{T(T-1)}{2}\\) 次，而RNN逐个输入也就 \\(\\large T\\) 次，所以生成速度上，CNN明显要慢。论文中提出的解决方案是把前面序列的卷积参数保留下来，不用重复计算，但并没有详细讲要怎么做，实现起来貌似难度挺大的。 参考文献 [1] Convolutional Sequence to Sequence Learning. 2017 [2] Language Modeling with Gated Convolutional Networks. 2016","link":"/post/cnn-seq/"},{"title":"《爱因斯坦传》读后随记","text":"由艾萨克森撰写的《爱因斯坦传》脱胎于爱因斯坦文稿计划，取材于爱因斯坦及其家族生前的文稿信件，是目前最权威真实的爱因斯坦传记。他是20世纪最伟大的科学家之一，推翻牛顿力学的变革者，直面量子论的坚守者，原子弹的提倡者和扼制者。宇宙万物，小到尘埃，大至恒星皆运转在他的理论之下。这位伟人的传奇一生宛若画卷一般徐徐展开，不知觉中看得热泪盈眶。 学习和学业 很多谬论说爱因斯坦学习成绩不好，实则不然，爱因斯坦的成绩非常优秀，入学考试是以9位应试学生中的第一名考入苏黎世联邦工学院的。不过吐槽一句，那个年代考大学的人真的很少啊。 爱因斯坦早期只对物理感兴趣，而对数学不感兴趣，因为他觉得数学太复杂，他喜欢优雅和简单。所以大学时期，他不太愿意去学数学，被数学老师称为“懒狗”。物理课上的爱因斯坦也很特立独行，完全不按老师的要求做，而是按自己的想法做，能够独立思考总不是坏事，何况爱因斯坦有与之相匹配的天赋。因为这种特立独行得罪了老师，爱因斯坦大学毕业成绩被打了低分，全班倒数。 爱因斯坦在找工作这件事上有点惨，大学毕业后很长一段时间都没能找到全职工作，大部分时间都是在兼职当家庭教师。爱因斯坦希望找到教职相关工作，但屡屡碰壁，事实上这跟他那跋扈的性格有点关系。他在写求职信的时候总是不忘评论或批评一下教授的研究成果，以此希望能招揽他，他会自信满满地说“他们一定会被我吓一跳”，但真实情况可能是，教授看到求职信后会生气得把信撕掉。这相当有意思，很多西方人从不吝啬得罪人的话语，这与东方的“以和为贵”相差甚远。爱伊斯坦最后还是靠着好友贝索托关系才找到了瑞士伯尔尼专利局的工作。 音乐 爱因斯坦喜欢拉小提琴，每当遇到难题想不清楚时，爱因斯坦就会拿起小提琴，短暂地逃避到音乐世界里去。他喜欢莫扎特而不喜欢贝多芬，他认为他们的音乐理念比较相近，莫扎特的音乐有种优雅与和谐，就好像本来就存在于宇宙中，这与爱因斯坦本人的物理学理念相通。 音乐也成为了爱因斯坦交友的方式，爱因斯坦一家时常会参加由其他家庭举办的音乐沙龙。说起来，西方家庭比较喜欢和邻居，朋友或同事等举行聚会，以此促进交流，这种文化可能在比较放松和慢节奏的环境下才能诞生。我隐约记得我小时候家里经常会与其他家庭聚会，但后来基本就没了，如今经济变好了，网络发达了，但人与人之间的隔阂却变深了。 种族与信仰 爱因斯坦小时候很愿意接受犹太教的教规，但在阅读科学书籍之后，就开始对宗教表示出厌恶，他向往自由而抵触管制，可以说，他的一生都在远离宗教仪式，他仅信任自然界上帝，而不是宗教上帝，他反对迷信权威。也许正是这种精神，让他敢于质疑和推翻权威理论，为物理学带来革命。 虽然爱因斯坦抵触犹太教规，反对犹太复国主义（以色列），因为当时他觉得犹太人没有必要抱团取暖，但他为建一所犹太大学，即希伯来大学，提供了许多帮助和支持。后来反犹主义日渐高涨时，他开始重新审视自己的犹太血统，他认为犹太人应该正视自己，这与信仰无关，重来都没有“犹太信仰”这种东西，而是一个民族的尊严和精神，所以他开始四处奔走为犹太复国筹集资金和声援。 因为爱因斯坦为犹太复国提供了巨大帮助，所以在他晚年时，以色列甚至想请他去当总统。不过要知道，以色列的总统就像英国的皇室，更多是象征意义，总理才是负责政务。 婚姻 爱因斯坦有过两段婚姻。第一个妻子是米列娃，米列娃学习理科，想从事科学事业，这与爱因斯坦对上了电波，甚至让他抛弃初恋女友和她结婚。但可惜米列娃没有太多的才能，成绩较差，而且考了两次都没考上博士，科学事业之路断了。事实上她也没能很好的与爱因斯坦相处，她嫉妒爱因斯坦的荣誉以及和他来往密切的同事，连爱因斯坦都在信件里吐槽她的嫉妒心，两人间隙越来越大，最后不得不离婚。一件惊人的事是，爱因斯坦承诺把诺贝尔奖的奖金全部交给米列娃，可以看出爱因斯坦还是很关心前妻和孩子们的。 第二段婚姻是爱尔莎。爱尔莎是个爱慕荣誉的人，但她不会嫉妒爱因斯坦的荣誉，而是乐于和他共享荣誉，她能得体的面对记者的询问和摄像头闪光灯。还有她非常愿意照顾爱因斯坦的生活各方面，使他能心无旁骛地投身到学术中。如果要对比，毫无疑问爱尔莎更适合成为爱因斯坦的妻子。 哲学理念 爱因斯坦追求一种物理实在论，他认为所有物理现象都是可以被确定的，即“上帝不掷骰子”。这与哥本哈根派形成了对立，哥本哈根派在认为量子具有不确定性，而爱因斯坦强烈反对这一理论，他认为量子的不确定性是因为不完备导致的，爱因斯坦希望把量子运动统一到他的引力场理论之中，即寻求一种统一场论，基于这一信念使得他在这之上耗费了半生。哥本哈根学派的领导人波尔指责爱因斯坦变得和当初那些反对相对论的人一样顽固。但我认为这有本质区别，爱因斯坦并非顽固守旧，恰恰相反，他一直是革新的坚守者，他的一生坚守着反潮流的理念，当大家都认为牛顿定理很完美的时候，他创造了相对论；当大家都接受了不确定性理论后，他则设法寻求确定性；当大家都认为应该大力发展核武器时，他则坚称核武器应当被扼制。 虽然爱因斯坦至死未能找到统一场论，量子不确定性就像一个魔咒一般，使得物理学就像被禁锢了一般无法前进，虽然爱因斯坦没能成功，但他的信念和精神鼓舞了之后的很多科学家，之后的数代科学家们前仆后继，相信总有一天能找到那个“万能理论”。 战争与和平 爱因斯坦能理解宇宙的奥妙，却不一定能理解复杂的人心。爱因斯坦是一个和平主义者和世界公民，他一直反对通过战争来解决问题，但当希特勒上台，德国实行军国主义，犹太人被赶出德国时，爱因斯坦发现通过谈判获得和平已是虚妄，必须有与之等同的力量才能守护和平，所以他写信给多国领导人需要警惕德国，甚至写信给当时的美国总统罗斯福，提倡研发原子弹，如果德国先研发出来，那对世界将是一个灾难。 但德国并未能做出原子弹，而当原子弹被投向日本后，爱因斯坦不由后悔，如果他知道德国无法做出原子弹，他誓死不会提倡美国研制原子弹。战后，他成为了“原子能科学家应急委员会”的主席，致力于呼吁各国管制和控制核武器。然而当时正是冷战时期，各国都在潜心研究核武器，不可能停止，所以很多人称爱因斯坦的想法为“幼稚”。事实上，爱因斯坦作为世界公民，自然不会站在某个国家的政治立场上思考问题，他关心的是全人类的命运。在那之后，他“幼稚”的想法启发了很多普通人和科学家的反思和共鸣，爱因斯坦签署的《罗素—爱因斯坦宣言》引发了科学家国际和平运动——帕格沃什运动。","link":"/post/einstein/"},{"title":"深度确定性策略梯度","text":"虽然DQN的表现很好，但它有一个致命的缺点是无法学习连续动作空间。回想一下，DQN 是根据greddy策略找到最大Q值对应的行为，即 \\(\\large a = \\mathop{argmax}\\limits_{a \\in \\mathcal{A}} Q(s)\\)，也就说每一个行为对应一个Q值，当动作是高维度的或者是连续的，那么DQN的计算代价就很大，甚至无法进行学习。这时使用策略梯度就能很好解决这个问题。我们把环境动作设计成连续的，用动作特征表示一个动作，使用策略梯度，我们不需要计算每一个动作的Q值，而是可以通过参数化近似的办法直接输出一个动作特征，而不是从某个值映射到某个动作。DDPG (Deep Deterministic Policy Gradient)算法很好的弥补了DQN的缺陷。当然，并不是DDPG只能解决连续动作问题，只要把动作特征映射到某一个动作上就能应对非连续动作空间问题了。 算法 DDPG使用演员-评论家模型（actor-critic），设计了4个参数化函数。其中actor包含策略函数 \\(\\large \\mu\\)，目标策略函数 \\(\\large \\mu&#39;\\)，而critic包含Q函数 \\(\\large Q\\)，目标Q函数 \\(\\large Q&#39;\\)。 我们的目标是把 \\(\\large \\mu\\) 参数训练好，因为届时我们获取动作特征的方法是， \\[ a = \\mu(s) \\] 在训练过程中，critic的 \\(\\large Q\\) 的作用是对actor策略 \\(\\large \\mu\\) 进行评估， \\[ \\begin{aligned} a &amp;= \\mu(s) \\\\ q &amp;= Q(s, a) \\end{aligned} \\] 通过 \\(\\large Q(s, a)\\) 可以一个评价当前actor在状态 \\(\\large s\\) 执行动作 \\(\\large a\\) 的分数。 上面提到的参数中，目标参数 \\(\\large \\mu&#39;\\) 和 \\(\\large Q&#39;\\) 作为监督者，以此来计算 \\(\\large \\mu\\) 和 \\(\\large Q\\) 函数的参数梯度。 每一个episode，我们先从经验池 \\(\\large R\\) 中抽样 \\(\\large N\\) 个转移， \\[ (s_i, a_i, r_i, s_{i+1})_i^N \\in R \\] 对于 \\(\\large Q\\) 的损失函数，我们定义为： \\[ \\begin{aligned} &amp; a_{i+1} = \\mu&#39;(s_{i+1}) \\\\ &amp; y_i = r_i + \\gamma Q&#39;(s_{i+1}, a_{i+1}) \\\\ &amp; J = \\frac{1}{N} \\sum_i (y_i-Q(s_i,a))^2 \\end{aligned} \\] 对于 \\(\\large \\mu\\) 的损函数，论文中直接给的是梯度公式。而我参考了其他人的实现，发现他们定义了一个相对简单的损失函数。 \\[ \\begin{aligned} &amp; a_i = \\mu(s_i) \\\\ &amp; J = -\\frac{1}{N} \\sum_i Q(s_i,a_i) \\end{aligned} \\] 大概理解就是，我们设法把 \\(\\large Q(s_i,a_i)\\) 最大化，即通过梯度下降，最小化其负数。 更新完梯度后，需要进行参数软更新（soft-update）： \\[ \\begin{aligned} w^{Q&#39;} &amp;= \\tau \\; w^{Q} + (1-\\tau)w^{Q&#39;} \\\\ w^{\\mu&#39;} &amp;= \\tau \\; w^{\\mu} + (1-\\tau)w^{\\mu&#39;} \\\\ \\tau &amp;\\ll 1 \\end{aligned} \\] 每次更新target参数时，我们不会完全更新，而是保留多一些过往的记忆。 论文中提到，为了让的动作更加丰富，使个体勇于探索，我们要在动作特征上加入噪音。 \\[ \\mu(s) = \\mu(s) + \\epsilon \\; \\mathcal{N} \\] 其中 \\(\\large \\mathcal{N}\\) 使用Ornstein-Uhlenbeck process算法生成噪音。其中 \\(\\large \\epsilon\\) 是一个动态的衰减系数，论文中没有提到，实际使用时如果不加可能导致训练过程非常震荡。 实验细节 论文后面附加部分提供了算法参数的一些细节。 梯度更新使用Adam，actor和critic的学习率分别为1e-4和1e-3。 \\(\\gamma\\) 为0.99 \\(\\large \\tau\\) 为0.001 参数化近似使用神经网络，actor和critic使用三层神经网络，分别是400，300和300个神经元。actor的输出层使用tanh。critic第一层输入状态特征，第一层的输出与动作特征一起输入第二层。 参数初始化，前两层使用fanin_init初始化，最后一层使用区间为[-3e-3, 3e-3]的归一化分布。 Ornstein-Uhlenbeck process的参数中，\\(\\large \\theta\\) 为0.15，\\(\\large \\sigma\\) 为0.2 还有一个就是噪音衰减的问题，这个问题论文中没有提到，但我自己测试时去发现了这个问题不容忽视。\\(\\large \\epsilon\\) 初始值为1，每完成一个episode就减去 \\(\\large d\\)，这个 \\(\\large d\\) 的很难选，假设是0.01，那么100个episode后 \\(\\large \\epsilon\\) 就衰减到0，之后就无法加入噪音了。如果你的模型训练难度较大，要训练几百上千个episode，你必须确保能够持续加入噪音，同时兼顾到训练后期减少噪音的加入。 代码实现 我用pytorch实现了DDPG算法，详见Github 测试 以下是我自己跑的测试结果，测试过程表明某些参数的调节是非常重要的。 MountainCarContinuous-v0 环境细节看这里，小车到达终点就能获得100的奖励。这个环境中小车很容易就找到前往终点的路，所以噪声的衰减可以大一些，设置 \\(\\large d=0.01\\)。 可以看到，小车通过探索很快就找到了终点，并且奖励也接近最优，但因为仍然在往动作中加入噪声，所以小车在尝试有没有更好的路径，经过一番挣扎后，小车发现别的路径都行不通，而此时噪声已经几乎没有了，所以不再进行探索，又回到了之前的获得高奖励的路径上。 如果我们设置 \\(\\large d=0.001\\)，表现如下： 可以发现，噪声衰减调小了，使得小车进行持续的探索。上图经历的episode较少，虽然最终可能找到优秀的路径，但时间代价会很大。原本很快就能解决问题的，没有必要进行长时间的探索。 Pendulum-v0 这个是一个未解决环境，即没有一个奖励区间能说明已经解决。环境细节可以参考这里。 因为环境动作多样化，我们可以进行更多的探索，所以把噪声衰减调节慢一点，设置 \\(\\large d=0.001\\)。 奖励虽然在增长，但一直在负数，后期收敛极慢。而曲线仍然在震荡是因为我们的噪声还没衰减到0。 参考文献 Continuous control with deep reinforcement learning","link":"/post/ddpg/"},{"title":"GloVe","text":"GloVe（Global Vector），是一种结合全局矩阵分解和本地上下文窗口的方法。LSA（latent semantic analysis）虽然能够有效的统计信息，但在词汇类比任务中表现很差。而Skip-gram虽然在词汇类比任务中表现很好，但依赖于窗口的移动，而不能有效统计全局的计数。论文作者认为，全局计数的对数加上双线性回归方法会非常合适。 算法 首先，我们需要对整个语料库建立一个统计全局词频的矩阵，一般称为共现矩阵（Co-ocurrence Matrix）。 假设，语料库有三句话： \\(I \\hspace{0.15cm} enjoy \\hspace{0.15cm} flying\\) \\(I \\hspace{0.15cm} like \\hspace{0.15cm} NLP\\) \\(I \\hspace{0.15cm} like \\hspace{0.15cm} deep \\hspace{0.15cm} learning\\) 那他们的共现矩阵就是： \\(\\large X_{ij}\\) 表示单词 \\(\\large i\\) 在单词 \\(\\large j\\) 上下文出现的次数。你会发现，共现矩阵是对称的（symmetric），因为他同时取左右两边的单词，若只选取左边或右边的任一边，则是非对称的（asymmetric）。 构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系： \\[ u_i^Tv_j + b_i + \\widetilde b_j \\approx log(X_{ij}) \\tag{1} \\] 这里的 \\(\\large u_i, v_j\\) 与Skip-Grams模型中的意义不同，它仅表示的是单词 \\(\\large i,j\\) 之间的联系，没有中心与周围的区分。（对于公式(1)，论文中做了相关推导，因为推导过程有些复杂，我把这部分放到文章后面。） 他的代价函数是： \\[ J = \\sum_{i,j=1}^V f(X_{ij})(u_i^Tv_j + b_i + \\widetilde b_j - log(X_{ij})) \\] 其中 \\(\\large log(X_{ij}) \\rightarrow log(1 + X_{ij})\\)，这么做的目的是防止出现 \\(\\large log0\\) 的情况。 \\(\\large f(X_{ij})\\) 是一个加权函数（weighting function），有些单词对经常同时出现，那么会导致 \\(\\large X_{ij}\\) 会很大，应该衰减它们；而有些单词对从不一起出现，这些单词对就不应该对代价函数产生任何贡献，它们没必要去计算损失值，这时使用 \\(\\large f(0) = 0\\) 来避免计算。 我们可以采用分段函数： \\[ f(x) = \\begin{cases} (\\frac{x}{x_{max}})^\\alpha &amp; if \\; x &lt; x_{max} \\\\ 1 &amp; otherwise \\end{cases} \\] 函数图像如下： 作者实验 \\(\\large \\alpha\\) 取值 0.75，\\(\\large x_{max}\\) 取值 100。 \\(\\large u_i, v_j\\) 的初始化可以不同，也可以相同。当训练得足够之后，\\(\\large u_i, v_j\\) 与 \\(\\large X_{ij}\\) 相似，而 \\(\\large X_{ij}\\) 是对称的，所以 \\(\\large u_i, v_j\\) 也是对称或接近对称。因为都是学习得到的参数，在我们看来没有本质区别。当初始化不同时，相当于加入了噪音，你可以选择把 \\(\\large e = u + v\\) 作为最终向量，这样有助于提高鲁棒性。 实验结果 论文中展示了glove与word2vec等方法的对比，但其实大家都旗鼓相当。 下图是glove在不同参数的情况下的表现： （图a 展示了向量维度对准确度的影响。图b和图c分别使用对称上下文和非对称上下文在不同窗口大小下的表现。以上所有测试都使用6B (billion) 的语料库） 可以发现最好的参数是300维度，窗口大小为5。再往上增加虽然能提高准确度，但也提高了计算代价，没有必要为了提升一点准确度而产生巨大的计算代价。 公式推导 这里对上面公式(1)进行推导。 论文中举了一个简单的例子： 试问单词 \\(\\large k\\) 应该是哪个单词？我们可以通过 \\(\\large P(k \\vert ice)/P(k \\vert stream)\\) 来判断。当 \\(\\large P_{ik}/P_{jk}\\) 很大的时候，说明 \\(\\large i\\) 与 \\(\\large k\\) 更接近一些。 然后，作者就想到，我们可以用参数去近似这个比值。 \\[ F(w_i, w_j, w_k) = \\frac{P_{ik}}{P_{jk}} \\] \\(\\large F\\) 这里暂且设为未知函数，因为它包含了三个参数，所以我们需要削减一下参数，因为参数是线性的，所以自然可以构造成相减的形式。 \\[ F(w_i - w_j, w_k) = \\frac{P_{ik}}{P_{jk}} \\] 然后我们用内积来表现两个单词向量的近似程度， \\[ F((w_i - w_j)^Tw_k) = \\frac{F(w_i^Tw_k)}{F(w_j^Tw_k)} \\] 提取其中一个单词的 \\(\\large F\\)， \\[ F(w_i^Tw_k) = P_{ik} = \\frac{X_{ik}}{X_i} \\] 我们设 \\(\\large F\\) 为指数函数，则， \\[ w_i^Tw_k = log(P_{ik}) = log(X_{ik}) - log({X_i}) \\] 然后，我们用偏置项来替代 \\(\\large log(X_i)\\)，即 \\[ w_i^Tw_k + b_i + b_k = log(X_{ik}) \\] 就这样，这个公式就被奇异地构造出来了。 代码实现 我自己用python实现了一下，但跑起来后发现这没什么意义。GloVe与Skip-gram不同，他构造共现矩阵需要花费大量时间和内存，这部分极度需要计算性能，用python跑半天都跑不完。 论文作者是使用纯C语言实现的，提供了不少预训练的模型，以及词向量的评估脚本，详见Github。 参考文献 GloVe: Global Vectors for Word Representation - Stanford NLP","link":"/post/glove/"},{"title":"『知晓天空之蓝的人啊』：追梦者的重新出发","text":"随着毕业的钟声敲响，学生们纷纷离开校园跃入人海，是坚守家乡，抑或去往城市，人生有路，各自精彩。当经历社会的磨难，他们将变成怎样的人呢？又是否后悔当初的决定？『知晓天空之蓝的人啊』讲述的正是一群中年人与年轻人对人生之路的选择与思考的故事。 本作是超和平Buster三部曲的最后一部，由长井龙雪负责导演，冈田麿里负责脚本，田中将贺负责人设和作画监督，他们在此之前还创作了『未闻花名』和『心灵想要大声呐喊』。我尝试分别从画面，音乐，和主题三个方面解构该作品。 画面 本作一大特点是非常写实的背景，诸多画面是基于埼玉县秩父市的实景进行复刻，而且几乎到以假乱真的程度，会让你感觉这个故事真实地发生在秩父市中。想看实景对比的话可以去看软软冰的圣地巡礼视频BV1aA411n7mx。要说缺点的话，可能就是精度不高的人物，与高精度的背景会产生一些视觉不平衡。 实景(左)与作画对比图1 实景(左)与作画对比图2 然后是一些演出较佳的画面。比如69分44秒左右的这个镜头，利用秋天枯萎的落叶与稀疏的树枝表现姐姐的失落，妹妹在近处，姐姐在远处的景深镜头，更是给观众一种在现场看着她们的感觉，加深了观众的忧伤。88分40秒左右，葵将慎之拉出房子，在慎之飞出时，给了一个葵的视角，可以看到慎之映在了蓝色的天空上，然后与屋内飞出的吉他重叠在一起，暗示慎之与断弦的吉他一般挣脱了束缚。 优秀的演出 音乐 本作音乐由横山克负责，横山克的音乐风格总是厚重而感人。当音乐响起时，画面，角色情感以及观众的心都会逐渐变重，并慢慢沉下来，随着音乐沉入极点。 71分24秒左右，葵偶然发现了茜的笔记本，并阅读笔记时，画面由暗到明，与音乐由轻到重，同步转换，而音乐在葵勇敢地做出行动时及时收住，在这个葵理解姐姐茜的关键点，恰当地渲染了感情，又绝不拖沓。 51分19秒左右，葵和慎之有一段谈话，背景音乐宛如葵的内心一般，迷茫和犹豫，在慎之弹她额头开始，似乎是下定决心撮合姐姐和慎之介，音乐也随之激昂起来。 优秀的背景音乐插入 主题 本作的一大主题是现实与梦想。秩父市是一个小地方，毕业后的慎之想前往大城市东京寻找自己的音乐梦想，而茜却选择了留下来。岁月如梭，转眼13年，当慎之介回到故乡时，整个人变了副模样，原本跋扈的红色头发没了，演奏时的眼神没了光，嘴角也不再上扬，像是被现实和岁月磨平了棱角。虽然确实在音乐上取得了成绩，但离他当初的梦想高度还是差了一大截。 而茜这边，一开始可能会让人认为是为了照顾葵才放弃梦想选择留下，包括葵自己也是这么认为的，所以才拼命想离开，让姐姐摆脱束缚，但从葵找到姐姐的“攻略笔记”后，就揭示了姐姐茜的真正梦想——照顾葵并抚养她长大。从一开始，姐姐就坚持做葵喜欢的昆布饭团而不是慎之喜欢的金枪鱼蛋黄酱饭团，姐姐的梦想从来就没变过，正如毕业纪念册上姐姐的留言：井底之蛙，不知大海之宽阔，却知晓天空之蓝。 慎之与慎之介 慎之介是幸运的，他还留下了另一个自己，被困住的慎之。年轻且充满斗志的慎之用话语与行动唤醒了麻木的慎之介，就像向枯槁的身躯重新注入了热血，枯萎的精神被注入新的灵魂，历尽千帆，仍是少年，青春不再，但不妨重新出发。 冈妈用故乡象征初心，用大城市象征梦想。也许冈妈想传达的是：故乡不是囚笼，而是绿洲，当梦想受到阻碍，你可以在这里憩息片刻，然后鼓起勇气重新上路吧，少年。 最后，以片尾曲『葵』的歌词结束本文： きっと仆たちが想像した未来は 我们所想像的未来肯定 幼い顷见つけた石ころみたいに丸っこくて 就像小时候 找到的石头一样圆 変な伤迹なんかもなくてさ 没有奇怪的伤痕 平和っていう汉字の通りなんだって思っていた 就像是「和平」这个字一样意思 肝心な凡人は梦を追って岛を出た 珍重而平凡的人为了追逐梦想而离开了小岛 胸に梦って书いて飞び出したあの大海原へ 在心中怀藏着梦想 飞奔向茫茫大海 変に泣いたって空気が浊るからさ 莫名地哭泣让空气混浊了起来 じゃあねって言う またねって言う 说了拜拜 又说了再见 石ころを空に投げた 将石头抛向空中 巨大な力で溃されそうな 被巨大的力量击溃的 孤独には その毒には 那孤独里 那毒药中 独特の世界を呼び起こす 有着呼唤起独特世界 魔法があるよ 的魔法唷 サヨナラ いつかの少年の影よ 再会了 曾几何时那少年的身影 また会おうな まだただいま 会再次见面 会再次归来 言える场所はとっておくぜ 让我保留着能说出口的地方 この大空の青さを瞳のパレットに 将广阔蓝天映入瞳孔的调色盘里 潜らせて 包み込んで 深深隐入 包覆其中 涙さえも味方に 连眼泪都深藏起来 きっと仆たちが创造した未来は 我们所创造的未来肯定 ガラスの靴や魔女が飞び交う絵本のような物语 就像是玻璃鞋般或是魔女飞舞般的童话故事 変な理由もなくそう信じていたんだ 没理由的就这样深深相信着 ヘイ ベイビー！なんておどけて笑い 嘿 Baby！那是什么荒谬地笑了 汗かいて大人になった 流汗之后 成为了大人 知らない间に溃されそうな 貌似在不知不觉中压倒了 幸せや野に芽吹く花が 在幸福的田野里发芽的花苞 どうか最期まで咲きますように 请让花朵盛开到最后 全てを光らせ 让一切闪耀光芒 サヨナラ あの日の少年の梦よ 再会了 那一天少年的梦想 また会おうか まだただいま 会再次见面 会再次归来 言えるくらいの余裕はあるだろ？ 你还能够从容地说出口吗？ この大空の青さを心のオアシスにして 将广阔蓝天变成心灵的绿洲 泳いで 潜り込んで 游向绿洲 深潜其中 また帰っておいで 还会再次归来 だから今は 所以现在 サヨナラ 少年の影よ 再会了 少年的身影 また会おうな またただいま 会再次见面 会再次归来 言える场所はとっておくぜ 让我保留着能说出口的地方 この大空の青さを瞳のパレットに 将广阔蓝天映入瞳孔的调色盘里 潜らせて 包み込んで 深深隐入 包覆其中 涙さえも味方に 连眼泪都深藏起来","link":"/post/her-blue-sky/"},{"title":"无锁环形缓冲","text":"无锁环形缓冲(Lock-free RingBuffer)，又称无锁管道，是一个在不使用锁的情况下允许多线程访问的缓冲区，是在竞争条件下维护数据的最高性能选择之一。该方法早在1994年的 Implementing Lock-Free Queues 论文中就开始被研究，该文尝试实现了无锁链表，而本文将尝试实现无锁的环形数组。 构成 一个RingBuffer主要包含如下字段： buf 缓冲数组，每个元素作为结构体node，包含了pos字段表示元素位置，data字段表示用户插入的数据。 cap 容量，整数 mask 容量减一，整数 queue 插入数据的指针，整数 dequeue 获取数据的指针，整数 disposed 是否停止阻塞，布尔 方法有如下： Put(v) 插入数据 Get() 获取数据 Cap() 获取容量 Len() 数据数量 Dispose() 中断阻塞 算法过程 下面用图来分析插入和获取数据的过程。 一开始，一个容量为4的缓存区里面每个node都自带从0开始递增的pos，queue和dequeue指针都是0。 当插入数据时，会在queue指向的位置插入数据，该位置pos加一；然后queue指针会移动指向下一个node，即值加一。 同理，我们再依次插入两个数据，这时queue为3，指向第4个node。 当获取数据时，会获取dequeue指向的位置的数据，然后该位置的pos=dequeue+cap，dequeue移动到下一个node，即加一。之所以pos=dequeue+cap是为了表示该位置已经空出，以便于插入时能通过queue判断出这一情况。 我们继续插入数据，这次会把数据插入到第4个node中，然后queue移动。 当获取数据后，第一个node就空了出来，意味着我们可以把数据插入到第一个node中。 当插入数据时，会作判断，必须是pos-queue=0时，我们才认为可以在该位置插入该数据，如果pos-queue&lt;0，意味着该位置的数据还没被获取，你不能去覆盖它，一般遇到这种情况会在一个循环中阻塞，直到该位置的数据被Get()或者被Dispose()。实际实现时，pos, queue等属性可能会作为无符号整型，pos-queue&lt;0的判断则应该变为pos-queue&gt;0。 若想获得当前缓冲区中有多少数据还没被获取，即Len()函数，则只要queue-dequeue即可。若想中断阻塞或将管道改为非阻塞，则只需要将disposed字段置为true，循环会自行打断。程序实现可以参考我写的这一版rtd/ringbuf.h。 无锁原因 之所以环形缓冲能实现无锁，是因为采用了双指针和CAS。双指针分别指向插入数据位置和获取数据位置，读写时互不干扰，唯一会发生干扰的情况就是当缓冲区满了，两者指向同一位置，必须等到该位置数据被Get之后，即其中一个指针被移开，数值发生了变化后，才能继续插入，而又这里仅仅涉及数值上的比较和更改，这部分完全可以简化为CAS原子操作。 Golang里面的channel用的也是类似方法，但也并非完全无锁，作为标准库必须最大程度确保安全，侧面说明了目前无锁队列的研究并没有完全结束。最理想的情况应该是基于链表实现的无锁队列，因为链表可以实现动态扩容，而环形数组会遇到装满后而必须阻塞的情况。无锁环形数组比较著名的应用有Disruptor，而无锁链表尽管多年来出了不少论文，但在工业上被使用的情况还是很少。 参考 Implementing Lock-Free Queues go-datastructures/queue/ring.go bounded-mpmc-queue Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms","link":"/post/ring-buffer/"},{"title":"压缩算法Snappy原理","text":"snappy是google开源的压缩算法实现，通过测试表明snappy拥有极高的性能表现，但google并没有发表相关论文，可以认为snappy是一个工业算法。snappy借鉴了LZ77的思路，LZ77的匹配过程时间复杂度过高，google对其做了许多优化。在讲snappy之前，我先简单说明LZ77的基本思想。 LZ77 假设我们有一个数列\\(S=[9,8,2,3,4,5,6,2,3,4]\\)，我们发现子数列\\(S_{2,4}=[2,3,4]\\)与\\(S_{7,9}=[2,3,4]\\)是相同的，则我们可以将该数列编码为\\(S=[9,8,2,3,4,5,6,(2,3)]\\)，2表示开始位置，3表示位数。解码时，我们将该元组替换为子数列\\(S_{2,2+3-1}\\)，即\\(S_{2,4}\\)。 完整的描述是，对于数列\\(S\\)，编码时，当发现子数列\\(S_{i,j}\\)与\\(S_{m,n}\\)相同，则可将\\(S_{m,n}\\)替换为元组\\((i,j-i+1)\\)；解码时，则将元组\\((i,n)\\)替换为子数列\\(S_{i,i+n-1}\\)。 对于匹配相同子数列的过程，即使用模式串去查找匹配串的过程。假设数列\\(S\\)，\\(n\\)为数列长度，\\(i\\)为模式串开始位置，\\(w\\)为模式串固定大小，也称为窗口大小。\\(j=i+w+1\\)为匹配串开始位置，最大匹配长度为\\(b\\)，即我们要用\\(S_{i,i+w}\\)去匹配\\(S_{j,j+b}\\)。时间复杂度大概为\\(O(n*w*b)\\)。 Snappy 为了直观，我先简单说明Snappy的编码和解码的核心过程，然后在其基础上分析其优化点。 编码 对于数组\\(S\\)，匹配串开始位置为\\(j\\)，\\(j\\)将以一定步长\\(s\\)增长，我们把找过的\\(j\\)存于数组\\(T\\)中，而模式串开始位置\\(i\\)必须\\(T\\)中获取，我们不限定模式串和匹配串的最大长度，而是限定它们的最小长度为\\(m=4\\)，并直到无法匹配为止，假设最终匹配长度为\\(l\\)，则\\(S_{i,i+l}\\)与\\(S_{j,j+l}\\)匹配。我们将\\(S_{j,j+l}\\)编码为一个三元组，在数组中将用三个位置去表示它，具体参数见下面的说明。 匹配过程为： \\(j = j + s\\) \\(T = T + j\\)，把\\(j\\)放入数组\\(T\\) \\(i = T_u\\)，从\\(T\\)中获取\\(i\\)，\\(u\\)暂且为随机数 若\\(S_{i,i+l}==S_{j,j+l}, \\; l&gt;m\\) 则认为两个子串相匹配 找到一个匹配后需要对其编码，假设把编码结果放在数组\\(D\\)中，其上一次编码后的位置为\\(d\\)，而模式串上一次最后匹配位置为\\(v\\)， （1）编码模式串 \\[ \\begin{aligned} &amp;D_{d}=tag0 \\\\ &amp;D_{d+1}=i-v+1 \\\\ &amp;D_{d+2,:}=S_{v,i} \\\\ &amp;d=d+2+(i-v+1) \\end{aligned} \\] 编码后的第一个数是编码标记\\(tag0\\)，是一个指定数值，用于解码时进行辨识；第二个数是模式串长度；第三个数开始是模式串的拷贝，\\(D_{d+2,:}\\)中的结束位以\\(:\\)表示省略，因为长度可由\\(S_{v,i}\\)确定。这是对不需要压缩的数据，即模式串，进行一个简单拷贝，我们也把该数据称为literal。 （2）编码匹配串 \\[ \\begin{aligned} &amp;D_{d}=tag1 \\\\ &amp;D_{d+1}=j-i \\\\ &amp;D_{d+1}=l \\\\ &amp;d=d+3 \\\\ &amp;v=i+l \\end{aligned} \\] 用3个数表示，\\(tag1\\)是编码标记，是一个指定数值，\\(j-i\\)是模式串相对于匹配串的位移，\\(l\\)为匹配长度 编码完成后，重复匹配过程，直到数组末尾。 解码 对于数组\\(S\\)，将其解码至\\(D\\)，对\\(S\\)解码的上次最后位置位\\(i\\)，而\\(D\\)的最后位置是\\(j\\)， 若\\(S_i=tag0\\)，解码模式串， \\[ \\begin{aligned} &amp;l=S_{i} \\\\ &amp;D_{j,:}=S_{i+1,i+1+l} \\\\ &amp;j=j+l \\\\ &amp;i=i+1+l \\end{aligned} \\] 若\\(S_i=tag1\\)，解码匹配串， \\[ \\begin{aligned} &amp;o=S_{i+1} \\\\ &amp;l=S_{i+2} \\\\ &amp;D_{j,j+l}=D_{j-o,:} \\\\ &amp;j=j+l \\\\ &amp;i=i+2 \\end{aligned} \\] 优化 上面的说明主要为了快速展现算法核心，其实省略了很多工程上的问题，google为其增加了很多优化点，但并没有特地写论文来为这些优化点提供数学支持。 编码位 在实际使用中，我们的数组将是一个常见的uint8数组，即每个元素为1个字节大小。当我们发现模式串与匹配串至少有连续4个相同字节时，我们就会进行编码，替换为3个整数的标记，但一个32位整数就要占4个字节，意味着，最少对4个字节的数据进行编码时，却要花费3*4=12个字节的空间。所以我们会根据这三个整数或两个整数的数值大小，通过位运算对其压缩，确保至少不会比4个字节更大。 比如我们在编码模式串(Literal)时，当匹配长度\\(l&lt;60\\)，则\\(S_{d}=l&lt;&lt;2|tag0\\)，用高位两位表示\\(l\\)，用低位两位表示\\(tag0\\)，相当于把两个32位整数压成了一个8位整数。解码时，\\(s=S_{i}\\&amp;3\\)取出后两位，若\\(s=tag0\\)，则\\(l=S_{i}&gt;&gt;2\\)取前两位，即可得到模式串的长度。 对于编码位压缩的更多种情况，snappy设计了4种tag来区分，这里不再赘述，具体可参考源码。 哈希查找 前面编码时说到，会把找过的匹配串位置放入数组\\(T\\)中，之后的模式串位置会从\\(T\\)中获取\\(i = T_u\\)，之前说\\(u\\)是随机数，但这里应该是要通过hash函数获得的值，并且为了减少重复查找同一个匹配位，获取过的位置会被替换位新的匹配位。假设hash函数为\\(H\\)，则 \\(j = jn\\) \\(jn = j + s\\) 预先计算下一个匹配串位置 \\(i = T_u\\) \\(T = T + j\\) \\(u = H(jn)\\) 计算下一次模式串位置 若\\(S_{i,i+l}==S_{j,j+l}, \\; l&gt;m\\) 则认为两个子串相匹配 通过hash来增加随机性，因为是根据匹配串位置来计算的哈希值，所以确保同一数据进行多次编码可以得到相同的结果。同时，当发现可编码数据后，会一次往\\(T\\)中插入两次匹配串位置，以增加碰撞几率。 步长 匹配位移动会根据步长\\(s\\)移动，步长一开始是1，每比较\\(2^5=32\\)次模式串和匹配串，步长\\(s\\)加1。当\\(s=k\\)时，在没有匹配成功的情况下，匹配位移动32次后实际移动了\\(2^{4+k}\\)位，随着\\(k\\)增大，总移动位数为\\(\\sum_1^k 2^{4+k}\\)。 对于长度为\\(n\\)的数组，可得 \\[ \\large n = argmax_k \\sum_1^k 2^{4+k} \\] \\(k\\)每加一，比较32次，每次比较4字节，所以在没有发现可编码数据时，时间复杂度为\\(O(32*k*4)\\) 参考 A universal algorithm for sequential data compression github.com/google/snappy github.com/golang/snappy","link":"/post/snappy/"},{"title":"负采样的Skip-Gram模型","text":"Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定 Input Word 来预测上下文。而CBOW是给定上下文来预测。本篇文章仅讲解Skip-Gram模型。 数据获取 首先，每次都需要从语料库中抽样一个批次的数据，batch size 为 \\(\\large n\\) 我采用窗口移动的方式获取，中心词（target）为 \\(\\large w_c\\)，窗口（Skip-window）为 \\(\\large m\\)，以中心词为中心，两扇窗口分别涵盖 \\(\\large m\\) 个上下文词汇，则以中心词为中心的取词跨度（span）是 \\(\\large 2 * m + 1\\)。 \\[ w_{c-m}, \\,.. \\, ,w_{c-1}, w_{c}, w_{c+1}, \\, .. \\,, w_{c+m} \\] 有一个参数是取词数量（num-skips），表示每次移动窗口取多少个上下文词汇。以上面为例，假设num-skips为2，那么可能取到的数据是 \\(\\large w_c \\rightarrow w_{c+m},\\; w_c \\rightarrow w_{c+1}\\)，没有规定分别取前后多少词，每次在跨度内除中心词外随机取。 中心词每向后移动一次，则重复上面的操作，直到取满批次大小，最少移动 \\(\\large n/\\text{num-skips}\\) 次。 算法 假设批次大小是 \\(\\large n\\)，Skip-gram的目标函数为： \\[ \\mathcal{L}_{SG}=-\\frac{1}{n}\\sum_{i=1}^n\\sum_{|j|\\le c} log\\;p(w_{i+j}|w_i) \\] 概率函数 \\(\\large p\\) 是我们重点讨论的对象。 我们使用两个词向量去拟合，假设词向量的的维度是 \\(\\large E\\)，词汇表有 \\(\\large V\\) 个单词。 \\(\\large w_i\\): 词汇表第 \\(\\large i\\) 个单词 \\(\\large V \\in \\mathbb{R}^{V \\times E}\\): 输入词向量 \\(\\large v_i\\): \\(\\large V\\)的第 \\(\\large i\\) 行，单词 \\(\\large w_i\\) 的输入向量表示 \\(\\large U \\in \\mathbb{R}^{V \\times E}\\): 输出词词向量 \\(\\large u_i\\): \\(\\large U\\)的的第 \\(\\large i\\) 行，单词 \\(\\large w_i\\) 的输出向量表示 中心词的词向量由输入词向量获得，上下文词的词向量由输出词向量获得。中心词 \\(\\large c\\) 与之对应的上下文词汇 \\(\\large w\\) 的词向量的乘积再求和就该中心词的得分 \\(\\large z_c^w = \\sum^E_i u_w^i v_c^i\\)。 以前会使用softmax函数，但总所周知，softmax函数的计算代价是昂贵的。所以论文作者提出了负采样（ negative samples），称为负采样Skip-gram（SGNS）。 假设单词在上下文中，我们称为正样本 \\(\\large w \\in D^+\\)，若单词不在上下文中，我们称为负样本 \\(\\large \\hat w \\in D^-\\)。每计算一个正样本会携带计算 \\(\\large K\\) 个负样本, 这个 \\(\\large K\\) 不会很大，一般5~20个，数据集越大，这个值越小。 我们要最大化，下面这个函数 \\[ \\begin{aligned} &amp; z_c^w = \\sum^E_i u_w^i v_c^i \\\\ &amp; \\hat z_c^{\\hat w} = \\sum^E_i u_{\\hat w}^i v_c^i \\\\ &amp; \\max_{U,V}\\;log\\;\\sigma(z_c^w)+\\sum_{(\\hat w_k,c)\\in D^-}^K log\\;\\sigma(-\\hat z_c^{\\hat w_k}) \\end{aligned} \\] 即尽量使得正采样的得分变大，而负采样的得分变小。 那么我们可以得到目标函数为， \\[ J = -\\frac{1}{n}\\sum_{i=1}^n \\Big(log\\;\\sigma(z_{c_i}^{w_i})+\\sum_{(\\hat w_k,c_i)\\in D^-}^K log\\;\\sigma(\\hat z_{c_i}^{\\hat w_k})\\Big) \\] 然后使用随机梯度下降最小化目标函数即可。Kaji, et al. (2017) 建议最好使用原生的随机梯度下降，学习率 \\(\\large \\alpha\\) 为1.0，不需要使用Adam或AdaGrad。实验发现，使用Adam或AdaGrad等得到的效果不如SGD，而且还会降低计算效率。 一般的，我们会以输入向量 \\(\\large V\\) 作为最终的词向量。 噪音分布（noise distribution） 负样本的选取并不是完全随机的，其中定义了一个噪声分布去选取负样本 [1]。 我们定义一元组分布（unigram distribution）为单个单词数量与总单词数量的比值的概率分布，即 \\[ U(w_i) = \\frac{count(w_i)}{\\sum_i count(w_i)} \\] 而噪声分布定义为 \\[ P_n(w_i) = \\frac{U(w_i)^{\\frac{3}{4}}}{Z} \\] 其中 \\(\\large Z\\) 是一个扩大系数，一般为 \\(\\text{0.001}\\)。 噪音分布的用意是使频繁出现的词更容易被选为负样本，而不频繁出现的词不作为负样本。为什么呢？比如说，类似于\\(\\text{is, a, the}\\) 这样的高频词，他们没有实际意义，而且不是我们所关心的，所以应该更多的把他们作为负样本。 为什么要选择\\(\\large \\frac{3}{4}\\) 次幂呢？在google输入plot y = x^(3/4) and y = x，可以看到曲线的变化趋势 其中蓝色曲线是\\(\\large \\frac{3}{4}\\) 次幂的，越接近1，增量就越小，目的是适当减少超高频词数量。 \\(\\large P_n(w_i)\\) 在实际使用中就是词汇 \\(\\large w_i\\) 在总负样本池中出现的次数。 代码实现 我用pytorch实现了SGNS算法，详见Github 测试 经过测试使用了噪音分布后，负样本池的大小减少了非常多，而且 spearmans_rho 得分也提高了一点。 eval-data use-noise non-noise EN-MC-30 0.2513 0.2405 EN-MTurk-287 0.2578 0.2357 （词向量300d，训练20万步） 参考文献 [1] Distributed Representations of Words and Phrases and their Compositionality. 2013 [2] Incremental Skip-gram Model with Negative Sampling. 2017 [3] negative sampling tutorial. 2017","link":"/post/skip-gram/"},{"title":"Flannel网络虚拟化探究","text":"Flannel作为容器和Kubernetes网络虚拟化插件，为多机集群的容器间通信提供了支持，目前有3种后端类型。这里我先简单分析Docker和Kubernetes的网络通信原理和问题，然后再分析Flannel的解决方案。 Docker网络 Docker会为每一个容器创建若干种命名空间，拥有独立的协议栈、路由表、进程、套接字和网络设备。容器内与主机之间的通信通过虚拟设备进行，把一个虚拟设备设为容器的命名空间，另一个虚拟设备在默认命名空间，两个虚拟设备相连，容器内的网络数据会通过虚拟设备流出，主机端虚拟设备接收。 为了能更好的转发数据包是，Docker会创建一个网桥docker0，所有主机端的虚拟设备都会与docker0相连，docker0被分配一个虚拟IP，所有容器的虚拟设备地址都属于docker0网段，各容器的虚拟设备地址会被网桥docker0记录。当发现数据包是从一个容器发至另一个容器，docker0通过解析其目的地址就能正确转发至对应容器；若目的地址不属于docker0网段，则会被发至协议栈。 flannel-3.jpg K8s网络 K8s以Pod为单位管理，一个Pod内的所有容器共享网络命名空间，同样通过虚拟设备与主机通信。在单机情况下，K8s网络与Docker网络无异，但虚拟地址仅能在主机内使用，为了能使得集群内的每一个Pod能通过虚拟IP相互通信，需要做一些额外的工作。 这实际涉及到一个网络虚拟化方案的问题，其实能做到这一点的方案有很多，各家公司或团队偏好和需求可能各不相同，为了满足这一点，CoreOS公司提出了CNI模型，旨在统一网络虚拟化的接口，使得各种虚拟化方案能够变为插件，在集群内可插拔的使用。 目前K8s机器支持的网络插件有Calico, Flannel, Wave Net等等。下面我以Flannel为例，讲讲它如何实现网络虚拟化。 Flannel后端类型 Fannel支持几种网络虚拟化方案，根据官方文档可知Fannel有三种后端类型。定义想要使用的后端类型只要在插入etcd的配置中声明即可，配置格式详见这里。下面分别介绍三种后端类型。 VXLAN VXLAN（Virtual Extensible Local Area Network，虚拟扩展局域网）是一种隧道技术，在集群中建立一条基于IP网络的逻辑隧道，这里的隧道可以理解为由很多个网络设备(VTEP)组成的网络，这里的网络设备相当于物理网桥或虚拟网桥，可以把报文相互转发。 VTEP将数据帧加一个VXLAN头部并进行封装在UDP报文里面，然后放入隧道进行传输，这里的封装是报文在隧道中传输的凭据，就像网络层报头是在网络层传输的凭据一般，隧道在把报文转发给目标主机前由一个VTEP设备把之前的封装拆掉。所以对于主机来说，它们发出和接收的数据帧依旧是原始数据帧，隧道中发生的一切对于主机来说都是无感。 关于VXLAN更详细的讲解可以参考这篇文章。 host-gw host-gw（host gateway，主机网关）。我们像划分物理网段那样，对所有主机划分虚拟网络段。原本一台主机的虚拟网络与另一台主机的虚拟网络没有关系，冲突了也无碍，但现在需要把集群中所有主机的虚拟网络统一起来，并按照规则划分，使其不会发生冲突。当确保虚拟网段不会冲突时，我们就可以预先的在每一台主机上对路由表进行修改，把所有虚拟网段直接映射到其对应的物理地址上，这样当一台主机访问一个虚拟地址时，就可以通过路由表直接投递到对应的物理地址并被目的主机接收。 举个例子，我们往路由表里加入一条规则，把虚拟网段映射到某个物理地址上， 110.1.88.0/24 via 192.168.223.13 dev ens33 这里表示凡是目的地址在虚拟网段10.1.88.0/24范围内的包，都通过设备ens33发送到，发送到物理网关192.168.223.13上。 一般来说，via后面都是定义下一跳物理网关（或路由器）的，这样就能使数据包通过网关传输。但这里的192.168.223.13实际上并非网关地址，而是主机地址，相当于我们把一台主机的物理地址作为另一条主机的虚拟网段的网关。 这一后端类型全程都在内核中完成，是三种类型中性能最高的。 UDP 使用UDP模式的前提同样是对集群的虚拟网段进行划分，我们会把虚拟网段和物理地址的映射记录到数据库中，比如etcd。当用户尝试向某个虚拟地址发包时会被路由到TUN设备flannel0上，并传输给常驻进程flanneld，flanneld把用户报文通过UDP封装一层，并查询数据库，把UDP报头目的地址设为目标物理地址，然后再发出去。目标主机收到包后传到flanneld进程里，flanneld对报文拆包，并把原始报文传给flannel0，然后就进入协议栈的流程。 UDP类型导致数据包传输过程中多4次用户态与内核态之间的切换，源主机上从flannel0到flanneld，再从flanneld到协议栈，目的主机上从协议栈到flanneld，再从flanneld到flannel0。性能最差，官方的说法是不建议在生产环境使用。 Flannel UDP的网络传输 当开启UDP类型后，会新增一个flannel0的TUN设备，具有一个网段，并且网桥docker0网段会属于该网段。下面以某个k8s节点的网络设备情况举例，可以看到docker0属于flannel0的子网， 123456# ip addrflannel0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1472 qdisc pfifo_fast state UNKNOWN group default qlen 500 inet 10.1.72.0/32 brd 10.1.72.0 scope global flannel0docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1472 qdisc noqueue state UP group default inet 10.1.72.1/24 brd 10.1.72.255 scope global docker0 docker0与flannel0之间不直连，docker0输出的包会经过协议栈，经过路由表或NAT到达flannel0。flannel0的另一端会连着一个flanneld进程，他会负责包的封装和拆封。 我们先来看看在UDP类型下K8s和Flannel是如何处理网络传输的。其实在K8s里，我们发送网络包的情况无非分为以下8种： 在容器内通过Cluster-IP发送到另一台主机上 在主机通过Cluster-IP发送到另一台主机上 在容器内通过Pod-IP发送到另一台主机上 在主机通过Pod-IP发送到另一台主机上 在容器内通过Cluster-IP发送到自己主机上 在主机通过Cluster-IP发送到自己主机上 在容器内通过Pod-IP发送到自己主机上 在主机通过Pod-IP发送到自己主机上 这里可以先下结论，在UDP类型下，只有情况2“在主机通过Cluster-IP发送到另一台主机上”是不可行的，其他情况都能做到。下面我会逐一分析以上几种情况的网络传输流程，具体验证方法可以通过Linux的iptables, tcpdump, ip route等工具完成。 情况1：在容器内通过Cluster-IP发送到另一台主机上 我们先看第一种情况，整个传输流程可以看下图， flannel-1.png 传输流程： 当Pod A请求Cluster-IP时，网络包经过协议栈并会从容器中的eth0发送到宿主机veth； 网络包通过veth进入docker0，通过docker0时，包的源地址被改写为docker0的地址； 网络包从docker0进入协议栈，netfilter的PREROUTING阶段的会发生NAT，把目的地址从Cluster-IP改写为Pod B-IP。NAT规则是由主机上的kube-proxy动态修改的； 网络包通过NAT转发至TUN设备flannel0，NAT后会马上进行一次路由选择，并且根据路由配置，会把源地址改为docker0地址； 从flannel0进入用户态进程flanneld，flanneld会包的源地址改为flannel0地址，并把包封装到UDP报文中，UDP报头的源地址设为主机地址，目的地址设为Pod B所在的主机地址及其flanneld端口； UDP报文通过协议栈进入主机网卡eth0，并发送到物理网络上； 另一台主机Node B的eth0收到网络包，网络包通过协议栈进入到flanneld进程； flanneld拆开UDP报文，并把原报文发给flannel0 网络包从flannel0进入协议栈，在通过PREROUTING后被路由表路由到docker0； 网络包从docker0到veth，再到容器内的eth0，通过协议栈到达用户进程。 情况2：在主机通过Cluster-IP发送到另一台主机上 前面下了结论，情况2无法做到，这里分析一下流程： 在宿主机请求Cluster-IP； 网络包会通过协议栈，在netfilter的OUT阶段发送NAT，并转发至flannel0，flannel0把包发至flanneld； 按常理，flanneld应该会如同情况1一般处理该网络包，但这里它并没有这么做。原因在于flanneld发现该网络包的源地址是主机地址，而非flannel0网段地址，所以会把这个网络包丢掉。 情况3：在容器内通过Pod-IP发送到另一台主机上 该情况与情况1大体相同，区别在于网络包从docker0进入协议栈后，并不会在PREROUTING阶段发送NAT，而是在之后被路由表路由到flannel0上，之后的步骤就和情况1完全相同了。 情况4：在主机通过Pod-IP发送到另一台主机上 该情况的流程为： 在宿主机请求Cluster-IP； 网络包被路由表路由至flannel0； 理论上来说，这一步会遇到和情况2相同的问题，那就是源地址是主机地址，会被flanneld扔掉，但路由表具有改写源地址的功能，在路由到flannel0之前，就会把包的源地址改写成flannel0地址，所以可以被flanneld处理，之后的步骤就与情况1完全相同了。 情况5：在容器内通过Cluster-IP发送到自己主机上 该情况与情况1的区别是，网络包从docker0进入协议栈，在netfilter的PREROUTING阶段会被NAT到docker0而非flannel0，因为docker0是flannel0的子网，Pod B的地址同时符合两个网段，而路由时会路由到更具体的网段上。网桥docker0会把网络包发至对应的虚拟设备上。 情况6：在主机通过Cluster-IP发送到自己主机上 这种情况下，网络包会通过OUT阶段的NAT转发到docker0，网桥docker0会把网络包发至对应的虚拟设备上。 情况7：在容器内通过Pod-IP发送到自己主机上 该情况下，网络包通过docker0后进入协议栈，被路由表路由至docker0。值得注意的是，因为源地址属于与docker0同一个网段，所以路由的时候会保留源地址。 情况8：在主机通过Pod-IP发送到自己主机上 这种情况下，从宿主机发出的网络包会被路由表直接路由至docker0，网桥docker0会把网络包发至对应的虚拟设备上，全程没有经过宿主机协议栈。值得注意的是，因为源地址属于与docker0不再同一个网段，所以路由的时候把源地址改为docker0地址。 Flannel host-gw的网络传输 在host-gw后端下，不会创建flannel0，flanneld进程不再负责封包与拆包。当flanneld启动后，会先从etcd当前集群的虚拟网段信息，然后自己创建一个新的虚拟网段并记录在etcd，这个网段是给本机docker0使用的，以便于集群中所有docker0的网段都不会发生冲突。同时，flanneld会把集群中所有网段信息，即虚拟网段与物理节点地址的映射，写入本机路由表。flanneld会监听etcd上关于集群网段的信息变化，以便于及时更新本机的路由表。 下面以集群中某个节点的路由表为例， 1234# ip route10.1.29.0/24 via 192.168.223.13 dev ens3310.1.97.0/24 dev docker0 proto kernel scope link src 10.1.97.1... 可以发现，10.1.97.0/24是本机的docker0网段，而10.1.29.0/24则是另一个节点的docker0网段，通过via 192.168.223.13直接指定其下一跳路由地址，即直接通过路由表将数据包传送至目标主机上。同时这样的配置不会修改数据包的目的地址，离开网卡时目的地址仍然是一个虚拟地址，只是通过路由表直接投递到目标主机上，当数据包到达目标主机时，可以通过目标主机的路由表能转发至docker0上。 使用host-gw模式，因为不存在flanneld把数据包丢掉的情况，所以上面提到的8种情况下，所有数据包都是可达的。同时因为数据包全程都在内核转发，不经过内核态和用户态的拷贝，性能是最高的。 下图是在host-gw后端下，在容器内通过Cluster-IP访问另一台主机的网络传输过程， flannel-2.jpg","link":"/post/flannel/"},{"title":"现代文件系统实现简述","text":"文件是磁盘信息的抽象概念，使用户不用了解实际磁盘工作方式等细节，而文件系统则是对文件在磁盘上的存储进行规范，同时为用户访问文件提供接口。这里主要描述文件系统的一些概念，工作原理和实现方法。 分区 磁盘可以划分为多个分区，每个分区可以运行一套独立的文件系统。磁盘的0号扇区，即开头位置，为主引导记录（Master Boot Record，MBR），在计算机启动时用来引导计算机。MBR的结尾有一个分区表，该表给出了每个分区的起始和结束地址。 虽然你的磁盘上可能有多个分区，但计算机必须先找到操作系统所在的分区，即活动分区，而该分区的第一个位置被称为引导块，用于引导计算机去启动该操作系统。其实，不管是不是活动分区，每个分区的第一个位置都默认作为引导块。在计算机被引导时，BIOS读入并执行MBR。MBR做的第一件事是找到活动分区，读入它的引导块（boot block），并执行之。 而分区的第二个位置则是超级块（superblock），在计算机启动时，会把超级块读入内存。超级块中的信息包括：确定文件系统类型用的魔数 （magic number）、文件系统中数据块的数量以及其他重要的管理信息。 file-1.png 接着是文件系统中空闲块的信息，例如，可以用位图或指针列表的形式给出。后面跟随的是一组i节点，这是一个数据结构数组，每个文件一个，i节点说明了文件的方方面面。接着是根目录，它存放文件系统目录树的根部。最后，磁盘的其他部分存放了其他所有的目录和文件。 i节点 i节点是一个数据结构，其记录着若干个磁盘块的位置。一般，一个文件就是一个i节点。因为磁盘块的大小是不定的，可能是1kb或2kb，而文件的大小可能大于该值，所以文件可能离散存到多个磁盘块中。 i节点的实现可以是多样的。比如把磁盘块存储结构设计为数组，用另一个变量表示其长度。也可以为一个i节点设计固定的磁盘块数量，然后用一个变量指向另一个磁盘块存储结构，来扩展磁盘块数量。 日志结构文件系统 Linux ext2等UNIX文件系统有一些弊端，一个是读写速度慢，每一次写操作都必须马上写入磁盘，而每次磁盘寻道操作都会对后面的操作造成延迟，有时也不能充分使用磁盘带宽，假设一个磁盘的带宽是1M，每次写操作要花10ms，如果你一次只写20kb，意味着写速度只有20kb/10ms。而如果你能缓冲1M的数据量，再一次性写入，这时的速度就是1M/10ms，充分利用了磁盘带宽。 日志结构文件系统（Log-structured File System，LFS），在内存中设立一个缓冲队列，把写操作缓冲到队列中，每隔一段时间，被缓冲在内存中的所有未决的写操作都被存到磁盘中。多个写操作可能组成一个段，一个段可能会包括i节点、目录块、数据块，如果所有的段平均在1MB左右，那么就几乎可以利用磁盘的完整带宽。 LFS会把i节点缓存到磁盘或内存中，称为i节点图。要打开一个文件，则首先需要从i节点图中找到文件的i节点。一旦i节点定位之后就可以找到相应的块的地址。 如果我们删除了一个文件，除了需要更新i节点图，还需要回收段中的i节点。LFS有一个清理线程，该清理线程周期地扫描日志进行磁盘压缩。线程会扫描每一个段的i节点，然后查看当前i节点图，判断该i节点是否有效以及文件块是否仍在使用中，如果没有使用，那么i节点和块就进入内存等待写入到下一个段中，原来的段被标记为空闲，以便可以用它来存放新的数据。 虽然基于日志结构的文件系统是一个很吸引人的想法，但是由于它们和现有的文件系统不相匹配，所以还没有被广泛应用。 日志文件系统 日志结构的基本想法是保存一个用于记录系统下一步将要做什么的日志。这样当系统在完成它们即将完成的任务前崩溃时，重新启动后，可以通过查看日志，获取崩溃前计划完成的任务，并完成它们。这样的文件系统被称为日志文件系统，并已经被实际应用。微软的NTFS文件系统、Linux ext3和ReiserFS文件系统都使用日志。 考虑一个删除文件的操作，可能分三个步骤: 一是在目录中删除文件，二是释放i节点到空闲i节点池，三是将磁盘块归还到空闲磁盘块池。假设系统在步骤一后崩溃，那么文件对应的i节点和磁盘块就无法得到释放，在其他步骤处崩溃也会导致不一致性。 日志文件系统会先把操作写作日志，然后持久化到磁盘，确保存入后才开始执行操作，操作全部成功后，才擦除日志。如果系统崩溃，则检查日志来重新运行未完成的操作。这么做意味着一个操作可能会被执行多次，这要求操作重复执行的结果是一样的，即操作具有幂等性。比如，“把i节点k加入空闲表的末端”就是不幂等的，而“如果k不在空闲表则把k加入”则是幂等的。 虚拟文件系统 事实上，一个操作系统可能用到多种文件系统，比如Linux可能把ext2作为根文件系统，而/home用的是ext3，以及装载一个CD到/mnt下，用的是ISO 9660。这时候需要用一个中间层来统一接口，屏蔽不同文件系统的操作差异，该中间层被称为虚拟文件系统（Virtual File System，VFS）。 file-2.png VFS提供一个POSIX接口给用户进程调用，一般程序访问的是VFS而不是实际的文件系统。 当系统启动时，装载进来的文件系统必须向VFS注册，注册时需要提供规定的函数列表，比如read, write, close等。当打开一个文件时，则先根据路径，找到它所在的文件系统，然后通过文件系统注册的接口去打开该文件。具体操作时，VFS会创建一个v节点去调用实际文件系统，文件系统会返回文件的i节点信息，并存到v节点中，v节点也会包含该文件系统的函数列表的指针。 v节点创建后，VFS会在文件描述符表中创建一个文件描述符，并把文件描述符存放一个代理数据结构中，该代理数据结构也会包含该v节点，VFS向调用者返回文件描述符。调用者可以用文件描述符作为POSIX接口的传入值，VFS通过文件描述符找到代理数据结构和v节点，进而操作文件。 比如用户调用read操作，传入文件描述符，VFS通过文件描述符找到v节点，从函数表找到对应函数，向函数传入i节点，由实际文件系统通过i节点找到对应的块并返回数据。 file-3.png 联合文件系统 联合文件系统（Union file system, UFS）允许把多个文件系统进行合并成一个对外文件系统。合并过程中会区分层级，一个文件系统就是一层，合并即是把层堆叠起来，对于用户来说，他打开联合文件系统时，看到的是所有层级合并的结果。如果相同路径和文件名的文件同时存在多个层级，那么只有最高层级的文件对用户可见，其他会被隐藏。联合文件系统有一个原则是：低层文件系统永远是只读的（read-only），只有最高层文件系统是可读写的。 当用户需要查找和读取文件时，会从最高层级开始往下查找，查找到后就返回给用户。当用户创建文件时，则只在最高层的文件系统里创建。当用户修改一个文件，会先判断该文件属于哪一层，如果属于最高层，则直接修改即可；否则，需要从低层把该文件复制到最高层，然后再进行修改，这就是所谓的写时复制（Copy On Write）。 当用户删除一个文件时，先判断文件属于哪一层，如果在最高层，直接删除即可。否则，会在最高层创建一个whiteout文件，标记哪个文件被删除，查找文件时就会止步，而实际上该文件并未从低层文件系统中删除。当用户删除一个目录时，会创建opaque目录，标记被删除的目录，工作流程和删除文件类似。重命名文件和目录时，只有文件和目录都在最高层才允许操作，否则返回EXDEV错误。 现在有诸多联合文件系统的实现版本，比如AUFS, OverlayFS等。 网络文件系统 网络文件系统(Network File System, NFS)，可以使服务器目录挂载到多个客户端的文件系统上。这个过程对于应用程序来说是透明的，应用程序并不用关心它访问的文件是本地的还是远程的，只需通过VFS的接口访问文件即可。 file-4.png NFS经过多个版本的迭代，其中第三版的NFS并不支持open和close操作，因为设计者认为对远程文件的读取和写入的操作都要求是幂等的，而且对于客户端来说，read和write只是涉及数据块的传输而已，真正发生open和close操作是在服务器上。而且这样的话，对于服务器来说是无状态的，read操作的偏移量和块字节数的维护都发生在客户端上。第四版的NFS支持open和close操作，即服务器的NFS变为有状态的。 对于NFS的实现原理，上面说过，VFS会对文件维护一个v节点，指向本地文件i节点。而客户端NFS则维护一个r节点，与远程文件的i节点对应。如果文件是一个远程文件，VFS的v节点则会指向其r节点。假设客户端需要读取一个远程文件，内核会先判断这是一个远程目录，然后NFS驱动向远程请求该文件的文件句柄(文件描述符)，并为此创建一个r节点，同时v节点指向该r节点。 后续的操作都是通过该文件描述符标识，VFS会通过NFS驱动向远程发出请求，消息包含描述符、偏移量和字节数等。一般默认的读取字节数是8kb。到客户端收到8kb块后，不管用户程序有没有继续请求，都会自动请求下一块，这个特性称为预读(read ahead)，以便于需要下一块时可以快速得到。同理，当客户端通过write写数据时，也会积累到8kb才会发送到服务器，除非你手动刷新或关闭文件。 NFS会提供缓存机制，方便对同一个文件进行多次快速读取。但缓存会导致多个客户端同时读写时的数据不一致问题，这时，要么可以通过定时清理缓存来解决，要么通过与服务器比较文件的最后修改时间解决。 参考 Modern Operating Systems (3rd Edition) Kernel Korner: Unionfs: Bringing Filesystems Together","link":"/post/file/"},{"title":"Linux Select 源码分析","text":"先说明一些关于Linux进程间通信的基本知识：Linux的进程间通信是基于文件的，也可以称之为设备，因为所有设备其实都是文件。描述符其实指的是文件描述符，与文件系统中的一个文件对应，每当你创建出一个描述符，就创建出一个设备驱动进程，该进程一直监听着该文件的变化。 核心 select 源码可以参考这里。 首先select不会一直轮询，轮询一次发现没有可处理的事件，进程就会挂起。那为何select中为何能够感知到某一个描述符发生了变化，其实是文件IO事件触发回调函数唤醒了挂起的select进程。而一旦被唤醒后，则记录一个triggered参数，使得select进程以后不再挂起。 当调用select函数，会创建一个实体缓存队列(poll_wqueues)，尔后会调用 poll_initwait(poll_wqueues) 去初始化该缓存队列，缓存队列将缓存一些实体(poll_table_entry)。（队列大小是有限的，可以通过 poll_table_page 结构体扩容，这个不是重点，这里不展开讲。） 12345678910111213141516171819202122232425262728293031323334353637// poll 表格typedef struct poll_table_struct &#123; poll_queue_proc _qproc; __poll_t _key;&#125; poll_table;// 实体缓存队列struct poll_wqueues &#123; poll_table pt; struct poll_table_page* table; struct task_struct* polling_task; //保存当前调用select的用户进程struct task_struct结构体 int triggered; // 当前用户进程被唤醒后置成1，以免该进程接着进睡眠 int error; // 错误码 int inline_index; // 数组inline_entries的引用下标 struct poll_table_entry inline_entries[N_INLINE_POLL_ENTRIES]; // 实体数组，后面会讲&#125;;// select 的核心函数int do_select(int n, fd_set_bits *fds, struct timespec *end_time)&#123; struct poll_wqueues table; // ... poll_initwait(&amp;table); // ...&#125;void poll_initwait(struct poll_wqueues *pwq)&#123; init_poll_funcptr(&amp;pwq-&gt;pt, __pollwait); // ...&#125;static inline void init_poll_funcptr(poll_table *pt, poll_queue_proc qproc)&#123; pt-&gt;qproc = qproc; pt-&gt;key = ~0UL; /* all events enabled */&#125; 每个设备驱动进程都有一个等待队列，等待队列存放一个等待项(wait_queue_entry_t)，首次发生事件时，会调用 pollwait 往队列放入一个等待项，等待项保存了回调函数，由等待项可会获得一个实体(entry)，实体存放一个指针函数与监听事件掩码key，监听事件掩码的不同二进制位存储是否监听对应事件，由此可知道用户想监听哪些事件。当驱动程序发送IO事件，就会扫描等待队列中的实体，检测是否注册了对应事件，并回调函数，该回调函数是一个唤醒函数(pollwake)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 等待项struct wait_queue_entry &#123; unsigned int flags; void *private; wait_queue_func_t func; // 回调函数 struct list_head entry;&#125;;// 实体struct poll_table_entry &#123; struct file *filp; // 指向特定fd对应的file结构体; unsigned long key; // 等待特定fd对应硬件设备的事件掩码，如POLLIN、 POLLOUT、POLLERR; wait_queue_entry wait; // 需要放入等待队列的等待项 wait_queue_head_t *wait_address; // 设备驱动程序中特定事件的等待队列头&#125;;// 新增一个等待项// pollwait() -&gt; __pollwait()static void __pollwait(struct file *filp, wait_queue_head_t *wait_address, poll_table *p) &#123; struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt); struct poll_table_entry *entry = poll_get_entry(pwq); if (!entry) return; get_file(filp); entry-&gt;filp = filp; // 保存对应的file结构体 entry-&gt;wait_address = wait_address; // 保存来自设备驱动程序的等待队列头 entry-&gt;key = p-&gt;key; // 保存对该fd关心的事件掩码 init_waitqueue_func_entry(&amp;entry-&gt;wait, pollwake);// 初始化等待队列项，pollwake是唤醒该等待队列项时候调用的函数 entry-&gt;wait.private = pwq; // 将poll_wqueues作为该等待队列项的私有数据，后面使用 add_wait_queue(wait_address, &amp;entry-&gt;wait);// 将该等待队列项添加到从驱动程序中传递过来的等待队列头中去。&#125;// 唤醒static int pollwake(wait_queue_entry *wait, unsigned mode, int sync, void *key)&#123; struct poll_table_entry *entry; entry = container_of(wait, struct poll_table_entry, wait);// 取得poll_table_entry结构体指针 if (key &amp;&amp; !((unsigned long)key &amp; entry-&gt;key))/*这里的条件判断至关重要，避免应用进程被误唤醒，什么意思？*/ return 0; return __pollwake(wait, mode, sync, key);&#125;static int __pollwake(wait_queue_t *wait, unsigned mode, int sync, void *key)&#123; struct poll_wqueues *pwq = wait-&gt;private; DECLARE_WAITQUEUE(dummy_wait, pwq-&gt;polling_task); smp_wmb(); pwq-&gt;triggered = 1; // select()用户进程只要有被唤醒过，就不可能再次进入睡眠，因为这个标志在睡眠的时候有用 return default_wake_function(&amp;dummy_wait, mode, sync, key); // 默认通用的唤醒函数&#125; 调用结构 123456789101112select() &#123; sys_select(); &#125;sys_select()&#123; // 将用户态参数拷贝到内核 core_sys_select();&#125;core_sys_select()&#123; // 填充 fd_set_bits do_select();&#125;do_select() &#123; // 轮询&#125; do select do_select 是上面我所说的一切工作的外部函数。 接口： 1static int do_select(int n, fd_set_bits *fds, struct timespec64 *end_time) - n 即n个描述符 - fds 类型fd_set_bits是一个结构体，包含了可读，可写，异常三个状态的监听数组，以及三个对应的状态输出数组。 1234typedef struct &#123; unsigned long *in, *out, *ex; unsigned long *res_in, *res_out, *res_ex;&#125; fd_set_bits; - end_time 结束时间 源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160static int do_select(int n, fd_set_bits *fds, struct timespec64 *end_time)&#123; ktime_t expire, *to = NULL; // 实体缓存队列 struct poll_wqueues table; // poll 表格 poll_table* wait; int retval, i, timed_out = 0; u64 slack = 0; __poll_t busy_flag = net_busy_loop_on() ? POLL_BUSY_LOOP : 0; unsigned long busy_start = 0; rcu_read_lock(); // 等一下按位移动查询，所以先计算需要位移的数量 // retval = n * BITS_PER_LONG // BITS_PER_LONG = in | out | exp retval = max_select_fd(n, fds); rcu_read_unlock(); if (retval &lt; 0) return retval; // 注意，这里给了n n = retval; // 初始化等待队列 poll_initwait(&amp;table); // 将初始化后的 poll 表格拿出来，后面传入vfs_poll用于构造等待项 wait = &amp;table.pt; if (end_time &amp;&amp; !end_time-&gt;tv_sec &amp;&amp; !end_time-&gt;tv_nsec) &#123; wait-&gt;_qproc = NULL; timed_out = 1; &#125; if (end_time &amp;&amp; !timed_out) slack = select_estimate_accuracy(end_time); retval = 0; for (;;) &#123; // 轮询 unsigned long *rinp, *routp, *rexp, *inp, *outp, *exp; bool can_busy_loop = false; inp = fds-&gt;in; outp = fds-&gt;out; exp = fds-&gt;ex; rinp = fds-&gt;res_in; routp = fds-&gt;res_out; rexp = fds-&gt;res_ex; // 在描述符数组上按位移动 for (i = 0; i &lt; n; ++rinp, ++routp, ++rexp) &#123; unsigned long in, out, ex, all_bits, bit = 1, j; unsigned long res_in = 0, res_out = 0, res_ex = 0; __poll_t mask; in = *inp++; out = *outp++; ex = *exp++; // 当前描述符监听了哪些事件 all_bits = in | out | ex; if (all_bits == 0) &#123; // 没有监听事件，到下一个描述符 i += BITS_PER_LONG; continue; &#125; for (j = 0; j &lt; BITS_PER_LONG; ++j, ++i, bit &lt;&lt;= 1) &#123; struct fd f; if (i &gt;= n) break; // bit 每次左移一位看看监听哪些事件 if (!(bit &amp; all_bits)) continue; // 找到一个监听的事件，则获取描述符的文件对象 f = fdget(i); if (f.file) &#123; wait_key_set(wait, in, out, bit, busy_flag); // 调用设备驱动程序的poll // 得到事件掩码 // 除了返回掩码外，内部将调用 pollwait() 构造等待项，并放入等待队列 mask = vfs_poll(f.file, wait); fdput(f); // 通过事件掩码查看到底发生了哪些事件 if ((mask &amp; POLLIN_SET) &amp;&amp; (in &amp; bit)) &#123; res_in |= bit; retval++; wait-&gt;_qproc = NULL; &#125; if ((mask &amp; POLLOUT_SET) &amp;&amp; (out &amp; bit)) &#123; res_out |= bit; retval++; wait-&gt;_qproc = NULL; &#125; if ((mask &amp; POLLEX_SET) &amp;&amp; (ex &amp; bit)) &#123; res_ex |= bit; retval++; wait-&gt;_qproc = NULL; &#125; /* got something, stop busy polling */ if (retval) &#123; can_busy_loop = false; busy_flag = 0; /* * only remember a returned * POLL_BUSY_LOOP if we asked for it */ &#125; else if (busy_flag &amp; mask) can_busy_loop = true; &#125; &#125; if (res_in) *rinp = res_in; if (res_out) *routp = res_out; if (res_ex) *rexp = res_ex; cond_resched(); &#125; wait-&gt;_qproc = NULL; // 发现事件 或 超时 或 当前进程有信号要处理 则打断轮询 if (retval || timed_out || signal_pending(current)) break; if (table.error) &#123; retval = table.error; break; &#125; /* only if found POLL_BUSY_LOOP sockets &amp;&amp; not out of time */ if (can_busy_loop &amp;&amp; !need_resched()) &#123; if (!busy_start) &#123; busy_start = busy_loop_current_time(); continue; &#125; if (!busy_loop_timeout(busy_start)) continue; &#125; busy_flag = 0; /* * If this is the first loop and we have a timeout * given, then we convert to ktime_t and set the to * pointer to the expiry value. */ if (end_time &amp;&amp; !to) &#123; expire = timespec64_to_ktime(*end_time); to = &amp;expire; &#125; // 挂起当前进程，直到被唤醒或timeout if (!poll_schedule_timeout(&amp;table, TASK_INTERRUPTIBLE, to, slack)) timed_out = 1; &#125; // 释放实体缓存队列的内存 poll_freewait(&amp;table); return retval;&#125; 总结过程 select调用的基本步骤就是， 调用 select select 轮询一次，找出需要监听事件的描述符，把等待项(监听掩码+回调函数)放到描述符对应的驱动程序的等待队列里边，并返回一个结果掩码，比对所有结果掩码，如果没有发生事件，则挂起。（一般第一次轮询都是挂起的，否则就是第一次轮询之前就发生了事件） 发生IO事件 select进程被唤醒，第二次轮询，找出发生事件的描述符及其具体事件，在fd事件数组上对应位置上做标记。但可能发生的IO事件并非用户要求监听的事件，这时候就不再挂起，而是一直轮询，等待下一次IO事件 清除缓存 返回一个发生事件描述符的数量 用户调用 FD_ISSET() 找出哪个描述符发生事件，处理一些事情 处理完后，又循环回到第1步 参考 https://my.oschina.net/fileoptions/blog/911091 https://elixir.bootlin.com/linux/latest/source/fs/select.c","link":"/post/linux-select/"},{"title":"Kafka结构分析","text":"分布式消息队列的优势是很好地对计算机系统进行了解耦，不同系统之间通过消息队列来交换消息。Kafka是分布式消息队列的代表作之一，很好的兼顾了性能和一致性，但代价就是使用起来更加复杂。我花了点时间阅读了Kafka 0.10版本的源码，本文不对繁杂的源码细节和实现做描述，只关注架构和思路。 1 主题 (topic) 主题是消息队列用来区分数据的逻辑概念，可以把不同类型的数据分开，让消费者只获取某个主题的数据。 2 分区 (partition) 知道主题之后，最直观的想法是，一个主题一条队列，这样就能做到只获取某一个主题的数据，这个想法是对的。但分布式的呢，可以想到一个主题多条队列，一条队列放在节点上，这样消费者就能充分发挥分布式IO效率获取该主题的数据。但是这样还不行，万一个节点挂了，存在该节点上的所有队列就丢失了。所以，每一条队列还要有副本，存放在不同的节点上，当某个节点挂了，消费者就可以从别的节点上的副本中找数据。而这条队列，就称为“分区”。 如上所说，每个分区的数据会维持多个副本分散在不同的节点上，其中有一个leader和若干个follower。写入数据时先写到leader中，随后leader将数据拷贝到followers上。如果某个follower挂了不会有影响，因为获取数据时优先从leader获取，kafka就再找一个节点作为该分区的新follower，然后让leader把数据拷贝过去。当leader挂了，就从followers上选出一个新leader。 3 生产者 生产者负责将用户输入的数据存入分区中。 3.1 发送方式 生产者客户端通过类似于 send(data, callback) 的函数发送输入数据，先经过自定义的拦截器做一些处理，下一步将数据序列化，通过哈希函数计算出该存到哪个分区上，然后将数据放入分区对应的缓冲区中。每个缓冲区可以存放多个batch，每个batch有一个内存块，内存块序列存储用户的输入数据。每次请求以batch为单位。输入数据先经过压缩器的压缩，然后插入到内存块中，插入后会记录一个元数据。如果输入数据超出内存块大小就新建一个batch，默认大小是16Mb。 元数据主要存储时间戳，crc，数据所在分区，在batch中的偏移offset，数据大小等参数，这个元数据不会发送出去，会作为用户回调函数的参数，用户可以获得这些参数，具体用途由用户自己决定。 每一个输入数据对应有一个thunk，我们把回调对象和元数据存入该对象中。当该batch发送成功并响应时，会遍历该batch下的所有thunk，调用每个thunk的回调方法。如果没有收到响应就重新发送。 我们开启一个IO线程，线程不断从缓冲区中获取batch。IO线程维护若干个客户端对象，把batch分给其中一个客户端，由它负责发起请求，发起请求可以是多线程的。每发起一个请求后就会把该请求缓存到一个等待响应队列中，当响应成功，就把请求从队列中去掉，我们也可以通过该队列判断客户端的负载情况。 3.2 集群元数据 客户端对象内部维护一个集群元数据，由此知道每个分区应该发送到哪个节点上，这些数据需要从zookeeper服务中获取集群元数据信息，那什么时候获取元数据呢？一个是定期请求，比如3分钟一次，另一个是发送数据时发生异常。一旦发起获取元数据请求，其他线程中的请求需要阻塞，直到元数据成功获取后再唤醒它们。 4 消费者 消费者负责从分区中获取数据。采用主动拉取的方式，通过类似于 poll() 的函数单线程轮询服务端并获得数据。生产者只需要把数据扔进分区中即可，而消费者不单纯把数据从分区中拿出来，它要考虑数据的一致性问题。 比如生产者生成了一个订单，扔进了分区。而消费者从该分区中拿出来，万一在处理过程中消费者挂掉了，那这个订单算怎么回事？你必须确保这个订单能够被正常处理。 4.1 一致性 总体来说，确保数据的一致性有两个方面，一个是确保数据不丢失，另一个是确保数据不重复。 这就是所谓的传递保证(Delivery guarantee semantic)： At most once： 消息可能会丢， 但绝不会重复传递。 At least once： 消息绝不会丢， 但可能会重复传递。 Exactly once： 每条消息只会被传递一次。 确保数据不丢失这个问题，我们可以通过偏移和提交来解决。高可用的消息队列一定会把数据存储起来，而不是单纯的将数据从一个地方传到另一个地方。因为我们使用了分区，所以可以由指定一个消费者去消费某一个分区的数据。而消费者根据偏移每次获取一份数据，处理完后就提交这个偏移。万一中途挂了，因为偏移没有被提交，所以仍然认为这个数据没有被处理。 对于重复的问题，生产者为了确保数据已安全放入分区，需要等一个响应。当没有等到响应时怎么办呢？如果数据发送过程出错，那就重新发送即可。但也有可能数据已经放入分区了，但响应包丢失了，这时生产者重新发送就会造成数据重复。一个比较好的解决办法是，每个消息一个全局唯一ID，如果消费者发现ID重复，就忽略掉。 4.2 分区分配策略 每个消费者会被分配到一个或多个分区，但可能随时加入新的消费者，或移除旧的消费者，不管如何，你要确保所有分区都有消费者在消费。也就是说，当加入新消费者或移除旧消费者，你要为所有消费者重新分配新的分区，这一行为称为再平衡(Rebalance)。 我们把对监听同一个主题的消费者看成是一个组(Consumer Group)，我们需要把该主题下的所有分区分配给该组消费者。 4.2.1 Rebalance 在讲怎么分配之前，我们需要先知道Rebalance的过程。 一般操作是，服务端维护了名为GroupCoordinator的中心控制台，当新开启一个消费者，消费者会获取一个GroupCoordinator，消费者会发送消息到中心控制台。中心控制台根据该消费者监听的主题从对应的组里选出一个Leader，然后给该组所有成员发送消息，告诉它们现在有新成员加入要重新分配任务，都把手动上的事情停一停。期间，只有Leader能知道该组所有成员的信息，Leader计算出分区分配结果，然后发给中心控制台，中心控制台再发给该组的所有成员，成员们拿到分区信息后就能开始工作了。 一个消费者同时监听多个主题的情况也是同样适用的，相当于一个消费者同时属于多个组而已。 4.2.2 分配方法 分配方法是可以在启动消费者时自定义的，但使用内置方法也是可以的。 方法一，RangeAssignor：针对每个Topic，n=分区数/消费者数量， m=分区数%消费者数量，前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个Partition。 方法二，RoundRobinAssignor：将所有Topic的Partition按照字典序排列， 然后对每个Consumer进行轮询分配。 举个例子，有C0、C1两个消费者和t0、t1两个Topic，每个Topic有三个分区编号都是0～2。使用RangeAssignor的分配结果是：C0: [t0p0, t0p1, t1p0, t1p1]， C1: [t0p2, t1p2]；使用RoundRobinAssignor的分配结果是：C0: [t0p0, t0p2, t1p1]、C1: [t0p1, t1p0, t1p2]。 4.3 心跳 服务端是多节点的，GroupCoordinator可能处于不同节点上，当节点挂掉，节点上的GroupCoordinator也就挂掉了。所以要确保在节点挂掉后，消费者能找到新的GroupCoordinator。这时就需要心跳协议，消费者定时向GroupCoordinator发送心跳，如果响应超时则认为该GroupCoordinator挂掉了，消费者需要再次获取一个GroupCoordinator。 反过来，如果GroupCoordinator长时间没有收到消费者的心跳，它也会认为消费者挂掉了，就会触发重新分配(Rebalance)。 5 服务端 服务端负责接收和存储生产者发送的消息，以及将数据提供给消费者。因为是多节点的，我们把每个节点称为Broker。 5.1 存储 在分布式领域中，我们一般把消息或数据看成日志，以日志的方式存储。一个Topic可以有多个分区，每个分区使用偏移量offest来标识一份数据。 物理存储结构是以&lt;topic_name&gt;-&lt;partition_id&gt;的目录存储，目录下保护一个分区的日志文件.log，日志文件可以分成多个，以该文件中第一份数据的偏移量作为文件名，比如0003458.log。 实际存储时，并不是来一条消息就马上写入文件中，而是尝试将多条消息合并压缩后再写入文件，这样可以节省存储空间。压缩后的结构将变为内层消息和外层消息，内层消息是真实的单条消息，外层消息是内层消息的集合。外层消息的偏移量以最后一条内层消息为准，同时维护一个内部偏移量。 为了提高查询效率，每一个日志文件附带一个索引文件0003458.index。索引文件存储消息的位置和偏移量的对应关系，已知偏移量，可以通过索引文件快速知道该消息在log文件中的位置。为了节省空间，索引文件使用了稀疏存储的方式。 通过对索引文件记录的二分查找，可以快速找到消息所在位置的附近，然后顺序查找一下即可。 一个log和它对应的index文件在程序中看作一个LogSegment。一个分区的所有LogSegment以跳表形式存储。 5.2 副本 每个分区维护多个副本，并且会从其副本集合中选出一个副本作为Leader副本，所有的读写请求都由选举出的Leader副本处理。剩余的其他副本都作为Follower副本，Follower副本会从Leader副本处获取消息并更新到自己的Log中。 一般情况下，同一分区的多个副本会被均匀地分配到集群中的不同Broker上，当Leader副本的所在的Broker出现故障后，可以重新选举新的Leader副本继续对外提供服务。 涉及到选举，那么就必须关心谁有资格参与选举。副本的同步涉及到多个节点，从Leader发往Followers，总有一些Followers同步速度跟不上，只有跟上Leader同步进度的Followers才有资格参与选举。所有副本统称为AR(Assigned Repllicas)，保持同步的副本称为ISR(In-Sync Replicas)，同步滞后的副本称为OSR(Out-Sync Relipcas)，能够参与选举的必须是该分区处于ISR的节点。 5.3 Broker Leader 因为存在多个Broker，为了方便管理，需要选出一个Leader来管理其他所有Broker。 不使用Leader的话，每个Broker都各自从ZooKeeper中获取元数据，而ZooKeeper本身的数据也在不断更新，这种情况下，不同Broker发起请求的时间差会导致可能获得到不同的数据，出现“脑裂”，“羊群效应”等问题。 使用Leader后，只有Leader能与ZooKeeper交互，Followers发生了什么问题统一向Leader请求指示，比如某分区Leader副本故障，或新增了消费者等。当然，要是Leader挂了，同样要从Followers中选举出新的Leader。","link":"/post/kafka/"},{"title":"Raft分布式系统构建方法论","text":"前文讲述了Raft算法的核心，本文提供将Raft算法应用于分布式系统的宏观指导，将不再赘述算法细节而是专注于用户层面和应用层面，描述一个Raft的分布式系统需要进行哪些工作。 组件 一个Raft节点，需要包含的组件有 Config 配置 Storage 日志存储器 Logger 打印程序运行时日志 Raft 算法核心 Node 一个节点的代称，是Raft的上层包装 提议和消息 我们把向Raft集群发起用户自定义操作的行为称为提议(Propose)，而一个提议称为(Proposal)。Raft内部沟通的信息称为消息(Message)。 对于一个Node，如果收到的是一个Message，则直接进入Raft的状态机中；如果向Leader发起一个Proposal，则需要先包裹成Message，然后再进入状态机。到了这一步再称为提议(Proposal)可能就不恰当了，我们可以称一个提议的数据实体为Entry。 快照 快照即把当前集群的状态保存下来。事实上，快照并不属于Raft算法的一部分，是由用户自行维护的。别以为很复杂，其实总共需要记录的数据就两个，term和commit index (applied index)。主要是为了再启动时能够回到关闭时的状态。 轮询 需要一个轮询时刻监听来自客户端的Proposal或Message。轮询需要做的事情大概如下： 接收到一个Proposal或Message，接收后就会进入状态机中处理。 更新时钟滴答，以便于适时发送心跳。 进入Ready阶段，发送Message，检查并处理提交后的提议。 以上操作需要顺序执行，且在单线程内完成。 而接收Proposal或Message的具体做法可以通过跨线程管道来处理，即CSP并发模型。这种多生产者，单消费者的模式在这种轮询场景下特别适用。 Ready 当我们把Message投入状态机后，经过状态机的一系列判断和转换，某些操作会产生一个新的Message，并存入内部日志中。要注意的是，Raft算法不应该包含网络传输，使用怎样的网络传输方式应该由外部决定，即用户需要自己从内部日志中获取该Message，然后再发送出去。 Raft会把需要发送的Message，和需要处理的Entry都包裹到一个名为Ready的结构中，它可以包含零个，一个或多个Message和Entry。在Ready阶段，主要对该结构进行处理。 在处理一个Ready时，你可能需要做以下几件事情： 持久化Entry。当本节点拿到新Entry时，需要手动持久化该Entry。注意是持久化Entry，而不仅是你所提议的数据，因为Entry还包含了term，index等信息。 保存快照。快照是对当前内存数据的一次集中存储，方便宕机后快速还原。 发送Message。根据状态机创建的Message中的目标节点ID，向其发送该Message。 处理提交后的Config Entry。提交后的Entry会放到缓存区中，在这里你需要尝试获取，如果有就进行处理。Config Entry是用于配置集群的指令，比如新增一个节点或删除一个节点。根据算法，必须在提议提交后才能正式加入新节点或删除节点，对于类似行为必须在此处进行操作。 处理提交后的Normal Entry。Normal Entry就是用户发起的提议。需要在提议提交后进行一些回调，比如，向客户端响应读操作的结果等。提议和Message一定会被顺序执行和提交，所以你可以使用链表等结构存储提议的回调函数或其他信息。另一个办法是在提议数据中包含一个唯一ID，凭此做对应的处理。 检查是否存储快照。一般记录每隔提交个entry就存储一次快照。 前进(Advance)。这里表示该Ready已经处理完成的意思，标记一下，然后就可以获取下一个Ready，不标记则永远返回同一个Ready。 滴答与心跳 Raft结构体只负责算法的实现，即定义了每隔多少个滴答(Tick)就发送心跳的行为，而程序运行时间和一个滴答的时间间隔则需要外部维护。比如一个滴答是100ms，当发现经过了100ms，就需要让Raft内部的滴答数加一，当滴答数到达3(假设)，Raft就自动发送心跳。 运行 讨论Raft集群运行过程中需要处理的问题。 配置 主要配置一些不会变动的信息，启动一个Raft节点主要需要配置以下信息： 节点ID 心跳的滴答数 重新选举的滴答数（没有收到心跳） 读操作的模式 是否开启预投票(Pre vote) 存储器 存储器是Raft节点加载和存储信息和数据的工具。使得Raft节点在启动能跟上集群的最新状态，以及重新启动时迅速回到上次关闭时的状态。 储存器需要存储和提供以下信息： Raft状态，包括commit index, leader, term, 成员ID等信息。 所有Entry。 快照(Snapshot)。 启动 当启动集群时，有一些需要注意的问题。 不要同时启动所有节点，而是先选定一个节点启动一段时间，至少该子节点自己成为了Leader，然后再启动其他节点。如果节点启动时间太近会导致难以选举出leader，因为大部分节点的term和index可能相同，大家都争着当候选人，但又无法拉开差距，进而不断发起选举。 时间滴答的时间间隔不能太小，如果太小会导致还没来得及响应投票选举出leader就发起了新一轮选举。 对于多节点启动，配置方式大概分为两种，一种是一开始就在成员配置表中写好所有节点，然后所有节点启动时都读取该配置，即静态配置；第二种方法是先启动一个节点，再启动其他节点，其他节点并没有成员配置表，而是走“新增节点”流程，即运行时配置。 新增节点 新增节点的过程大概如下： 新增节点向leader发送一个请求新增节点的指令，需要附带上自己的ID和RPC端口。 leader收到指令后通过网络库提取出该节点的host地址(IP地址)，和它发送过来的RPC端口号可以组成该节点程序的具体地址。 leader创建一个Conf Message，填入新节点的ID和地址，然后投入状态机。 当多数节点收到该Message后就会被提交，提交后所有成员将该节点加入成员记录中。 再启动 再次启动时需要读取本地存储中的快照，即term和commit index，以便于直接跳过已经提交的日志。 删除节点 删除节点过程则简单很多。向leader发起删除指定节点的提议，然后leader将该节点从记录中删除即可。 与对外服务沟通 一般集群服务都会运行一个对外服务程序与外部沟通，可以是TCP Server或HTTP Server。该服务要与Raft服务相互沟通，则可以使对外服务作为生产者，往管道中投入一个Proposal，使其进入Raft的轮询中。并构建一个反向的管道，在回调中向反向管道输入结果，而对外服务则接收结果。 读操作可以向任何节点请求，而写操作只能向Leader请求。一般，所有成员都运行同一份代码，收到提议的节点会通过ID标识并缓存提议和回调，虽然Entry提交后所有节点都会做出同样的响应操作，但只有具有该ID缓存的那个节点才会成功向客户端响应。 框架化 上面讲到的点很多都是通用的，不同的仅仅是业务逻辑。我们可以提炼出一个框架来处理。事实上，框架化是一个很个性化的事情，不同人的思路和实现方式都可能不同，下面仅提供一种思路。 首先，业务逻辑和消息队列交给用户自己维护，抽象为RaftService，而Raft的轮询和算法抽象为RaftServer。我们的目标是把RaftService传入RaftServer中即可完成Raft集群的一系列工作，同时我们也可以把RaftService传入对外服务中，起到沟通作用。 RaftService实现定义一系列接口，具体实现交给用户。我们可以提炼出的接口有： recv() -&gt; Message 从管道(或队列)获取一个Message。 send(Message) 向管道(或队列)投入一个Message。注意，该Message应该是框架定义的对象，而不是Raft里面的Message，因为可能还要包含其他操作。 update_context(Context) 更新上下文(Context)，上下文对象用于储存集群状态信息，该接口被框架定时调用并传入新的Context。 context() -&gt; Context 用户获取可以通过该接口获得集群状态信息。 propose(bytes, callback) -&gt; Something 向Raft发起一个提议，用户自行将需要提交的数据包装成Message，并放入管道中。callback是指提交之后的回调函数，由用户自行管理。一般有一个返回值，告诉上游是否提议成功，比如有些提议只能由leader发起。 propose_conf_change(ConfChange) -&gt; Something 用于发布配置操作。 on_committed(bytes) 提交后的处理，Raft集群提交一个Entry后调用。 其中提议传入bytes只是为了通用，如果你使用的语言有更强大的泛型系统，可以使用泛型取代。 实现 根据上面的理论，我用Rust实现了一个Raft服务框架，详见github。","link":"/post/raft-build/"},{"title":"词性标注的简单综述","text":"词性标注(Part-of-Speech Tagging, 简称POS tagging)是将句子中的每个词做一些标记，如动词，名词，副词，形容词等。词性很有用，因为它们揭示了一个单词及其相邻词的很多信息。知道一个单词是名词还是动词可以告诉我们可能的相邻单词(名词前面有限定词和形容词，动词前面有名词)和句法结构单词(名词通常是名词短语的一部分)。一个单词的词性甚至可以在语音识别或合成中发挥作用，因为有些单词不同词性时的读音是不同的。在本综述中，将讨论词性标注的相关算法，比如早期的隐马尔可夫模型 (Hidden Markov Model, HMM)和随机条件域 (Conditional Random Fields, CRF)，以及近几年的神经网络。 1 介绍 词性标注的研究始于20世纪60年代初。词性标注是自然语言处理的重要工具。它是许多NLP应用程序中最简单的统计模型之一。词性标注是信息提取、归纳、检索、机器翻译、语音转换的初始步骤。在上世纪80年代末，人们使用基于隐马尔可夫模型已经使词性标注已经最高达到了95%的准确度。而最近几年，由于神经网络的完善和推广，有些模型可以达到97%的准确度。 2 早期算法 早期解决词性标注问题以隐马尔可夫模型算法为主。 2.1 HMM HMMs和随机语法被广泛应用于文本和语音处理的各种问题，包括主题分割、词性标注、信息提取和句法消歧。 在计算语言学和计算机科学中，隐马尔可夫模型(Hidden Markov models, HMMs)和随机语法被广泛应用于文本和语音处理的各种问题，包括主题分割、词性标注、信息提取和句法消歧。 2.1.1 Hidden Markov Model 马尔可夫链是一个模型，它告诉我们随机变量序列的概率。这些集合可以是表示任何东西的单词、标记或符号，例如天气。马尔可夫链有一个重要的假设，如果我们想在序列中预测未来，重要的是当前状态，当前状态之前的所有状态对未来都没有影响。就好像要预测明天的天气，你可以检查今天的天气，但是你不允许查看昨天的天气。 而马尔可夫假设(Markov Assumption)表示为， \\[ P(q_i=a|q_1 ...q_{i-1} ) = P(q_i=a|q_{i-1} ) \\] 规定，离开给定状态的弧的值之和必须为1。一种马尔可夫链，用于为单词序列\\(w_1...w_n\\)分配一个概率，它表示一个双语言模型，每条边表示概率\\(p(w_i|w_j)\\)。 \\(Q=q_1q_2...q_N\\) 表示N个状态的集合。 \\(A=a_{12}a_{12}...a_{n1}...a_{nm}\\) 一个转移概率矩阵。\\(a_{ij}\\) 表示状态\\(i\\)到状态\\(j\\)的概率。 \\(\\pi=\\pi_1\\pi_2...\\pi_N\\) 一个开始概率分布。 隐马尔可夫模型，即我们不直接观测状态。例如，我们通常不会在文本中观察词性标记。相反，我们看到单词，必须从单词序列中推断出标记。我们将这些标记称为隐藏标记，因为它们没有被观察到。 \\(Q=q_1q_2...q_N\\) 表示N个状态的集合。 \\(A=a_{12}a_{12}...a_{n1}...a_{nm}\\) 一个转移概率矩阵。\\(a_{ij}\\) 表示状态\\(i\\)到状态\\(j\\)的概率。 \\(O=o_1o_2...o_N\\) 序列\\(T\\)的观测。 \\(B=b_i(o_t)\\) 表示由状态\\(i\\)产生的观测值\\(o_t\\)的概率 \\(\\pi=\\pi_1\\pi_2...\\pi_N\\) 一个开始概率分布。 第二个假设：输出观测\\(o_i\\)的概率只取决于产生观测\\(q_i\\)的状态，而不取决于任何其他状态或任何其他观测: \\[ P(o_i|q_1...q_{i-1}) = P(o_i|q_{i-1}) \\] 2.1.2 HMM 标记器 对我们的语料库构建一个词性转移概率矩阵\\(A\\)，包含\\(P(t_i|t_{i-1})\\)，表示给定前一个标记得到当前标记的概率，比如像will这样的情态动词后面很可能跟一个基本形式的动词。 而构建这样的词性转移概率矩阵，需要统计语料库中的词性转移次数， \\[ P(t_i|t_{i-1}) = \\frac{C(t_{i-1},t_i)}{C(t_{i-1})} \\] 比如，MD在WSJ语料库中出现的次数是13124，而MD之后出现will的次数为4046， \\[ P(will|MD) = \\frac{C(MD,will)}{C(MD)} = \\frac{4046}{13124} = 0.31 \\] 2.1.3 HMM 解码器 解码器的定义是：输入一个HMM \\(\\lambda=(A, B)\\) 和观测\\(O=o_1o_2...o_N\\)，找到最有可能的状态序列 \\(Q=q_1q_2...q_N\\)。 n个单词的序列\\(w^n\\)，标记序列\\(t^n\\)，则最终序列结果\\(\\hat{t}^n\\)由： \\[ \\hat{t}^n = \\mathop{argmax}_{t^n}P(t^n|w^n) \\] 有贝叶斯定理可得： \\[ \\hat{t}^n = \\mathop{argmax}_{t^n}\\frac{P(w^n|t^n)P(t^n)}{P(w^n)} \\] 为了简化公式，去掉分母， \\[ \\hat{t}^n = \\mathop{argmax}_{t^n}P(w^n|t^n)P(t^n) \\] 根据第一个假设可知，一个标记只取决于前一个标记。 \\[ P(t^n) \\approx \\prod_{i=1}^n P(t_i|t_{i-1}) \\] 根据第二个假设可知，一个单词(观测)依赖于自身的标记(状态)以及相邻单词及其标记，可得 \\[ P(w^n|t^n) \\approx \\prod_{i=1}^n P(w_i|t_i) \\] 最终可得， \\[ \\hat{t}^n = \\mathop{argmax}_{t^n}P(t^n|w^n) \\approx \\mathop{argmax}_{t^n} \\prod_{i=1}^n P(w_i|t_i)P(t_i|t_{i-1}) \\] 2.1.4 The Viterbi Algorithm 维特比算法建立一个概率矩阵或格子(lattice)，这个格子，的每一列表示一个观测，而每一行表示一个状态。 对于格子中的每一个单元格 \\(v_t(j)\\) 表示HMM \\(\\lambda\\) 经过了\\(t\\)个观测来到状态\\(j\\)中。 \\[ v_t(j)=\\mathop{max}_{q_1...q_{t-1}}P(q_1...q_{t-1},o_1,o_2...o_t,q_t=j|\\lambda) \\] 其中\\(q_1...q_{t-1}\\)表示该单元格经过的所有状态，我们需要像动态规划那样最大化这个状态序列，以便于最终取得最优值。 可得迭代公式， \\[ v_t(j)=\\mathop{max}^N_{i=1}v_{t-1}(i)a_{ij}b_j(o_t) \\] 2.2 CRF HMMs和随机语法是生成模型，将联合概率分配给成对的观察和标签序列。为了定义观测序列和标号序列的联合概率，生成模型需要枚举所有可能的观测序列。特别是，表示多个相互作用的特征或观测值的长期依赖关系是不现实的，因为这类模型的推理问题是难以解决的。 条件随机域 (conditional random fields, CRF) (Lafferty, McCallum &amp; Pereira, 2001)是一个单一的指数模型，可以计算得到给定观测序列的整个标签序列的联合概率。 我们用\\(z = {z_1,..., z_n}\\)表示一个通用的输入序列，\\(z_i\\) 表示第\\(i\\)个单词的向量。\\(y={y_1,...,y_n}\\) 表示\\(z\\)的标签。\\(\\mathcal{Y}(z)\\) 表示\\(z\\)的标签序列集。序列的条件随机域定义为一个条件概率模型 \\(p(y|z;W,b)\\)。给定\\(z\\)与所有可能的\\(y\\)组成： \\[ p(y | z ; W, b)=\\frac{\\prod_{i=1}^{n} \\psi_{i}\\left(y_{i-1}, y_{i}, z\\right)}{\\sum_{y^{\\prime} \\in \\mathcal{Y}(z)} \\prod_{i=1}^{n} \\psi_{i}\\left(y_{i-1}^{\\prime}, y_{i}^{\\prime}, z\\right)} \\] 其中，\\(\\psi_{i}\\left(y_{i-1}, y_{i}, z\\right) = \\exp \\left(W_{y^{\\prime}, y}^{T} z_{i}+b_{y^{\\prime}, y} r\\right)\\) ，而 \\(W_{y^{\\prime}, y}^{T}\\) 与 \\(b_{y^{\\prime}, y}\\) 分布为权重向量和对应标签对的偏移。 对于CRF训练，我们使用最大条件似然估计： \\[ L(W, {b})=\\sum_{i} \\log p(y | {z} ; {W}, {b}) \\] 只要最大化 \\(L(W, {b})\\) 即可。 而解码过程就是搜索具有最高条件概率的标签序列 \\(y^*\\)： \\[ {y}^{*}=\\underset{y \\in \\mathcal{Y}({z})}{\\operatorname{argmax}}\\; p({y} | {z} ; {W}, {b}) \\] 3 神经机器学习 早期算法HMM，CRF等严重依赖手工制作的特性和特定于任务的资源。这种特定于任务的知识开发成本很高，使得序列标记模型难以适应新任务或新领域。近年来，作为输入分布式词表示的非线性神经网络，也称为词嵌入，在NLP问题中得到了广泛的应用，并取得了很大的成功。最近，递归神经网络(RNN) (Goller and Kuchler, 1996) 及其变体，如长短时记忆(LSTM) (Hochreiter and Schmidhuber, 1997)和门控循环神经单元 (GRU) (Cho et al., 2014)在序列数据建模方面取得了巨大的成功。 3.1 LSTM 递归神经网络(RNNs)是一个强大的连接主义模型家族，它通过图中的周期来捕捉时间动态。虽然在理论上，RNNs能够捕获远程依赖关系。但在实践中，由于梯度消失/爆炸问题，基本上失败了。 LSTMs (Hochreiter and Schmidhuber, 1997) 是RNNs的变体，用于处理这些梯度消失问题。基本上，LSTM单元由三个乘法门组成，它们控制信息的比例，以便遗忘和传递到下一个时间步骤。 形式上，LSTM单元在 \\(t\\) 时间步更新的公式为: \\[ \\begin{aligned} \\mathbf{i}_{t} &amp;=\\sigma\\left(\\boldsymbol{W}_{i} \\mathbf{h}_{t-1}+\\boldsymbol{U}_{i \\mathbf{x}_{t}}+\\boldsymbol{b}_{i}\\right) \\\\ \\mathbf{f}_{t} &amp;=\\sigma\\left(\\boldsymbol{W}_{f} \\mathbf{h}_{t-1}+\\boldsymbol{U}_{f} \\mathbf{x}_{t}+\\boldsymbol{b}_{f}\\right) \\\\ \\tilde{\\mathbf{c}}_{t} &amp;=\\tanh \\left(\\boldsymbol{W}_{c-1} \\mathbf{h}_{t-1}+\\boldsymbol{U}_{c} \\mathbf{x}_{t}+\\boldsymbol{b}_{c}\\right) \\\\ \\mathbf{c}_{t} &amp;=\\mathbf{f}_{t} \\odot \\mathbf{c}_{t-1}+\\mathbf{i}_{t} \\odot \\tilde{\\mathbf{c}}_{t} \\\\ \\mathbf{o}_{t} &amp;=\\sigma\\left(\\boldsymbol{W}_{o} \\mathbf{h}_{t-1}+\\boldsymbol{U}_{o} \\mathbf{x}_{t}+\\boldsymbol{b}_{o}\\right) \\\\ \\mathbf{h}_{t} &amp;=\\mathbf{o}_{t} \\odot \\tanh \\left(\\mathbf{c}_{t}\\right) \\end{aligned} \\] 其中 \\(\\sigma\\) 表示 sigmoid 函数，\\(\\odot\\) 表示点积，\\(x_t\\) 是在 \\(t\\) 时间步的输入向量，\\(h_t\\) 是隐藏状态向量，包含了当前及前面所有时间步的信息。\\(U_i, U_f, U_c, U_o\\) 表示不同门的输入权重矩阵。而\\(W_i, W_f, W_c, W_o\\)表示不同门的隐藏状态权重矩阵，\\(b_i, b_f, b_c, b_o\\) 表示偏移向量。 3.1.1 双向 LSTM (BLSTM) 对于许多序列标记任务，访问过去(左)和未来都是有益的。然而，单向LSTM的隐藏状态\\(h_t\\)只从过去获取信息，对未来一无所知。一个优雅的解决方案是双向LSTM，其有效性已经被之前的工作证明 (Chiu et al., 2016)。其通过向前和向后传播的两个隐藏状态，分别捕获过去和未来的信息，然后将这两个隐藏状态连接起来，形成最终的输出。 3.2 用于字符级表示的CNN 卷积神经网络(CNN)是一种从单词字符中提取形态学信息(如单词的前缀或后缀)并将其编码成神经表示的有效方法 (Ma, &amp; Hovy, 2016)。Figure 1 展现了用CNN提取给定单词的字符级表示。 Figure 1: 卷积神经网络用于提取单词的字符级表示。 虚线箭头表示在向CNN输入字符嵌入之前应用了一个dropout层 用CNN计算出每个单词的字符级表示，然后字符级向量表示连接词级向量组成最终的输入向量。 4 训练与评估 词嵌入。 常使用斯坦福大学公开的GloVe 嵌入式系统，该系统训练了来自维基百科和网络文本的60亿个单词 (Pennington et al., 2014)。 数据集。 常使用宾夕法尼亚大学的《华尔街日报》部分 Treebank (PTB) (Marcus et al., 1993)，其中包含45个不同的POS标签。并分为训练集和测试集。而准确度标准采用F1分数。 Mocel Acc. Bi-LSTM (Plank et al., 2016) 97.22 Feed Forward (Vaswani et a. 2016) 97.4 NCRF++ (Yang and Zhang, 2018) 97.49 LSTM-CNNs-CRF (Ma and Hovy, 2016) 97.55 Adversarial Bi-LSTM (Yasunaga et al., 2018) 97.59 Meta BiLSTM (Bohnet et al., 2018) 97.96 Table 1: 近几年的各种模型的测试结果。 5 目前与未来研究 过去以及目前，很多模型都字符级或词级的词嵌入来改善模型性能，还有一些是通过优化语言模型来助力提高词性标注的准确性。未来，一种可能是会通过预训练的强大语言模型或词嵌入来继续提高词性标注性能；二是提出一种新型的网络模型来产生突破性的成果。 参考 Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Goller, C., &amp; Kuchler, A. (1996, June). Learning task-dependent distributed representations by backpropagation through structure. In Proceedings of International Conference on Neural Networks (ICNN'96) (Vol. 1, pp. 347-352). IEEE. Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780. Cho, K., Van Merriënboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259. Chiu, J. P., &amp; Nichols, E. (2016). Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics, 4, 357-370. Ma, X., &amp; Hovy, E. (2016). End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354. Pennington, J., Socher, R., &amp; Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543). Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: The Penn Treebank. Plank, B., Søgaard, A., &amp; Goldberg, Y. (2016). Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss. arXiv preprint arXiv:1604.05529. Vaswani, A., Bisk, Y., Sagae, K., &amp; Musa, R. (2016). Supertagging with lstms. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 232-237). Yang, J., &amp; Zhang, Y. (2018). Ncrf++: An open-source neural sequence labeling toolkit. arXiv preprint arXiv:1806.05626. Yasunaga, M., Kasai, J., &amp; Radev, D. (2017). Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint arXiv:1711.04903. Bohnet, B., McDonald, R., Simoes, G., Andor, D., Pitler, E., &amp; Maynez, J. (2018). Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings. arXiv preprint arXiv:1805.08237.","link":"/post/pos-review/"},{"title":"Raft 简述","text":"Raft 是一种分布式一致性算法，详细可见论文。主要作用是确保分布式系统的一致性问题。考虑主从模式的集群下，服务器挂掉或网络故障等情况发生时如何确保数据安全。在raft中，主节点称为leader，从节点称为follower。 下面我尝试以简洁流畅的语言描述raft算法的各个重要部分，某些地方会结合etcd/raft的源码说明。也可以结合raft.github.io这个网站的可视化工具来理解raft。 节点感知 leader和follower需要相互知道的对方存在，一般采用发心跳信息的方式。在raft中，一定是leader主动发心跳信息给follower，然后follower回应。 日志复制 日志复制也叫日志同步或日志分发，之所以叫日志复制是因为总是leader单方面将日志复制到followers上。raft尽可能保证leader和followers的日志进度相同，自动进行着日志的同步。每条日志有一个 log index 参数，严格递增，可以通过该参数判断该follower的同步进度。 可能新增日志的速度快于同步速度，为了确保顺序，会等旧日志同步完后再同步新日志。实现方法可能是，leader与各个follower之间维护一个队列，leader往队列投入日志，followers从队列获取日志。 日志提交 当一个日志同步给超过半数的节点后，leader就可以提交该日志。所谓提交在不同分布式场景中可能有不同的具体表现。比如，在分布式存储中，leader把数据复制到其他节点后并非就真的存储完成，可能只是把数据放到了内存上，只有超过半数节点拥有这一数据才肯定该数据是安全的，然后leader执行提交，通知其他所有节点可以把该数据存储到磁盘上。 follower故障 如果leader给follower发送的心跳信息长时间没有回应，可能有两种情况，一是follower挂了，二是网络分区(网络连接失败)。如果是前者，则该follower是不可用的，如果是后者，可能过段时间网络恢复后又会重新连上来。但leader无法分辨是哪种情况，一般措施是依旧把follower维持在集群中，并不断发送消息，尽管不能得到回复。只要集群中有过半节点正常运行，集群就是可用状态。 领导选举 leader并非万无一失，也可能会发生故障而挂掉。需要选举出新的leader，每选举出一个leader，每个节点上维护的任期号term加一。 当某个follower持续一定时间没有收到leader的心跳信息，该follower就会认为leader挂掉了，这时他会把自己变成candidate，先把自己的任期号term加一，给自己投一票后广播信息请求其他节点给它投票，信息会携带自己的term和log index。 其他节点收到信息后，先看看自己能不能投，可以重复投给一个节点，但投过一个后就不能投给另一个。可以投票的话就先比较term，如果对方term更大，就投票给它，如果term相同且对方log index更大，同样投票给它，否则拒绝投票 [src]。拒绝投票意味着对方没有资格参与选举，因为它的term或log index落后于人。拒绝投票消息会携带当前term，对方收到消息后就会把自己的term更新为消息中的term，然后把自己变回follower [src]。 如果请求投票期间，candidate收到leader的心跳，也会把自己变回follower [src]。 当一个candidate收到半数以上的投票，它就把自己变成leader并向其他节点发信息宣告这一消息。当然，也可能选举失败，比如若干个candidate拿到相同的票数，并且没有多余的票数，超时之后将自己的term加一并继续新一轮选举。 若是前一个leader在分发某一个日志过程中挂掉了怎么办？如果这条日志还没来得及分发到任何一个节点上，leader就已经挂掉，那么这条日志必然随着leader的挂掉而丢失。如果成功分发到一个节点及以上，那么新leader一定是具有该日志的，否则log index落后一定不会成为leader。新leader发现该日志还没提交，就会分发该日志。与之前任期重复分发了也没关系，同步时follower发现已存在log index，则以leader新发送的为准强制覆盖。 多数派 当一个candidate收到半数以上（多数派）的投票，它就把自己变成leader。但若是集群中过半节点都挂掉了，则意味着candidate永远无法获得过半数的投票，但它并不知道集群其他节点什么时候能恢复，选举超时后，它会继续发起选举。选举期间集群不可用，或者说不可对外服务，所以也就意味着集群中过半节点挂掉，则集群将不可用。 成员变更 当新加入节点或减少节点，需要将新的集群配置表分发到各个节点上，这一行为与其他日志分发并无不同。如果在分发过程中，leader挂掉了，那么可能发送一种情况是：具有旧集群配置表的节点们会选出一个leader，而新集群配置表的节点们也可能选举出一个leader，最终造成一个集群两个leader。 解决办法是每次只变更一个节点。事实证明，每次只变更一个节点不会产生分裂的情况，因为不会出现同时有两个超过半数以上子集群的存在。当添加一个节点，leader把变更日志分发到其他节点，假设leader出现了崩溃，重新选举。这个时候，会有2个结果： （1）新配置复制到了集群的大多数并提交，如果leader挂掉，新配置范围内至少有一个节点，它是从旧配置变为新配置，它与新节点和旧节点都相互感知，并且它的log index领先于旧节点，它能同时得到新节点和旧节点的选票，所以肯定能在新一轮选举中胜出，新leader肯定使用的是新的配置。 需要注意的是，要确保leader挂掉后以旧节点视角来看必须还有过半节点存活，否则旧节点将认为集群不可用，这种情况一般出现在集群原本只有2个节点，然后新增1个节点。 （2）新配置没有复制到集群的大多数或还未提交，实际上新配置并没有被提交，所有节点用的还是旧配置，旧节点还未感知新节点，所以新leader必然还是从旧节点当中选出，接收到变更日志的节点具有更大log index，新leader会在它们中选出，选出后新leader继续同步变更日志，接着上一轮还没进行完的成员变更过程。 可以参考论文中的图： change-single-server 如果同时加入超过一个节点，可能新节点加leader就超过了半数，很可能日志只同步到新节点就提交了，若leader挂掉，则新配置范围和旧配置范围不存在相互感知的节点，新节点知道旧节点，但旧节点不知道新节点。如果新节点数量少于半数，则新节点范围的集群会陷入不可用状态，而旧节点照常选举。如果新节点超过半数，则导致新节点范围自己选举，旧节点范围也自己举行选举，导致脑裂。 加入新节点 当加入新节点时，需要告诉leader，有新节点加入，但新节点不会马上加入集群的运作当中。leader会先向改节点同步数据，分多轮同步，直到新节点的数据追上集群的进度才加入集群。也就是说上面说到的成员变更加入新节点的过程一定是新节点进度追上集群后才发生的。通常，新节点追进度时会被称为learner。 既然日志同步是自动进行的，为什么还要等新节点的进度追上来后才加入集群？因为新节点必须把过往日志都同步才能确认log index更大的新日志。如果不设置learner阶段，若是刚加入一个新节点，就有一个节点挂掉了，而新加入节点的log index没有跟上，它长时间无法确认新日志，相当于同时有两个节点不可用，如果集群当前有4个节点，两个不可用，整个集群就不可用了。 删除节点和网络分区 一个节点被移除后，它自己可能不知道这个情况，它发现没有leader的心跳，自己就会发起选举。如果它的日志不是最新的，它总是无法获得投票，但是又没有leader，所以隔一段时间又再次发起投票，以此类推，老是发起选举会干扰到集群；如果它的日志是最新的，会导致集群节点给它投票，进而可能让一个被移除的节点成为leader。 为了防止被移除的节点成为leader，每个节点在投票之前需要检查自己在一定时间内有没有收到心跳，如果又收到心跳，说明leader还存活，没必要给别人投票。然而因为被移除节点无法知道自己已经被移除，所以还可以发起选举，虽然不会成为leader，但一直持续下去也不好，最好在通知leader移除节点后把该节点对应的进程停掉。其实可以考虑加一种移除消息类型，当节点收到该消息后把自己停掉就好了。 网络分区类似，在节点通信故障后又重新连回集群后，该节点在多次发起选举后term可能变得很大，收到leader的心跳后发现自己的term比leader的更大，就回应一个消息 [src]，leader发现对方term比自己大，就把自己变成follower [src]，等待一段时间后触发新一轮选举。 解决网络分区term过大的问题的办法是PreVote，即预投票。Candidate先发起一次预投票，若是自己能获得大多数节点的投票就发起一次正式投票，如果不能则不能增加term。","link":"/post/raft/"},{"title":"Red black tree implementation","text":"Red black tree is a data structure for searching data and can self-balance after modification. I will try to implement it and log in this article. You can find the source code in here. Node Evey Node have five member properties. color_ represent the color of node, red or black. 123456789101112enum RbTreeColor &#123; rb_red = false, rb_black = true &#125;;template &lt;typename T&gt;struct RBNode &#123; typedef RBNode&lt;T&gt;* SelfPtr; typedef const RBNode&lt;T&gt;* ConstSelfPtr; RbTreeColor color_; SelfPtr parent_; SelfPtr left_; SelfPtr right_; T value_; Iterator function increment() find the last node that is larger than current node, corresponding operator++. function decrement() find the last node that is smaller than current node, corresponding operator--. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061template &lt;typename T, typename Ref, typename Ptr&gt;struct RBTreeIterator &#123; // ... typedef RBTreeIterator&lt;T, Ref, Ptr&gt; Self; typedef RBNode&lt;T&gt;* NodePtr; NodePtr node_; explicit RBTreeIterator() = default; explicit RBTreeIterator(NodePtr x): node_(x) &#123;&#125; void increment() &#123; if(node_-&gt;right_ != 0) &#123; node_ = node_-&gt;right_; while(node_-&gt;left_ != 0) &#123; node_ = node_-&gt;left_; &#125; &#125; else &#123; NodePtr y = node_-&gt;parent_; while (node_ == y-&gt;right_) &#123; node_ = y; y = y-&gt;parent_; &#125; if(node_-&gt;right_ != y) &#123; node_ = y; &#125; &#125; &#125; void decrement() &#123; if(node_-&gt;color_ == rb_red &amp;&amp; node_-&gt;parent_-&gt;parent_ == node_) &#123; node_ = node_-&gt;right_; &#125; else if (node_-&gt;left_ != 0) &#123; NodePtr y = node_-&gt;left_; while (y-&gt;right_ != 0)&#123; y = y-&gt;right_; &#125; node_ = y; &#125; else &#123; NodePtr y = node_-&gt;parent_; while (node_ == y-&gt;left_) &#123; node_ = y; y = y-&gt;parent_; &#125; node_ = y; &#125; &#125; reference operator*() const &#123; return node_-&gt;value_; &#125; pointer operator-&gt;() const &#123; return &amp;(node_-&gt;value_); &#125; Self&amp; operator++() &#123; increment(); return *this; &#125; Self&amp; operator--() &#123; decrement(); return *this; &#125; // ... Tree Property header rb-1.jpg parent: link to root if exist left: also call leftmost, link to the most left node of tree right: also call leftmost, link to the most right node of tree color: default is red root rb-2.jpg parent: link to header left: link to nullptr if no node right: link to nullptr if no node color: always is black Multi Node See a tree with multi nodes. The children of leaf are nullptr. rb-3.jpg Rotation Left Rotation When the right sub-tree is taller, we need to rotate left to shorten it. 1234567891011121314151617181920212223242526/* * x y * / \\ / \\ * u y =&gt; x b * / \\ / \\ * a b u a * */inline void _rotate_left(NodePtr x, NodePtr&amp; root) &#123; NodePtr y = x-&gt;right_; x-&gt;right_ = y-&gt;left_; if(y-&gt;left_ != 0) &#123; y-&gt;left_-&gt;parent_ = x; &#125; y-&gt;parent_ = x-&gt;parent_; if(x == root) &#123; root = y; &#125; else if(x-&gt;parent_-&gt;right_ == x) &#123; x-&gt;parent_-&gt;right_ = y; &#125; else &#123; x-&gt;parent_-&gt;left_ = y; &#125; y-&gt;left_ = x; x-&gt;parent_ = y;&#125; Right Rotation When the left sub-tree is taller, we need to rotate right to shorten it. 12345678910111213141516171819202122232425/* * x y * / \\ / \\ * y u =&gt; a x * / \\ / \\ * a b b u */inline void _rotate_right(NodePtr x, NodePtr&amp; root) &#123; NodePtr y = x-&gt;left_; x-&gt;left_ = y-&gt;right_; if(y-&gt;right_ != 0) &#123; y-&gt;right_-&gt;parent_ = x; &#125; y-&gt;parent_ = x-&gt;parent_; if(x == root) &#123; root = y; &#125; else if(x-&gt;parent_-&gt;right_ == x) &#123; x-&gt;parent_-&gt;right_ = y; &#125; else &#123; x-&gt;parent_-&gt;left_ = y; &#125; y-&gt;right_ = x; x-&gt;parent_ = y;&#125; Rule and Balance The color of node is eithor red or black. The root node always is black. If a node is red, its children must be black. Every path of every sub-tree have the same number of black nodes. As the rule 4, we can ensure the different of the height of the tallest sub-tree and that of the shortest sub-tree cannot over 2 times. The color is the measure of the balance for red black tree, as the height difference for the AVL tree. Insertion We insert a not-allow-replaced key with three considerations: insert to the left of leftmost insert to the right of y if just bigger than y insert to the left of y if just bigger than decrement(y) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061pair&lt;iterator, bool&gt; insertUnique(const Val &amp;v) &#123; NodePtr y = header_; NodePtr x = root(); bool comp = true; while(x != nullptr) &#123; y = x; comp = keyCompare_(KeyOfValue()(v), key(x)); x = comp ? left(x) : right(x); &#125; // while end, y will be the parent of the node to insert iterator j = iterator(y); if (comp) &#123; // comp is true, insert to left if (j == begin()) &#123; return pair&lt;iterator, bool&gt;(_insert(x, y, v), true); &#125; else &#123; // if j is not the leftmost --j; // need to --j, because KeyOfValue(v) probably equal to --j, // it need to be bigger than --j as the following comparsion. &#125; &#125; if (keyCompare_(key(j.node_), KeyOfValue()(v))) &#123; return pair&lt;iterator, bool&gt;(_insert(x, y, v), true); &#125; return pair&lt;iterator, bool&gt;(j, false);&#125;// x is the position need to be inserted// y is the parent of x// v is the new valueiterator _insert(NodePtr x, NodePtr y, const Val&amp; v) &#123; NodePtr z; if(y == header_ || x != nullptr || keyCompare_(KeyOfValue()(v), key(y))) &#123; // insert to left of y z = createNode(v); left(y) = z; if(y == header_) &#123; root() = z; rightmost() = z; &#125; else if (y == leftmost()) &#123; leftmost() = z; &#125; &#125; else &#123; z = createNode(v); right(y) = z; if(y == rightmost()) &#123; rightmost() = z; &#125; &#125; parent(z) = y; left(z) = nullptr; right(z) = nullptr; // rebalance the tree _rebalance(z, root()); ++nodeCount_; return iterator(z);&#125; Inside and Outside For convenient, we define the inside and outside as shown blow. If the cur node direction is not equal to the parent direction, we call it inside, or else we call it outside. 12345678910111213/* * v * / \\ * a u * / \\ a is a left node * b c b is outside, c is inside * * v * / \\ * u a * / \\ a is a right node * c b b is outside, c is inside */ ReBalance Initially, x is new inserted node. x is red default. When its parent is red, which break the rule 3. If its parent is black, it do not breaks any rules, which does not need to rebalance. You need to consider four situations: - First, we need to think about the parent of x is either a left or a right, which determines the rotation direction. - And then, consider the uncle of x is either red or black, which determines how to change the color. In the process of percolation and rebalance, the parent of x initially is red, when percolate up, we change its color. How to change is determines to its sibling (uncle of x), which we need to make the color as same as its sibling. If uncle is red, the grandparent must be black, all of you need to do is to change the color of parent and uncle to black, and grandparent's color to red, for following the rule 3. But these probably break the rules in higher position, so we need to percolate up to the grandparent. After percolate up, if you find the parent is black, you can stop the rebalance, because the number of black have not changed and followed the rule 3 as above step. But if the parent is red, two continuous red node break the rule 3 again. And its uncle must be black or null. Looking at the following steps. If uncle is black or null, If cur node is in outside, change parent's color to black and grandpa's color to red. These will make the number of black nodes in its parent sub-tree increase 1 and greater than that of the uncle sub-tree, so we do a rotation to make the parent move to the grandpa position to make anthor direction sub-tree have the same number of black nodes. The rotation direction is opposite to its direction to its parent. If cur node is in inside, we need to do a rotate firstly (the rotation direction is same to its direction to its parent) to make the cur node becomes the parent node, and the original parent node becomes the cur node in the outside, as same as the situation above, so do the same thing as above. (total two rotations) Finally, making the root's color to black, these action do not break any rule. 12345678910111213141516171819202122232425262728293031323334353637383940414243inline void _rebalance(NodePtr x, NodePtr&amp; root) &#123; x-&gt;color_ = rb_red; while(x != root &amp;&amp; x-&gt;parent_-&gt;color_ == rb_red) &#123; if(x-&gt;parent_ == x-&gt;parent_-&gt;parent_-&gt;left_) &#123; // parent is a left node NodePtr y = x-&gt;parent_-&gt;parent_-&gt;right_; // consider uncle node if(y &amp;&amp; y-&gt;color_ == rb_red) &#123; // uncle is red // parent and uncle change to black x-&gt;parent_-&gt;color_ = rb_black; y-&gt;color_ = rb_black; // grandparent change to red x-&gt;parent_-&gt;parent_-&gt;color_ = rb_red; x = x-&gt;parent_-&gt;parent_; // go up &#125; else &#123; // uncle is black or is null if(x == x-&gt;parent_-&gt;right_) &#123; // x is inside x = x-&gt;parent_; _rotate_left(x, root); // rotate left &#125; // parent's color become as same as uncle's x-&gt;parent_-&gt;color_ = rb_black; x-&gt;parent_-&gt;parent_-&gt;color_ = rb_red; _rotate_right(x-&gt;parent_-&gt;parent_, root); // rotate right &#125; &#125; else &#123; // parent is a right node NodePtr y = x-&gt;parent_-&gt;parent_-&gt;left_; // uncle if(y &amp;&amp; y-&gt;color_ == rb_red) &#123; // uncle is red // parent and uncle change to black x-&gt;parent_-&gt;color_ = rb_black; y-&gt;color_ = rb_black; x-&gt;parent_-&gt;parent_-&gt;color_ = rb_red; &#125; else &#123; // uncle is black or is null if(x == x-&gt;parent_-&gt;left_) &#123; // x is inside x = x-&gt;parent_; _rotate_right(x, root); // rotate left &#125; x-&gt;parent_-&gt;color_ = rb_black; x-&gt;parent_-&gt;parent_-&gt;color_ = rb_red; _rotate_left(x-&gt;parent_-&gt;parent_, root); &#125; &#125; &#125; // ensure root is black root-&gt;color_ = rb_black;&#125;","link":"/post/rb-tree/"},{"title":"循环神经机器翻译","text":"编码和解码 我们把源句子（source sentence）表示为 \\(\\large (x_1, x_2, \\dots, x_n) \\in x\\), 目标句子（target sentence）表示为 \\(\\large (y_1, y_2, \\dots, y_n) \\in y\\)。 使用编码网路（encoder）对源句子进行编码，使用解码网络（decoder）对源句子的编码进行解码，解码出预测句子。同时，作为监督，把目标句子作为解码网络的输入。 本文主要讲使用循环神经网络构建encoder-decoder模型。 源句子的最大序列长度为 \\(\\large T\\), 目标句子的最大序列长度为 \\(\\large T&#39;\\)，批次大小为 \\(\\large B\\)，词向量大小为 \\(\\large E\\)，RNN的隐藏层大小为 \\(\\large H\\), 源语言词汇数量为 \\(\\large V\\)，目标语言词汇数量为 \\(\\large V&#39;\\)。 一个batch即一个句子，把句子填充到长度为 \\(\\large T\\)， 依次输入一个词，经过词向量 \\(\\large X \\in \\mathbb{R}^{T \\times B \\times E}\\)。 encoder 一次性输入整个源句子 \\(\\large X\\) 和隐藏状态 \\(\\large h \\in \\mathbb{R}^{T \\times B \\times H}\\) encoder 输出 \\(\\large X \\in \\mathbb{R}^{T \\times B \\times H}\\) 和 \\(\\large h \\in \\mathbb{R}^{T \\times B \\times H}\\) decoder 把时间步拆开，每一时间步的输入为上一时间步的隐藏状态和预测词，除了预测词，也可以用目标句子中的词，一个时间步输入数据为 \\(\\large X_i \\in \\mathbb{R}^{1 \\times B \\times E}\\) 和上一时间步的隐藏状态 \\(\\large h \\in \\mathbb{R}^{1 \\times B \\times H}\\) decoder 一个时间步的输出为 $O ^{1 B H} $ 和隐藏状态 \\(\\large h\\)。然后再下一个词向量 \\(\\large X\\) 和 \\(\\large h\\) 输入到下一时间步，执行第4步，直到句子末尾。 我们把decoder每一时间步的输出收集起来 \\(\\large Y \\in \\mathbb{R}^{T&#39; \\times B \\times E}\\) decoder的输出要用于softmax函数，所以每一时间步的输出通过一个线性层 $W ^{H V'} $，则最后的输出 $Y ^{T' B V'} $ 为了方便计算，把decoder的输出压缩成二维 $Y ^{(T' * B) V'} $ ，并使用softmax交叉熵计算损失值 反向传播并梯度更新 seq2seq网络结构如图所示： 因为我们是按批次计算，一个批次中的句子有长有短，为了统一长度，会对所有句子填充 \\(\\large \\langle pad \\rangle\\)。而句子尾部会填充 \\(\\large \\langle eos \\rangle\\) 表示结束，目标句子会在句首加一个\\(\\large \\langle sos \\rangle\\)，通过这个起始标志输出第一个词汇。 目标句子包含 \\(\\large \\langle eos \\rangle\\) , \\(\\large \\langle sos \\rangle\\) 和 \\(\\large \\langle pad \\rangle\\) 的序列长度是 \\(\\large T&#39;\\)， \\[ \\large \\langle sos \\rangle, I, love, you,\\large \\langle eos \\rangle, \\large \\langle pad \\rangle, \\dots, \\large \\langle pad \\rangle \\] 但因为decoder以 \\(\\large \\langle sos \\rangle\\) 为起点，预测输出的是不包含 \\(\\large \\langle sos \\rangle\\) 的，所以计算交叉熵时目标句子应该去掉 \\(\\large \\langle sos \\rangle\\) ，长度变为 \\(\\large T&#39;-1\\)， 但是decoder输出长度为 \\(\\large T&#39;\\)，长度不一致。这时注意到，句子末尾是一堆无意义的 \\(\\large \\langle pad \\rangle\\) ，预测出来的同样是无意义的，所以我们可以把decoder输入的最后一个去掉，这样序列长度就一致了。 注意力机制（Attention Mechanism） 注意力机制有两个作用： 其一，是区分哪部分是重要的。当你听到句子“the ball is on the ﬁeld”，你不会认为这 6 个单词都一样重要。你首先会注意到单词“ball”，“on” 和 “ﬁeld”，因为这些单词你是觉得最“重要”的。类似，Bahdanau 等人注意到使用 RNN 的最终状态作为 Seq2Seq 模型的单个“上下文向量”的缺点：一般对于输入的不同部分具有不同的重要程度。再者，此外，输出的不同部分甚至可以考虑输入的不同部分“重要”。 其二，是具有更长时间的记忆力。你会注意到，当你通过编码网络输出特征向量后，解码阶段就只是根据这个向量来解码，而不再有其他信息。如果我们可以持续从编码网络获取信息，那么我们的预测不是更准确一些吗？就好像人类翻译员，他们肯定也不是只看一眼原句，就能翻译出来，而是对照着原句一个一个翻译。 注意力机制的基本方法就是在解码阶段获取编码阶段的信息。 全局注意力（Global Attention） 所谓全局注意力，即把encoder的所有输出的信息参加到decoder每一时间步的输出的计算中。简单的说就是，当我们预测每一个词时，我们会参考源句子中每一个词，每个源词汇都会得到一个注意力得分（score）。 为了计算注意力，我们把decoder的每一时间步拆开计算。 假设当前时间步的输出为 \\(\\large y \\in \\mathbb{R}^{1 \\times B \\times H}\\)，而encoder的输出为 $O ^{T B H} $。则得分函数可以有以下三种， \\[ score(y, O) = \\begin{cases} y^TO \\\\ (y^TW)^TO \\\\ v^T tanh(W[y;O]) \\end{cases} \\] （一般用第二种） 为了得到得分矩阵，我们计算时会进行转置，\\(\\large y \\in \\mathbb{R}^{B \\times 1 \\times H}\\)， $O ^{B H T} $，矩阵相乘后得到注意力得分 \\(\\large a \\in \\mathbb{R}^{B \\times 1 \\times T}\\)，一般我们还会对它用softmax计算。 我们需要得到一个上下文向量 \\(\\large a \\times O \\rightarrow context \\in \\mathbb{R}^{B \\times 1 \\times H}\\) 把上下文向量与decoder时间步的输出相连，并经过线性层输出注意力向量 \\(\\large concat(context, y)^T W \\rightarrow attn \\in \\mathbb{R}^{B \\times 1 \\times H}\\) 把中间维度去掉后，经过线性层输出 $y ^{B V'} $ 当decoder的 \\(\\large T&#39;\\) 个时间步算完后，我们就能完整的输出序列 \\(\\large Y \\in \\mathbb{R}^{T&#39; \\times B \\times V&#39;}\\) ，以及所有的注意力得分 \\(\\large a\\) 组成了对齐矩阵（alignment matrix）\\(\\large align \\in \\mathbb{R}^{B \\times T&#39; \\times T}\\) 对齐矩阵如图所示： 这个表是将源句子中的单词映射到目标句子中的相应单词，越深色表示相关性越大。 全局注意力网络结构： Input-Feeding Luong et al. (2015) 中提出了Input-Feeding模型 [1]，将注意力向量 \\(\\large attn\\) 和目标词向量连接后作为decoder RNN的输入，即每一时间步的输入为 \\(\\large y \\in \\mathbb{R}^{B \\times 1 \\times (E+H)}\\)。 Teacher Forcing 所谓Teacher Forcing就是对每一时间步的输入使用目标句子中正确的词汇，而不是上一时间步输出的预测词。 Scheduled Sampling Scheduled Sampling 直译“计划抽样” [2]，是介于Teacher Forcing和Without Teacher Forcing之间的方法。我们设置一个使用Teacher Forcing的概率分布，对于每一时间步，我们使用该概率分布来绝对是否使用Teacher Forcing。若使用，我们就输入目标句子中的词汇，否则输入上一时间步的预测词。 论文中的方案是每一时间步就抛硬币选择一次，但实际训练中为了统一，一般是一个batch选择一次。 评估 我们对机器翻译的评估通常会用BLEU算法 [3]。它是一种比较参考翻译（Reference），即人工翻译，与机器翻译的得分算法。 我们对参考句子和预测句子取n元组（n-grams）, 机器翻译结果为 \\(\\large \\hat y\\), 得分为： \\[ P_n = \\frac{\\sum_{ngrams \\in \\hat y}Count_{ref}(ngrams)}{\\sum_{ngrams \\in \\hat y}Count_{MT}(ngrams)} \\] 而BLEU得分的定义为，我们将1至n元组的得分求和再求均值，然后用指数函数放大， \\[ BLEU = \\beta \\; exp({\\frac{1}{n}\\sum_{i=1}^n P_i}) \\] 其中 \\(\\large \\beta\\) 是惩罚因子, 也叫简短惩罚（brevity penalty）。如果你输出了一个非常短的翻译，那么它会更容易得到一个高精确度。因为输出的大部分词可能都出现在参考之中，这时就要对得分进行惩罚。 假设，参考翻译的长度为 \\(\\large r\\), 若有多个句子就取平均值。机器翻译句子长度为 \\(\\large m\\)。 \\[ \\beta = \\begin{cases} 1 &amp; if \\, m &gt; r \\\\ exp(1-\\frac{r}{m}) &amp; otherwise \\end{cases} \\] 如果机器翻译句子大于参考翻译句子长度，我们就不进行惩罚；若小于，\\(\\large 1-\\frac{r}{m}\\) 为负数, 而指数函数会得到一个小数，使得bleu得分降低。 束搜索 （Beam Search） 你也许会直觉的认为，解码器就是直接将每个时间步的输出通过softmax找到最可能的那个词汇即可，这也叫贪心查找（Greedy Search）。但在这只是考虑到一个词的最优选择，而不是整体最优选择。 如图，一句话可能会有成多种翻译版本 如果按照局部最优的选择，那可能会输出第一句。如果按照整体最优，那可能是第三句。而实际上，我们应该考虑整体最优。 \\[ \\mathop{argmax}_{y_1,y_2,...,y_T}\\,P(y_1,y_2,...,y_T|x) \\] 目前比较好的方法是束搜索（Beam search）[4]。 定义一个束宽 \\(\\large b\\)，每个序列得分函数为 \\(\\large F(w)\\)。 第一个时间步输出时，我们会从词汇表 \\(\\large V\\) 中找 \\(\\large b\\) 个得分 \\(\\large F(w)\\) 最大的词汇，加入到对应的序列当中。假设 \\(\\large b = 3\\)，那么就有3个候选序列 \\(\\large s_1(w_1), s_2(w_2), s_3(w_3)\\) 对于第一个时间步的每一个候选，依次输入到第二个时间步中，找得分最大的 \\(\\large b\\) 个词汇分别加入到该候选序列中，由一个候选序列变成 \\(\\large b​\\) 个候选序列 \\[ s(w_1,w_4), s(w_1, w_5), s(w_1, w_6) \\] 总共可以得到 \\(\\large b^2\\) 个候选序列 \\(\\large s_1^1, s_1^2, s_1^3, s_2^1, s_2^2, \\dots\\) 。假设序列长度为 \\(\\large L\\) ，一个序列的总得分为每个词的得分之和 $F(s) = _i^ L F(w_i) $，则该序列的平均得分为 \\(\\large score = F(s) / L\\)。我们保留 \\(\\large b\\) 个平均得分最高的序列，进入下一时间步。 往后重复类似于第2步的操作，当其中一个序列的最后一个词为 \\(\\large \\langle eos \\rangle\\) 时，该序列就停止搜索，直到所有序列都停止或者到达指定的最大长度为止。 最后从 \\(\\large b\\) 个序列选取平均得分最高的序列。 对于得分函数，我们一般使用 \\[ F=log(softmax(Y)) \\] 束搜索一般用在测试阶段。 卷积机器翻译 使用循环神经网络训练的感觉就是慢。使用卷积网络进行机器翻译可以实现并行化，大大提高训练效率。这部分我打算放到另一篇文章中讲。 参考文献 [1] Effective Approaches to Attention-based Neural Machine Translation. 2015 [2] Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. 2015 [3] BLEU: a Method for Automatic Evaluation of Machine Translation. 2002 [4] Sequence-to-Sequence Learning as Beam-Search Optimization. 2016","link":"/post/rnn-nmt/"},{"title":"Transformer模型与自注意力","text":"Facebook激进地使用卷积网络处理NLP问题，意外地取得了很不错的效果。而 Google 一不做二不休，发布了一种新型的网络结构，transformer模型 [1]，该网络结构既不使用RNN，也不使用CNN，而且也获得不错的效果。 1. 模型结构 Encoder和Decoder都是从下往上的栈结构。 1.1 Encoder Encoder有6个相同的独立层，每层有2个子层。第一个子层是多头自注意力层（multi-head self-attention），第二个子层是前馈传播层。 1.2 Decoder Decoder同样有6个相同的独立层，每层有3个子层。第一个子层是遮罩了未来信息的多头自注意力层，第二个子层是联合encoder最后一层的输出的多头注意力层，第三个子层是前馈传播层。 下图是当只有1个独立层时候的网络结构 [1]： 1.3 注意力 注意力函数从 Query, Key, Value 映射到一个输出，这里的 Query, Key, Value 是什么可以先不管，先理解注意力模型的构造。 1.3.1 缩放的点积注意力 （Scaled Dot-Product Attention ） 假设 query, key, value 描述为矩阵 \\(\\large Q, K, V\\), query 和 key 的特征维度是 \\(\\large d_k\\)，而 value 的特征维度是 \\(\\large d_v\\)，那么缩放的点积注意力函数为， \\[ Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\] 1.3.2 多头注意力（Multi-Head Attention） 把 query, key, value 从一个大的特征维度映射出多个小特征维度，由 \\(\\large d_{model}\\) 映射到若干个 \\(\\large d_k\\) 或 \\(\\large d_v\\)，假设有 \\(\\large h\\) 个head，那么就有 \\(\\large d_k = d_v = d_{model}/h\\)，每一个 head 进行一次 Scaled Dot-Product Attention，最后再把所有head连接回 \\(\\large d_{model}\\) 维度。 注意力结构如下图： 那么 query, key, value 到底是什么呢？联系上面的模型结构图。 对于encoder来说，第一层输入的 query, key, value 三者皆是源句子的词向量，而后面的层的输入就是上一层的输出。 decoder的第一个子层和encoder相同，三者皆是目标句子的词向量，但是有一个Masked的标识，说明对于decoder来说，我们需要消除该词汇位置往后的参数的影响，消除方法是设置参数为负无穷。 decoder的第二个子层中，query 是第一个子层的输出，key 和 value 是encoder 最后一层的输出。 1.4 前馈传播网络 前馈传播网络包含两个线性层， 计算如下： \\[ FFN(x) = f_2(ReLU(f_1(x))) \\] 1.5 位置编码（Positional Encoding） 一个非RNN的模型无法很好的判断一个词汇在句子中的位置，解决办法是为词向量附加一个位置参数，使得模型能够学习和判断词汇的位置。论文中提出了一个使用余弦和正弦曲线的方法， \\[ PE(pos, 2i)=sin(pos/10000^{2i/d_{model}}) \\\\ PE(pos, 2i+1)=cos(pos/10000^{2i/d_{model}}) \\] 其中 \\(\\large pos\\) 为词在序列的位置，\\(\\large i\\) 为词向量的维度，最后的编码矩阵 \\(\\large PE \\in \\mathbb{R}^{T \\times E}\\)。 假设当前序列长 \\(\\large t\\) ，那么词向量为 \\(\\large emb + PE(t) \\rightarrow emb \\in \\mathbb{R}^{t \\times E}\\) 。 函数对于维度的数值是区分偶数和奇数的，以偶数为例，当位置为1, 5, 10时，位置向量的曲线如图所示， 可以发现，不同位置会进行不同的线性变换，模型会学习到不同位置的固有位置向量，当词向量附加了某一位置向量后，模型就能知道该词在句子中的所在位置。 2. 详细过程 前面大概讲了一下结构和概念，但实际实现模型时还是有很多细节需要弄懂。 源句子的最大序列长度为 \\(\\large T\\), 目标句子的最大序列长度为 \\(\\large T&#39;\\)，批次大小为 \\(\\large B\\)，词向量大小为 \\(\\large E\\)， 源语言词汇数量为 \\(\\large V\\)，目标语言词汇数量为 \\(\\large V&#39;\\)。 2.1 Encoder 计算整个句子的词向量为 \\(\\large dropout(s \\cdot emb(x) + emb\\_pos(x)) \\rightarrow X \\in \\mathbb{R}^{B \\times T \\times E}\\)，\\(\\large s\\) 为 \\(\\large \\sqrt{E}\\) 进入第一子层，记录残差 \\(\\large R \\leftarrow X\\)，标准化 \\(\\large X \\leftarrow Norm(X)\\) 把 \\(\\large X\\) 作为query, key, value，输入自注意力层 \\(\\large Atten(X, X, X) \\rightarrow X \\in \\mathbb{R}^{B \\times T \\times D}\\)，维度 \\(\\large D\\) 即 \\(\\large d_{model}\\) ，和词向量大小 \\(\\large E\\) 是一样的 dropout 正则化后加上残差 \\(\\large X \\leftarrow dropout(X) + R\\) 进入第二子层，记录残差 \\(\\large R \\leftarrow X\\)，标准化 \\(\\large X \\leftarrow Norm(X)\\) 输入到前馈传播网络，再加上残差 \\(\\large X \\leftarrow FFN(X) + R\\) 从步骤 (2) 到 (6) 就结束一个层了，多个层则做同样计算即可。 最终输出为 \\(\\large O \\in \\mathbb{R}^{B \\times T \\times E}\\) 2.2 Decoder Decoder 的第一个子层和第三个子层的步骤和encoder基本相同，这里列出第二子层的过程 记录残差 \\(\\large R \\leftarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times E}\\) [2]，标准化 \\(\\large X \\leftarrow Norm(X)\\) [3] 需要输入到另外一个自注意力层中 \\(\\large Atten(X, O, O) \\rightarrow X\\) dropout 正则化后加上残差 \\(\\large X \\leftarrow dropout(X) + R\\)，再记录残差 \\(\\large R \\leftarrow X\\) 输入到第三子层中 最后一层输出后，通过线性层映射出去 \\(\\large W^TX \\rightarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times V&#39;}\\) 计算softmax交叉熵，反向传播，更新梯度 2.3 Self-Attention \\(\\large d_k = d_v = D / h\\)，则query，key 和 value 分别对应的线性层为 \\(W_q \\in \\mathbb{R}^{D \\times (h\\cdot d)}\\)，\\(W_q \\in \\mathbb{R}^{D \\times (h\\cdot d)}\\) ，\\(W_q \\in \\mathbb{R}^{D \\times (h\\cdot d)}\\) 三个输入参数分别经过线性层后，并把维度转换为 \\(\\large Q, K, V \\in \\mathbb{R}^{B \\times h \\times T \\times d}\\) 矩阵乘法得到相似性得分矩阵 \\(\\large (s \\cdot Q) * K \\rightarrow A \\in \\mathbb{R}^{B \\times h \\times T&#39; \\times T}\\)，其中 \\(\\large s=1/\\sqrt{d}\\) 遮罩计算，\\(\\large A \\leftarrow masked(A)\\)。对于encoder的第一个子层和decoder的第二个子层，把源句子中的边缘位置得分设为负无穷，对于decoder的第一个子层，我们还需要把每一个位置的后面位置的得分设为负无穷。 计算softmax得到归一化的相似性矩阵，\\(\\large A \\leftarrow softmax(A)\\)，正则化 \\(\\large A \\leftarrow dropout(A)\\) 通过矩阵乘法与 value 加权求和，求得注意力得分 \\(\\large A * V \\rightarrow X \\in \\mathbb{R}^{B \\times h \\times T \\times d}\\) 连接 \\(\\large h\\) 个head \\(\\large X \\in \\mathbb{R}^{B \\times T \\times D}\\) 经过一个线性层后输出 \\(\\large W^TX \\rightarrow X \\in \\mathbb{R}^{B \\times T \\times D}\\) 前面提到我们在计算decoder第一子层的注意力时把每一个位置的后面位置的得分设为负无穷，目的是防止未来无用信息的干扰。我们构造该句子的方法是在去边缘的遮罩矩阵基础上，取该矩阵的上三角矩阵，即把相似性矩阵的边缘位置和上三角位置都设为负无穷。 2.4 Feed-Forward Network 构造两个线性层，\\(\\large W_1 \\in \\mathbb{R}^{D \\times F}\\) 和 \\(\\large W_2 \\in \\mathbb{R}^{F \\times D}\\) 。 记录残差 \\(\\large R \\leftarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times D}\\)，标准化 \\(\\large X \\leftarrow Norm(X)\\) 通过第一个线性层并使用ReLU激活函数 \\(\\large ReLu(W_1^TX) \\rightarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times F}\\) 正则化后在通过第二个线性层 \\(\\large W_2^T(dropout(X)) \\rightarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times D}\\) 加上残差后输出 \\(\\large X \\leftarrow X + R\\) 上述过程与论文不同的是，我们把标准化 Layer Normalization 移到注意力层的前面。这么做的原因在于当我不做改动时，模型预测结果很糟糕，结果全是 padding 或 eos，个人感觉是因为残差连接后再标准化会把残差丢失，详细原因也不是很肯定。而tensor2tensor中的建议也是放在前面的，既然如此，我就放在前面了。 2.5 Optimizer 基于Adam，设 \\(\\large \\beta_1 =0.9\\)，\\(\\large \\beta_2=0.98\\) 和 \\(\\large \\epsilon = 10^{-9}\\) ，热更新步为 \\(\\large w=4000\\)，在第 \\(\\large s\\) 步时，则学习率更新规则为， \\[ lr = D^{-0.5} \\cdot min(s^{-0.5}, s \\cdot w^{-1.5}) \\] 学习率的变化如图所示， 4. 参数细节 词向量大小取 512 head的数量为 8 encoder和decoder都是 6 层 前馈传播网络的维度 \\(\\large F\\) 为 2048 dropout 为 0.1 5. 结果 从论文中公示的结果来看，Transformer 模型得到了相当不错的结果，略胜 ConvS2S 一筹。 6. 自注意力的有效性 自注意力（self-Attention）主要特点是解决了远程依赖问题（long-range dependencies）。信号传递的距离越远，信号就变得越弱，远程依赖就越弱。RNN需要通过时间步的计算传递，信号传递长度是 \\(\\large \\mathcal{O}(n)\\)，而自注意力是把整个序列的信号进行前一层和后一层的直接计算，所以只要 \\(\\large \\mathcal{O}(1)\\)。 参考文献 [1] Attention Is All You Need. 2017 [2] Deep Residual Learning for Image Recognition. 2015 [3] Layer Normalization. 2016","link":"/post/transformer/"},{"title":"AVL Tree Implementation","text":"AVL tree is a self-balancing binary search tree. In an AVL tree, the heights of the two child subtrees of any node differ by at most one; if at any time they differ by more than one, rebalancing is done to restore this property. I will try to implement it and log in this article. You can find the source code in here. Node Evey Node have five member properties. balFactor_ is the balance factor that equal to -1, 0, 1 representing the left sub-tree 1 layer taller than right sub-tree, tall equally and converse, respectively. 123456789template &lt;typename T&gt;struct AVLNode &#123; typedef AVLNode&lt;T&gt;* NodePtr; T value_; NodePtr parent_; NodePtr left_; NodePtr right_; signed char balFactor_; Iterator function increment() find the last node that is larger than current node, corresponding operator++. function decrement() find the last node that is smaller than current node, corresponding operator--. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465template &lt;typename T&gt;struct AVLTreeIterator &#123; // ... typedef AVLTreeIterator&lt;T&gt; Self; typedef AVLNode&lt;T&gt;* NodePtr; NodePtr node_; explicit AVLTreeIterator(NodePtr x): node_(x) &#123;&#125; reference operator*() const &#123; return node_-&gt;value_; &#125; pointer operator-&gt;() const &#123; return &amp;(operator*()); &#125; void increment() &#123; if(node_-&gt;right_ != 0) &#123; node_ = node_-&gt;right_; while(node_-&gt;left_ != 0) &#123; node_ = node_-&gt;left_; &#125; &#125; else &#123; NodePtr y = node_-&gt;parent_; while (node_ == y-&gt;right_) &#123; node_ = y; y = y-&gt;parent_; &#125; if(node_-&gt;right_ != y) &#123; node_ = y; &#125; &#125; &#125; void decrement() &#123; if(node_-&gt;parent_-&gt;parent_ == node_ &amp;&amp; node_-&gt;balFactor_ == -2) &#123; // end() node node_ = node_-&gt;right_; &#125; else if (node_-&gt;left_ != 0) &#123; NodePtr y = node_-&gt;left_; while (y-&gt;right_ != 0)&#123; y = y-&gt;right_; &#125; node_ = y; &#125; else &#123; NodePtr y = node_-&gt;parent_; while (node_ == y-&gt;left_) &#123; node_ = y; y = y-&gt;parent_; &#125; node_ = y; &#125; &#125; Self&amp; operator++() &#123; increment(); return *this; &#125; Self&amp; operator--() &#123; decrement(); return *this; &#125; // ... Tree Property header Let us see a tree without value node, and the only node called header. avl-1.jpg parent: link to root if exist left: also call leftmost, link to the most left node of tree right: also call leftmost, link to the most right node of tree balFactor: always equal to -2 (special) root root is the top node of a tree. avl-2.jpg parent: link to header left: link to nullptr if no node right: link to nullptr if no node 1NodePtr&amp; root() const &#123; return header_-&gt;parent_; &#125; Multi Node See a tree with three nodes. The children of leaf are nullptr. avl-3.jpg Balance AVL Tree (Adelson-Velskii-Landis tree) is a kind of balanced binary search tree with the demand that the difference heigth of the left and right tree cannot over 1. avl-4.jpg the sub-tree or tree with a root node whose balance factor = -2 or = 2 is an unbalance tree. We can rebalance it by rotation. Rotation Left Rotation When the right sub-tree is taller, we need to rotate left to shorten it. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** / /* x y* / \\ / \\* a y =&gt; x c* / \\ / \\* [b] c a [b]*/void _rotateLeft(NodePtr x, NodePtr &amp;root) &#123; NodePtr y = x-&gt;right_; // change the child x-&gt;right_ = y-&gt;left_; y-&gt;left_ = x; // change the parent y-&gt;parent_ = x-&gt;parent_; x-&gt;parent_ = y; if(x-&gt;right_ != nullptr) &#123; // b x-&gt;right_-&gt;parent_ = x; &#125; if(x == root) &#123; root = y; &#125; else if(y-&gt;parent_-&gt;right_ == x) &#123; // x is the right son of his parent y-&gt;parent_-&gt;right_ = y; &#125; else &#123; y-&gt;parent_-&gt;left_ = y; &#125; if(y-&gt;balFactor_ == 1) &#123; /* * / / * x y * / \\ / \\ * a y =&gt; x c * / \\ / \\ / * b c a b d * / * d */ y-&gt;balFactor_ = 0; x-&gt;balFactor_ = 0; &#125; else &#123; /* * / / * x y * / \\ / \\ * a y =&gt; x c * / \\ / \\ * b c a b * \\ \\ * d d */ y-&gt;balFactor_ = -1; x-&gt;balFactor_ = 1; &#125;&#125; Right Rotation When the left sub-tree is taller, we need to rotate right to shorten it. 123456789101112131415161718192021222324252627282930313233343536/** / /* x y* / \\ =&gt; / \\* y a c x* / \\ / \\* c [b] [b] a*/void _rotateRight(NodePtr x, NodePtr &amp;root) &#123; NodePtr y = x-&gt;left_; x-&gt;left_ = y-&gt;right_; y-&gt;right_ = x; y-&gt;parent_ = x-&gt;parent_; x-&gt;parent_ = y; if(x-&gt;left_ != nullptr) &#123; // b x-&gt;left_-&gt;parent_ = x; &#125; if(x == root) &#123; root = y; &#125; else if(y-&gt;parent_-&gt;right_ == x) &#123; y-&gt;parent_-&gt;right_ = y; &#125; else &#123; y-&gt;parent_-&gt;left_ = y; &#125; if(y-&gt;balFactor_ == -1) &#123; y-&gt;balFactor_ = 0; x-&gt;balFactor_ = 0; &#125; else &#123; y-&gt;balFactor_ = 1; x-&gt;balFactor_ = -1; &#125;&#125; Left Right Rotation As for Double rotation, we need to look four layers of tree. When the top node of tree or sub-tree has 2 layers taller left sub-tree, and the left sub-tree have a taller right sub-tree, we need to do a left rotation to make the latter right sub-tree shorter firstly and then do a right rotation to make the former left sub-tree shorter shorter secondly 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** / /* a c* / \\ / \\* b g =&gt; / \\* / \\ b a* d c / \\ / \\* / \\ d e f g* e f*/void _rotateLeftRight(NodePtr a, NodePtr &amp;root) &#123; NodePtr b = a-&gt;left_; NodePtr c = b-&gt;right_; // a and b link to c' sons a-&gt;left_ = c-&gt;right_; b-&gt;right_ = c-&gt;left_; // c becomes the parent of a and b c-&gt;right_ = a; c-&gt;left_ = b; // c links to the parent of a and b c-&gt;parent_ = a-&gt;parent_; a-&gt;parent_ = b-&gt;parent_ = c; // check c' sons and link to their new parent if(a-&gt;left_) &#123; // f a-&gt;left_-&gt;parent_ = a; &#125; if(b-&gt;right_) &#123; // e b-&gt;right_-&gt;parent_ = b; &#125; // the parent of a and b link to c if(a == root) &#123; root = c; &#125; else if(c-&gt;parent_-&gt;left_ == a) &#123; c-&gt;parent_-&gt;left_ = c; &#125; else &#123; c-&gt;parent_-&gt;right_ = c; &#125; switch (c-&gt;balFactor_) &#123; case -1: /* * c * / * e */ a-&gt;balFactor_ = 1; b-&gt;balFactor_ = 0; break; case 0: a-&gt;balFactor_ = 0; b-&gt;balFactor_ = 0; break; case 1: /* * c * \\ * f */ a-&gt;balFactor_ = 0; b-&gt;balFactor_ = -1; break; default: assert(false); &#125; c-&gt;balFactor_ = 0;&#125; Right Left Rotation As for Double rotation, we need to look four layers of tree. When the top node of tree or sub-tree has 2 layers taller right sub-tree, and the right sub-tree have a taller left sub-tree, we need to do a right rotation to make the latter left sub-tree shorter firstly and then do a left rotation to make the former right sub-tree shorter shorter secondly. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** / /* a c* / \\ / \\* d b =&gt; / \\* / \\ a b* c g / \\ / \\* / \\ d e f g* e f*/void _rotateRightLeft(NodePtr a, NodePtr &amp;root) &#123; NodePtr b = a-&gt;right_; NodePtr c = b-&gt;left_; a-&gt;right_ = c-&gt;left_; b-&gt;left_ = c-&gt;right_; c-&gt;left_ = a; c-&gt;right_ = b; c-&gt;parent_ = a-&gt;parent_; a-&gt;parent_ = b-&gt;parent_ = c; if(a-&gt;right_) &#123; a-&gt;right_-&gt;parent_ = a; &#125; if(b-&gt;left_) &#123; b-&gt;left_-&gt;parent_ = b; &#125; if(a == root) &#123; root = c; &#125; else if(c-&gt;parent_-&gt;left_ == a) &#123; c-&gt;parent_-&gt;left_ = c; &#125; else &#123; c-&gt;parent_-&gt;right_ = c; &#125; switch(c-&gt;balFactor_) &#123; case -1: /* * c * / * e */ a-&gt;balFactor_ = 0; b-&gt;balFactor_ = 1; break; case 0: a-&gt;balFactor_ = 0; b-&gt;balFactor_ = 0; break; case 1: /* * c * \\ * f */ a-&gt;balFactor_ = -1; b-&gt;balFactor_ = 0; break; default: assert(false); &#125; c-&gt;balFactor_ = 0;&#125; Insertion When we insert a node to tree, we consider three section: Ensuring insert to the child of the leaf Considering the parent of new node is header, means that the new node is root, and that we need to add the relation between header and root, including the parent of them each other and the leftmost or rightmost. Rebalance. We consider the parent of x at the start, and percolate up the path from leaf to root. We at least do zero or once rotation for certein. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// [x] is the new node to insert// [p] is the parent of [x]void _insertAndRebalance(bool insertLeft, NodePtr x, NodePtr p) &#123; // construct the new node to insert x-&gt;parent_ = p; x-&gt;left_ = nullptr; x-&gt;right_ = nullptr; x-&gt;balFactor_ = 0; if(insertLeft) &#123; p-&gt;left_ = x; if(p == header_) &#123; // consider that p is header header_-&gt;parent_ = x; header_-&gt;right_ = x; &#125; else if(p == header_-&gt;left_) &#123; // consider that p is son of header header_-&gt;left_ = x; &#125; &#125; else &#123; p-&gt;right_ = x; if(p == header_-&gt;right_) &#123; header_-&gt;right_ = x; &#125; &#125; // rebalance while(x != root()) &#123; switch (x-&gt;parent_-&gt;balFactor_) &#123; case 0: // same tall right and left sub tree of x parent. // One of them will become taller after insert a node. x-&gt;parent_-&gt;balFactor_ = (x == x-&gt;parent_-&gt;left_) ? -1 : 1; x = x-&gt;parent_; // percolate up the path break; case 1: // right sub-tree is taller if(x == x-&gt;parent_-&gt;left_) &#123; // If inserted node in the left, become same tall. x-&gt;parent_-&gt;balFactor_ = 0; &#125; else &#123; // If inserted node in the right, become more taller probably. // For shortening it, we need to rotate it. if(x-&gt;balFactor_ == -1) &#123; // x have a taller left, do a right left rotation to shorten the left _rotateRightLeft(x-&gt;parent_, root()); &#125; else &#123; // just do a left rotation to shorten the right. _rotateLeft(x-&gt;parent_, root()); &#125; &#125; // adjust once is enough return; case -1: // -1 mean that x parent have a taller sub-left tree if(x == x-&gt;parent_-&gt;left_) &#123; // but insert a new node in sub-left tree // leading to become more taller (probably) // For shortening it, we need to rotate it. if(x-&gt;balFactor_ == 1) &#123; // If x have a taller sub-right tree, // doing a left right rotation can shorten the right. _rotateLeftRight(x-&gt;parent_, root()); &#125; else &#123; // Else, just do a right rotation to shorten the left. _rotateRight(x-&gt;parent_, root()); &#125; &#125; else &#123; x-&gt;parent_-&gt;balFactor_ = 0; &#125; return; default: assert(false); &#125; &#125;&#125; We insert a not-allow-replaced key with three considerations: insert to the left of leftmost insert to the right of y if just bigger than y insert to the left of y if just bigger than decrement(y) 123456789101112131415161718192021222324252627282930313233343536pair&lt;iterator, bool&gt; insertUnique(const Val&amp; v) &#123; NodePtr x = root(); NodePtr y = header_; bool comp = true; while(x != nullptr) &#123; y = x; comp = keyComp_(KeyOfValue()(v), key(x)); x = comp ? left(x) : right(x); &#125; iterator j = iterator(y); if(comp) &#123; // left if(j == begin()) &#123; // leftmost return pair&lt;iterator, bool&gt;(_insert(x, y, v), true); &#125; else &#123; --j; &#125; &#125; if(keyComp_(key(j.node_), KeyOfValue()(v))) &#123; return pair&lt;iterator, bool&gt;(_insert(x, y, v), true); &#125; return pair&lt;iterator, bool&gt;(j, false);&#125;// [x] is the child of [p],// always is nullptr passed from insertUnique() and insertEqual(),// do not need to consider, in theory.// [p] is the parent of [x]iterator _insert(NodePtr x, NodePtr p, const Val&amp; v) &#123; bool insertLeft = (x != nullptr || p == header_ || keyComp_(KeyOfValue()(v), key(p))); NodePtr z = createNode(v); _insertAndRebalance(insertLeft, z, p); ++nodeCount_; return iterator(z);&#125; Insertion of allowing equal key. 123456789iterator insertEqual(const Val&amp; v) &#123; NodePtr x = root(); NodePtr y = header_; while(x != nullptr) &#123; y = x; x = keyComp_(KeyOfValue()(v), key(x)) ? left(x) : right(x); &#125; return _insert(x, y, v);&#125; Deletion When we delete a node from a tree, we consider three sections: Find the successor to replace the position of deleted node. If the node to be deleting has only one child, the successor is the child. If it has two children, the successor is the last node that is larger than it. The replacement action including three steps: Disconnecting to its children and make the successor links to them Suturing the wound caused by successor moved Its parent link to the successor Rebalance. We will percolate up and rotate multi times probably until the balFactor of certain node is 0, we just change its balFactor to -1 or 1 (depending the deleted node in right or left). the balFactor of certain node after rotation becomes from 1 to -1 or from -1 to 1. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132// erase node [z] and return the deleted nodeNodePtr _eraseAndRebalance(NodePtr z) &#123; NodePtr y = z; NodePtr x = nullptr; NodePtr xParent = nullptr; // x is the child of y if(y-&gt;left_ == nullptr) &#123; x = y-&gt;right_; &#125; else if(y-&gt;right_ == nullptr) &#123; x = y-&gt;left_; &#125; else &#123; // y has two children // find the successor y = y-&gt;right_; while(y-&gt;left_) &#123; y = y-&gt;left_; &#125; x = y-&gt;right_; //y has no left child, so x is the successor &#125; if(y != z) &#123; // mean that z has two children // we make the successor y to replace z // z will disconnect to its children and y link to them z-&gt;left_-&gt;parent_ = y; y-&gt;left_ = z-&gt;left_; if(y != z-&gt;right_) &#123; // y will be moved, need to link its child to its parent xParent = y-&gt;parent_; if(x) &#123; x-&gt;parent_ = y-&gt;parent_; &#125; y-&gt;parent_-&gt;left_ = x; y-&gt;right_ = z-&gt;right_; z-&gt;right_-&gt;parent_ = y; &#125; else &#123; // y == z-&gt;right, mean y has no left child and x is the right of y xParent = y; &#125; // link z's parent to y if(root() == z) &#123; root() = y; &#125; else if(z-&gt;parent_-&gt;left_ == z) &#123; z-&gt;parent_-&gt;left_ = y; &#125; else &#123; z-&gt;parent_-&gt;right_ = y; &#125; y-&gt;parent_ = z-&gt;parent_; y-&gt;balFactor_ = z-&gt;balFactor_; &#125; else &#123; // mean that z has one child or none and y == z // x become the successor xParent = y-&gt;parent_; if(x) &#123; x-&gt;parent_ = y-&gt;parent_; &#125; if(root() == z) &#123; root() = x; &#125; else if (z-&gt;parent_-&gt;left_ == z) &#123; z-&gt;parent_-&gt;left_ = x; &#125; else &#123; z-&gt;parent_-&gt;right_ = x; &#125; if(leftmost() == z) &#123; if(z-&gt;right_ == nullptr) &#123; leftmost() = z-&gt;parent_; &#125; else &#123; leftmost() = Node::minimum(x); &#125; &#125; if(rightmost() == z) &#123; if(z-&gt;left_ == nullptr) &#123; rightmost() = z-&gt;parent_; &#125; else &#123; rightmost() = Node::maximum(x); &#125; &#125; &#125; // rebalance while (x != root()) &#123; switch (xParent-&gt;balFactor_) &#123; case 0: xParent-&gt;balFactor_ = (x == xParent-&gt;right_) ? -1 : 1; return z; case -1: // left sub-tree taller if(x == xParent-&gt;left_) &#123; // erase the node in the left, balance just right xParent-&gt;balFactor_ = 0; x = xParent; xParent = xParent-&gt;parent_; &#125; else &#123; // erase the node in the right, need to shorten the left NodePtr a = xParent-&gt;left_; if(a-&gt;balFactor_ == 1) &#123; // check whether need a double rotation _rotateLeftRight(a, root()); &#125; else &#123; _rotateRight(xParent, root()); &#125; // percolate up x = xParent-&gt;parent_; xParent = xParent-&gt;parent_-&gt;parent_; if(x-&gt;balFactor_ == 1) &#123; return z; &#125; &#125; break; case 1: // right sub-tree taller if(x == xParent-&gt;right_) &#123; xParent-&gt;balFactor_ = 0; x = xParent; xParent = xParent-&gt;parent_; &#125; else &#123; NodePtr a = xParent-&gt;right_; if(a-&gt;balFactor_ == -1) &#123; _rotateRightLeft(a, root()); &#125; else &#123; _rotateLeft(xParent, root()); &#125; x = xParent-&gt;parent_; xParent = xParent-&gt;parent_-&gt;parent_; if(x-&gt;balFactor_ == -1) &#123; return z; &#125; &#125; break; default: assert(false); &#125; &#125; return z;&#125; Nothing additional thing If need to delete a known node. 12345void erase(iterator pos) &#123; NodePtr y = _eraseAndRebalance(pos.node_); destroyNode(y); --nodeCount_;&#125; Deleting a key that probably exists in multi node, we need to find the range and delete them one by one. lowerBound() can find the last node that greater than or equal to a certein key. And upperBound() find the last node that greater than a certein key. So the deleted range is [lowerBound(), upperBound()). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748size_type erase(const Key&amp; k) &#123; pair&lt;iterator, iterator&gt; p = equalRange(k); size_type old = size(); erase(p.first, p.second); return old - size();&#125;void erase(iterator first, iterator last) &#123; if(first == begin() &amp;&amp; last == end()) &#123; clear(); &#125; else &#123; while (first != last) &#123; erase(first++); &#125; &#125;&#125;pair&lt;iterator, iterator&gt; equalRange(const Key&amp; k) const &#123; return pair&lt;iterator, iterator&gt;(lowerBound(k), upperBound(k));&#125;iterator lowerBound(const Key&amp; k) const &#123; NodePtr y = header_; NodePtr x = root(); while(x != nullptr) &#123; if(!keyComp_(key(x), k)) &#123; y = x; x = left(x); &#125; else &#123; x = right(x); &#125; &#125; return iterator(y);&#125;iterator upperBound(const Key&amp; k) const &#123; NodePtr y = header_; NodePtr x = root(); while(x != nullptr) &#123; if(keyComp_(k, key(x))) &#123; y = x; x = left(x); &#125; else &#123; x = right(x); &#125; &#125; return iterator(y);&#125; Find Using lowerBound, we can find the last node that greater than or equal to a certein key, if no, return the header node. And we only need the node with the same key. 1234567891011121314151617181920iterator lowerBound(const Key&amp; k) const &#123; NodePtr y = header_; NodePtr x = root(); while(x != nullptr) &#123; if(!keyComp_(key(x), k)) &#123; y = x; x = left(x); &#125; else &#123; x = right(x); &#125; &#125; return iterator(y);&#125;iterator find(const Key&amp; k) &#123; iterator j = lowerBound(k); // 1. j will be end() if cannot find // 2. key(j) probably less than k return (j == end() || keyComp_(k, key(j.node_))) ? end() : j;&#125;","link":"/post/avl-tree/"},{"title":"操作系统网络概述","text":"这里主要讲讲操作系统底层网络模块的一些概念和工作流程，了解数据在操作系统层面如何传输和转换，以及虚拟网络的构造和原理。 网络接口控制器 网络接口控制器(Network interface controller, NIC)，又称网卡，是接收网络数据的硬件设备。NIC负责接收网络数据帧，然后通知设备驱动程序读取，该过程的实现原理是，设备驱动程序初始化时会向NIC请求一个IRQ编号（中断请求编号）以及告诉NIC读取和写入数据的内存地址（队列），然后向CPU的中断表注册该IRQ编号的处理函数(interrput handler)。 当NIC接收当帧数据时，会先把数据写入到指定队列，然后向CPU发送IRQ（中断请求），CPU根据IRQ编号找到设备驱动程序注册的处理函数，并将其调用；处理函数会先调用NIC的接口要求其暂时关闭IRQ，因为驱动程序已经知道需要读取数据，NIC不需要再继续发送IRQ，当数据被驱动程序读取完而队列为空时才会重新开启IRQ。这种关闭IRQ的方案被称为NAPI(New API)，而与之相对的旧API(netif_rx)是没有关闭IRQ功能的。 相反，当驱动程序收到应用层下来的数据时会把数据写入队列，并向NIC发送IRQ，NIC收到IRQ后则关闭该驱动的IRQ功能，然后读取数据并发送出去。在IRQ被关闭的期间，NIC并不知道该队列里有新数据，所以它会采用轮询的方式，检查每一条IRQ关闭的队列，该过程在内核中由net_rx_action()处理。当队列为空时，则发送IRQ给CPU，CPU再回调驱动的相应函数。 中断处理 中断处理的方式可以有很多种，多是为了提供性能而设计的。比如在高流量情况下，每一帧都触发一个IRQ会频繁占用CPU时间，而采用通知一次后便关闭IRQ的方案的话，若驱动程序没有及时处理数据，就会导致NIC缓冲区被占满导致数据丢失。还有一种方案是定时中断，比如规定每100ms才发送一次中断，这样能保证IRQ不会过于频繁，但在低流量的情况下，会造成不必要的延迟。有时候我们可以结合上面提到的两种方案，分别用于低流量和高流量场景。 中断处理面临的不只是网络流量大小问题，更多是操作系统内部的处理时间问题。当CPU接收到一个硬件中断时，调用其中断处理函数的这个过程是不可暂停的，若中断频繁发生，而某个中断处理函数占用过多时间则导致其他中断请求发生延迟。为了解决这个问题，一般处理函数会分为上半部和下半部，上半部是非抢占的，必须快速处理，而下半部是可抢占的，这种处理函数一般被称为软IRQ。 可抢占程序的一般设计思路是使用线程和锁。使用线程时，即为每个下半部处理函数都创建一个内核线程去处理，进而可以使其在执行过程中可休眠或中断。而加锁则是为了处理相同的处理函数在多个CPU中同时执行时能够访问共享数据。 协议栈 跨主机的网络传输需要设定格式，以便于对方收到数据后可以正确的解析，这种数据格式规范称之为协议。操作系统在处理数据时会经过多层协议处理，比如典型的TCP/IP五层协议，数据从本机发送出去，总共需要经过五层协议的处理。 在应用层，数据单位是消息(message)，协议由应用程序自己决定。到了传输层，数据单位称为段(segment)，会把应用层消息分成多个段，在每个段前面添加上一个报头，该报头包含的典型数据就是源端口号和目的端口号，其他参数与程序所采用的传输层协议有关，常见的是TCP和UDP协议。 到了网络层，数据单位是包(packet)，其会在传输层每一段的前面加上报头，包含的典型数据是源IP，目的IP以及传输层协议，同理，报头的其他参数与程序所采用的网络层协议有关，常见的是IPv4和IPv6协议。到了数据链路层，数据单位是帧(frame)，在包的前面再加一个报头，一般包含数据有源MAC地址，目的MAC地址以及网络层协议，常用的链路层协议是Ethernet。注意，这里的目的MAC地址并非最终主机的MAC地址，而是帧传输的第一个路由器MAC地址。 到了物理层，数据在铜缆线或光纤缆线中传输，数据单位是比特(bit)。但并非通过缆线一次性传输到最终主机，而是可能会经过若干路由器。路由器在收到帧后，会把报头删掉，并读取包的目的IP地址，如果发现不是自己，就给包加上一个新报头，赋予新的目的MAC地址，然后传出去，在之后可能被下一个路由器接收，以此类推。 最后，帧的目的MAC地址将会是最终目的主机的MAC地址，每张网卡(NIC)都会绑定唯一的MAC地址，所以可以认为MAC地址就是网卡地址。网卡收到帧后会交给驱动程序，即进入数据链路层，程序检查帧的网络层协议，并将帧剥离报头变为包，交由网络层协议对应的函数处理。网络层函数若确定包的目的地址就是自己的IP地址，就会在本地进行处理，检查运输层协议，剥离网络层报头变为段，交给运输层协议对应的函数来处理。运输层从报头获得目的端口，剥离运输层报头变为消息，交给监听该端口的应用程序处理。 到此你会发现，数据从发出到接收，协议会遵循后进先出的处理顺序，相当于栈，所以称之为协议栈。 net-1 上图的(d)经过路由器的处理变成了(e)。 socket 应用程序可以开启一个socket监听某个端口。当接受的数据到达运输层后，剥离运输层报头，并解析报头得到目的端口，然后判断该socket的receive buffer是不是满了，如果receive buffer满了，则会丢弃数据包。socket允许用户设置过滤器，在这个阶段把不符合条件的数据包过滤掉，这个过程在源码中的sock_queue_rcv_skb函数里，注意这一步并没有把数据放到队列中，而是做一些判断和过滤。 下一步，就会把数据包放到socket的接收队列尾部(__skb_queue_tail)，然后通知socket数据包准备就绪(sk_data_ready)。到目前为止的操作都属于运输层。 在应用层，用户一般会通过select或epoll去监听socket。为了不占用CPU资源，没有数据时select或epoll会将进程挂起，当socket有数据时就会唤醒监听的进程。 TUN/TAP TUN/TAP分别是两种虚拟网络设备。所谓虚拟网络设备其实就是操作系统上的一个进程，并运行着驱动程序。 TAP用于把链路层的帧数据转发至用户空间应用程序。启动一个TAP后，会自动生成一个虚拟MAC地址，你需要对路由表配置什么样的数据会转发至该TAP，并让你的应用程序监听该TAP来接收数据。网卡把接收到的帧数据交给协议栈，协议栈在链路层根据路由表把帧数据转发至对应的TAP。 TUN用于把网络层的包数据转发至用户空间应用程序。启动一个TUN后，你要手动指定一个IP地址给TUN，在路由表配置什么样的数据会转发至该TUN，并让你的应用程序监听该TUN来接收数据。网卡把接收到的帧数据交给协议栈，协议栈在网络层根据路由表把包数据转发至对应的TUN。 同时，应用程序通过socket发送出去的数据可以在协议栈被路由表转发至对应TUN/TAP。到了这里，你就会发现，你可以通过TUN/TAP拦截应用程序发出的网络数据或从主机外接收的数据，将它们交由另一个应用程序处理。一般使用场景有数据压缩解压，加解密，VPN等。 应用程序可以往TUN/TAP写入数据，数据会经过协议栈处理，然后再从网卡发送出去。 veth veth是一种成对出现的虚拟设备，默认与协议栈相连，你可以在两个veth之间建立管道，为它们设置虚拟IP，当你往一个veth里面发数据，它会通过管道传递到另一个veth上。 路由表 路由表能提供三层转发功能，三层是指OSI第三层网络层，即通过IP地址映射转发至预设好的目的设备上，目的设备可以是物理网卡或虚拟设备，网络包经过路由表时被称为路由选择。每次路由最少包含三个参数： 目的网络：路由器会通过预先设置的子网规则来匹配包的目的网络IP。 出口设备：匹配成功后的包应该转发至哪个设备上。 下一跳网关：当目的网络与本机不直连时，需要由其他路由器转发，这时会下面其下一跳路由器地址。 NAT NAT(Network Address Translation)，是一种网络地址转发技术。一般用于私有网络访问公有网络时，对私有的源地址改为公网地址，再向公网请求。为了确保回包能准确发给对应的私有地址，NAT网关会为每一个连接分配一个独有的端口，回包时通过端口号就能知道该映射成哪个私有地址了。假设物理机公网地址是220.138.23.45，通过虚拟机192.168.10.11:12345访问外部网络，物理机将私有地址192.168.10.11:12345映射为端口号18888，发包的源地址变为220.138.23.45:18888，远程主机回包时的地址也是这个，物理机收到回包后，根据18888就能知道该把回包转发给192.168.10.11:12345了。 当然上面只是以私有和公有网络举例，NAT可以充当一种内核层面的转发器，NAT之后会进行一次路由选择，如果不是本机IP则会被转发出去，使得主机可以把接收到请求后根据NAT转换成的地址由路由表转发至另一台主机上。 网桥 网络设备之间的转发硬件设备有三种： 中继器(repeater)，按比特传输数据，也被称为集线器。 网桥(bridge)，按帧传输数据，即等待收集到一帧数据后再转发出去，也被称为交换机。 路由器(router)，按包传输数据，并且能解析目的IP地址并精确地转发到目的终端。 这里插入一个题外话，把两个网络设备连接在一起的设备都可以称为网关(Gateway)，所以中继器、网桥和路由器都可以作为网关，网关的一个重要功能就是负责转发，网络数据从设备中发出后会先经过网关，然后再从网关转发到目的设备，至于是精准转发还是广播转发就要看网关设备自己的功能了。具体一点的例子是，在局域网内，我们给作为网关的路由器分配一个IP，如192.168.30.1/24，其他设备的IP则改变末尾数，分配为30.2，30.3等等，这样它就处于同一网段。当处于另一个网络的设备，比如31.2发数据给30.2时，可能会先发至自己的网关31.1，再根据路由表发至网关30.1，它再转发至30.2。当然，网关设备的转发算法不强制配成同一网段IP，就算你把IP乱配也不会影响它的功能，但有时候根据路由算法，会优先发给与目的IP处于同一网段的网关。 最简单的网桥会有两个端口，这里的端口是指网线的插口，可以将两个局域网(LAN)连接在一起，而常见的网桥会有多个端口，可以将多个LAN连接在一起。当多个LAN连接在一起后，由一台主机发出的数据，会被网桥连接的所有LAN的所有主机收到，其中部分主机发现数据不是发给自己的就会丢弃掉。所以你会发现网桥只是把不同局域网连接在一起，并把数据广播给所有终端，并不能精确地把数据发送给目的终端。 如果你需要减少广播带来的不必要网络损坏，可以使用路由器连接多个LAN，路由器之间会交换和缓存所有LAN的所有终端的IP地址，进而可以解析出包的目的IP地址并精确发给目的终端。 如果仔细思考，会发现网桥是可以知道目的终端处于哪个LAN上的。当网桥收到主机A发出的帧时，它就知道了主机A处于哪个LAN上，那么假设以后主机B向主机A发送数据，它就可以只把数据转发至主机A所在的LAN，减少广播压力。这种方法称为地址学习，这里的地址是指MAC地址。但这个方法也有个缺陷，就是主机A的位置可能被挪到别的LAN上，这时数据转发就会失败，具备该功能的网桥也应该具备老化机制，即在一段时间内没有收到来自主机A的帧，就会把主机A缓存清掉。 网桥不能用于环路拓扑的网络结构中，最简单的环路就是两个LAN之间用两台网桥连接，这会可能导致一帧数据在两个网桥之间来回转发。STP(Spanning Tree Protocol，生成树协议)可以解决环路网络拓扑的问题，具体不再展开讲。 虚拟网桥 虚拟网桥是操作系统中的一个进程，默认与协议栈相连，也可以和物理设备(NIC)与虚拟设备(TUN/TAP, veth)相连。本质与物理网桥并无区别，通过MAC地址进行广播和转发。网桥本身是用来做转发的，为了方便路由算法根据网段查找，你可以为虚拟网桥配置虚拟IP。 当你在物理机上开虚拟机时，一般会把物理网卡与虚拟网桥相连，虚拟机的虚拟网卡与物理机上的veth相连，veth再与虚拟网桥相连。这样的模式，通过虚拟网桥，虚拟机之间可以相互通信，虚拟机与物理机相互通信，虚拟机与外网通信。当虚拟机与外网通信时，网络数据先从虚拟网卡到veth，再从虚拟网桥到协议栈，再从物理网卡出去。 bridge netfilter/iptables netfilter顾名思义是一个网络数据的过滤器，它在协议栈IP层处理数据的5个阶段，提供给用户注册回调方法的机会，使用户能够对数据包在特定位置做特定处理。而iptables则是提供给用户配置回调方法的工具，一般我们把回调方法被调用的地方称为钩子。iptables默认只影响IPv4的数据包，IPv6的数据包由ip6tables负责配置。 netfilter提供了5个钩子： NF_IP_PRE_ROUTING：数据包还没经过路由选择 NF_IP_LOCAL_IN：数据包经过路由选择，目的IP是本机 NF_IP_FORWARD：数据包经过路由选择，目的IP不是本机 NF_IP_LOCAL_OUT：本机程序发出的数据包，经过路由选择 NF_IP_POST_ROUTING：本机程序发出的数据包，经过路由选择 整理一下不同情况数据包经过钩子的流程： 目的IP是本机的数据包：NF_IP_PRE_ROUTING -&gt; NF_IP_LOCAL_IN 目的IP不是本机的数据包：NF_IP_PRE_ROUTING -&gt; NF_IP_FORWARD -&gt; NF_IP_POST_ROUTING 本机发出的数据包：NF_IP_LOCAL_OUT -&gt; NF_IP_POST_ROUTING iptables提供了5种规则(rule)，用户可以注册这5种规则对应的处理函数： Filter: 用于过滤数据，规定哪些数据可以通过，哪些不行。 NAT: 控制网络地址的转换。 Mangle: 用于修改IP数据包结构体的包头。注意，并不是直接修改IP数据包，而是内核中的数据包结构体(skb)的包头。 Raw: netfilter有一个功能叫connection tracking，用于追踪所有连接，Raw则是可以控制哪些数据包不被追踪。 Security: 用于在数据包上设置SELinux相关标记，方便后面SELinux相关模块的处理。 5种rule会分别放在5个table中，iptables中的tables说的就是这5个表。那么我们要如何把rule对应的回调函数与netfilter的钩子相关联呢？每个表都会有若干条链表(chain)，一条链表对应一个钩子，链表存放rule函数。当经过一个钩子，就会先找表，再从表中找到对应的链表，然后依次执行链表上的rule函数。 一个钩子并不是同时支持所有rule函数的，比如NF_IP_PRE_ROUTING钩子不支持filter，这个阶段还没经过路由选择，内核开发者可能认为这个阶段做过滤没有意义，所以NF_IP_PRE_ROUTING钩子没有filter表。 一个钩子可以支持多种rule函数，那它们执行顺序是什么呢？事实上，不同钩子对rule的执行顺序是不同的。 NF_IP_PRE_ROUTING: Raw, (Connection tracking), Mangle, DNAT NF_IP_LOCAL_IN: Mangle, Filter, Security, SNAT NF_IP_FORWARD: Mangle, Filter, Security NF_IP_LOCAL_OUT: Raw, (Connection tracking), Mangle, DNAT, Filter, Filter, Security NF_IP_POST_ROUTING: Mangle, SNAT 上面把NAT分为DNAT和SNAT表示分别对目的地址和源地址做转换。 每种rule需要做两件事情： Matching：匹配一个数据包，比如协议类型、地址、端口、包头里面的数据以及连接状态等。 Targets：收到数据包后怎么处理。DROP：丢弃数据包；RETURN：跳出chain，后续rule不执行；QUEUE：把数据包放入用户空间队列；ACCEPT：继续执行后续rule；跳转到用户自定义chain。 上面提到用户自定义chain，这种chain没有与之对应的钩子，所以必须从内建钩子的处理函数中触发跳转。如果你想知道如何使用iptables可以参考该文档。 这里再讲一下连接追踪(Connection tracking)。一般来说，网络传输没有连接这种物理概念，连接是一种逻辑概念，比如TCP双方传输若干个握手包，则认为连接建立成功。连接追踪则是基于这种理念设计的，根据两个端传输数据包的情况，来判断两个端处于什么连接状态。 连接追踪包含如下几种状态： NEW: 当收到一个包，从地址上判断这个包与现有所有连接都没有关联，而且该包是一个TCP SYN或UPD包，则认为该包尝试建立连接。 ESTABLISHED：当检测到NEW连接中发出一个反向包。比如TCP ACK包。 RELATED：数据包不属于现有连接，但与某个ESTABLISHED连接有关联。比如FTP。 INVALID: 数据包不属于任何连接，也不是一个请求建立连接的包。比如一些发错的垃圾数据包。 UNTRACKED: 被Raw表标记为不需要追踪的数据包。 参考 Understanding Linux Network Internals man ip man iptables netfilter/iptables简介 Linux虚拟网络设备之veth Linux虚拟网络设备之tun/tap Linux虚拟网络设备之bridge Linux网络 - 数据包的发送过程 Linux网络 - 数据包的接收过程","link":"/post/net/"},{"title":"Kubernetes Client-go 源码解析","text":"Client-go是认识Kubernetes开发接口的良好载体，理解Client-go的运行机制有助于理解Kubernetes的运行机制。其本身的设计模式和理念也值得学习和借鉴。 ResourceVersion etcd默认保留5分钟以内的变更记录，每个资源发生变更都会更新一个更大的资源版本ResourceVersion，ResourceVersion是一个所有资源类型共享的全局变量。 对于watch请求来说，你可以指定一个resourceVersion=0来获取5分钟以内的任意变更记录及其之后，这种表现很奇怪，所以不建议指定0。可以指定一个resourceVersion来获取这个资源版本之后的变更记录，但这个资源版本早于5分钟以内保留的最小版本，则会回复一个410状态码，如果大于最大版本，则可能会一直等下去，直到超时。 对于list，请求后会返回一个Kind=XXList的资源类型，XXList这种资源类型是按照惯例附带创建的，比如Pod和PodList，如果你写过CRD应该能明白了；items字段内包含资源列表，metadata包含的了resourceVersion，但这个resourceVersion是PodList的资源版本，而不是Pod的资源版本，指定resourceVersion=0来获取任意的PodList，也可以指定一个resourceVersion来获取这个资源版本或之后的PodList，如果指定的resourceVersion小于当前最新资源版本，它总是返回最新的PodList，如果大于则返回504状态码。但如果你指定了limit参数或resourceVersionMatch=Excat，就意味着apiserver必须精准匹配你填写的resourceVersion，这时候就和watch一样了，如果找不到指定的resourceVersion（可能是超过了5分钟），则会返回410状态码。关于resourceVersion的解释可以看官方文档，虽然官方文档写得不清晰，但你可以结合我上面说的来理解。 变更事件有四种：ADD, DELETE, MODIFY, BOOKMARK。前面三个容易看懂，但第四个BOOKMARK是干什么的？正如前面所说etcd只保留5分钟的变更记录，万一客户端很长时间内都没有watch到变更，然后断连之后又重连到apiserver时，客户端可能按常规的把上次收到的resourceVersion传到url里，但这个resourceVersion已经是一个过期的资源版本，apiserver找不到资源版本，就会回复一个410状态码。那么这时客户端为了能获取最新的资源版本号就不得不先list一次。为了防止这种情况，apiserver会定期发送BOOKMARK事件，BOOKMARK将包含一个当前最新的资源版本号，尽管这个版本号对应的资源类型并不是你监听的那种，但这样是为了客户端能更新最新的资源版本号。 API访问例子 这里提供API访问的例子，首先是list，通过resourceVersion=0来获取任意版本的PodList 1234567891011# curl -s &quot;http://127.0.0.1:8001/api/v1/namespaces/default/pods?resourceVersion=0&quot; 2&gt;&amp;1 | head&#123; &quot;kind&quot;: &quot;PodList&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123; &quot;selfLink&quot;: &quot;/api/v1/namespaces/default/pods&quot;, &quot;resourceVersion&quot;: &quot;1084093&quot; &#125;, &quot;items&quot;: [ &#123; &quot;metadata&quot;: &#123; 指定resourceVersionMatch=Exact时，返回了410，是因为1084093这个资源版本的变更时间在5分钟以前。 123456789# curl -s &quot;http://127.0.0.1:8001/api/v1/namespaces/default/pods?resourceVersion=1083300&amp;resourceVersionMatch=Exact&quot; 2&gt;&amp;1 | head&#123; &quot;kind&quot;: &quot;Status&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123;&#125;, &quot;status&quot;: &quot;Failure&quot;, &quot;message&quot;: &quot;The resourceVersion for the provided list is too old.&quot;, &quot;reason&quot;: &quot;Expired&quot;, &quot;code&quot;: 410 用watch获取某个资源版本之后的变更记录，这里先看看数据结构， 12345678910111213# curl -s &quot;http://127.0.0.1:8001/api/v1/namespaces/default/pods?watch=1&amp;resourceVersion=1075698&quot; 2&gt;&amp;1 | jq | head -n 15&#123; &quot;type&quot;: &quot;MODIFIED&quot;, &quot;object&quot;: &#123; &quot;kind&quot;: &quot;Pod&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123; &quot;name&quot;: &quot;centos&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;selfLink&quot;: &quot;/api/v1/namespaces/default/pods/centos&quot;, &quot;uid&quot;: &quot;2c357511-a557-4246-b77c-4ffc5d8efcb4&quot;, &quot;resourceVersion&quot;: &quot;1075702&quot;, &quot;creationTimestamp&quot;: &quot;2021-08-14T09:36:26Z&quot;, 这里只打印他们的resourceVersion， 123456# curl -s &quot;http://127.0.0.1:8001/api/v1/namespaces/default/pods?watch=1&amp;resourceVersion=1075698&quot; 2&gt;&amp;1 | jq .object.metadata.resourceVersion&quot;1075702&quot;&quot;1075740&quot;&quot;1075774&quot;&quot;1083264&quot;&quot;1083273&quot; Informer Informer的意思是通知器，内部会以list和watch形式请求API Server，来监控对应资源，当资源有更新时就会调用对应处理函数。一个Informer只处理一种资源类型。 12345678910podListWatcher := cache.NewListWatchFromClient(clientset.CoreV1().RESTClient(), \"pods\", v1.NamespaceAll, fields.Everything())store, controller := cache.NewInformer(podListWatcher, &amp;v1.Pod&#123;&#125;, 0, cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123;&#125;, UpdateFunc: func(old interface&#123;&#125;, new interface&#123;&#125;) &#123;&#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123;&#125;, &#125;)controller.Run(stopCh) 会返回一个store和controller，store等下再讲。这个controller其实就是informer，我也不知道为什么官方要这么定义名字，命名调用了NewInformer()，结果却返回的是一个controller结构体。 controller.Run()之后就会阻塞协程，一直等到回调处理函数。 Store NewInformer时返回了一个store，顾名思义就是存储了资源信息的存储器， 12345678910type Store interface &#123; Add(obj interface&#123;&#125;) error Update(obj interface&#123;&#125;) error Delete(obj interface&#123;&#125;) error List() []interface&#123;&#125; ListKeys() []string Get(obj interface&#123;&#125;) (item interface&#123;&#125;, exists bool, err error) GetByKey(key string) (item interface&#123;&#125;, exists bool, err error) Replace([]interface&#123;&#125;, string) error Resync() error 内部Reflector执行list和watch更新的数据都会放到存储器里面。Store在Reflector的实现是一个DeltaFIFO，队列数据会被定时消费并传入处理函数中。Store在Informer里面的实现是一个threadSafeMap，每当有更新，对象从Reflector传入到Informer，Informer会将其更新至threadSafeMap中 [src]。 DeltaFIFO 这个结构体由两部分组成，一个是Delta，所有更新的对象都会包装到这个结构里面，并被DeltaFIFO用一个Map存储 [src]， 123456789101112type Delta struct &#123; Type DeltaType // Delta类型，增、删、减、同步 Object interface&#123;&#125; // 对象&#125;type DeltaType string // Delta的类型用字符串表达const ( Added DeltaType = \"Added\" // 增加 Updated DeltaType = \"Updated\" // 更新 Deleted DeltaType = \"Deleted\" // 删除 Sync DeltaType = \"Sync\" // 同步) 另一个是Queue， 12345678type Queue interface &#123; Store Pop(PopProcessFunc) (interface&#123;&#125;, error) AddIfNotPresent(interface&#123;&#125;) error HasSynced() bool Close()&#125; DeltaFIFO的结构提如下，这里我只标出三个成员变量， items的值Deltas本质上是一个Delta数组，为什么是数组后面再解释；items的key则是对象的一个字符串表示，是通过另一个成员keyFunc算出来的，具体视传进来的函数是什么。 queue是用一个字符串数组模拟队列，装的是对象的key。 12345678type DeltaFIFO struct &#123; // ... items map[string]Deltas keyFunc KeyFunc queue []string&#125;type Deltas []Delta threadSafeMap Informer的threadSafeMap会被DeltaFIFO所持有，成员变量是knownObjects，每次Resync时都会顺便遍历threadSafeMap，把它和最新的list作比较，已经不存在的对象会调用queueActionLocked()触发一次Deleted事件，以便于将其从threadSafeMap删除 [src]。其中，删除时并不是马上删除，而是在原本的对象上套一个DeletedFinalStateUnknown结构体 [src]。 Pop DeltaFIFO的关键方法Pop()会被controller调用 [src]，Pop函数会调用一个信号量一直阻塞 [src]，直到队里中有元素，所有尝试往队列添加元素的操作后面都会用该信号量发起广播，来唤醒阻塞。Pop内部会调用传进来的process方法进行回调处理对象 [src]，这个process方法的主体在这里可以看到，当尝试往threadSafeMap更新值失败时就会返回err，DeltaFIFO发现是ErrRequeue则会重新加入队列。 HasSynced HasSynced()方法会检查第一次list资源后是否全部Pop出来 [src]，Controller的HasSynced()其实就是调用了这个HasSynced() [src]。 官方的 client-go/examples/workqueue 例子里面，会一直等待这个函数返回true，才开始runWorker [src]，再说具体一点就是，第一次list资源后，等到所有资源都从DeltaFIFO中pop出来，并且全部回调处理函数，例子中的处理函数就是把对象都放进workqueue里面，然后才开始从workqueue里面消费数据。 Resync Resync()做的事情就是把threadSafeMap里面的所有对象都触发一次Sync事件 [src]，很多Controller会定时调用以便于把资源同步到期望状态。理论上，如果你的资源的更新逻辑处理得足够好，其实是不用Resync的，Resync更像是一种弥补逻辑处理漏掉的万金油。还是要强调一下Resync不会访问apiserver，下面提到的ReList才会。 Deltas Deltas作为一个数组其实是为了处理多时间内同一个对象被多次操作的情况。比如新增对象时产生一个Added Delta，然后短时间内又删除该对象，又产生了一个Deleted Delta；通过对象的key，如果发现队列中已经有这个对象，那么会把Delta插入它的Deltas数组里 [src]。 一个新Delta插入Deltas数组后会有一个合并操作，当数组最后两个元素都是Deleted Delta，意味着一个对象被连续执行两次删除操作，这是多余的，所以会去掉后面那个Delta。而为什么Add Delta不需要处理合并，是因为资源的namespace/name的唯一性由apiserver保证，如果namespace/name冲突，apiserver层面就会返回错误，轮不到client-go去处理。这里额外提一句，成员变量keyFunc大部分情况下都会传入cache.MetaNamespaceKeyFunc()函数，它算出来的key的格式就是{namespace}/{name}。 Indexer Indexer就是索引器，是为Store建立索引的。 12345678type Indexer interface &#123; Store Index(indexName string, obj interface&#123;&#125;) ([]interface&#123;&#125;, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(indexName string) []string ByIndex(indexName, indexKey string) ([]interface&#123;&#125;, error) GetIndexers() Indexers AddIndexers(newIndexers Indexers) error 你会发现它继承了Store然后提供了一堆通过索引查询的函数。举个例子，下面这么写，可以把namespace=default的所有资源对象查出来。不过因为Informer只处理一种资源，所以本质上这里返回的是这种资源在default namespace下的所有实例， 1items, err := indexer.Index(\"namespace\", &amp;metav1.ObjectMeta&#123;Namespace: \"default\"&#125;) threadSafeMap实现了Indexer的接口。你不需要手动创建它，可以用NewIndexerInformer()顺便创建出来。 threadSafeMap实现了一套比较复杂的索引分类，它支持外部用户通过AddIndexers()接口传入计算索引key的函数，然后每次更新数据都会更新其索引，简单的说就是一个用若干Map实现的小型索引数据库。但目前K8s的所有使用场景下，只有两种索引Key计算函数，一个是cache.MetaNamespaceKeyFunc() [src]，以namespace建立索引，使得我们可以通过namespace获取其所有资源；另一种是indexByPodNodeName()，只在deamon controller里面使用，以nodeName建立索引 [src]。 IndexerInformer IndexerInformer与Informer不同的是，它需要在最后传入一个indexers，这个可以用默认的那个cache.Indexers{}，其实它本质上就是一个Map，只是建了一个别名而已。IndexerInformer返回的是indexer而非store，但其实从源码看来，构造的时候都会构造一个threadSafeMap，只是返回了不同的抽象类型而已 [src]。 12345678indexer, controller := cache.NewIndexerInformer(podListWatcher, &amp;v1.Pod&#123;&#125;, 0, cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123;&#125;, UpdateFunc: func(old interface&#123;&#125;, new interface&#123;&#125;) &#123;&#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123;&#125;, &#125;, cache.Indexers&#123;&#125;)controller.Run(stopCh) Reflector Reflector的工作就是通过Lister和Watcher得到的数据放进DeltaFIFO里。Reflector的数据类型如下，只保留几个关键的， 123456type Reflector struct &#123; expectedType reflect.Type store Store listerWatcher ListerWatcher resyncPeriod time.Duration // ... - expectedType 就是希望反射出来的类型，具体一点就是K8s里面的资源类型，比如v1.Pod{}等。 - store 这里将传入DeltaFIFO - listerWatcher Lister和Watcher的结合体，负责从apiserver拉取列表和监听资源变化，这个结构具体后面再讲。 - resyncPeriod 全量同步的时间间隔 ListAndWatch Reflector中的一个函数ListAndWatch()会负责三件事情，一件是list资源（从apiserver获取资源列表，并全量同步），第二件是开一个协程去定时resync（全量同步），第三件是watch监听资源。 第一次list资源会设置资源版本号为空 [src]，旧版会设为0 [src]，拉完后就更新资源版本 [src]，后面watch的时候只要关心比这个资源版本大的资源。list的时候会把ListWatch对象包裹在pager对象里 [src]，这个对象的作用是控制分页查询，比如资源对象太多时，为了防止过大的网络IO，pager可以通过控制url的limit和continue参数来指定一次请求获取的资源数量 [src]。 Resync的时候实际上会调用DeltaFIFO的Resync函数 [src]，这个上面说过了。 watch的时候会开启一个死循环 [src]，ListerWatcher会返回要一个watch对象及其内部的一条channel，没有数据时则一直阻塞监听channel，只要有新资源变化就会停止阻塞 [src]，然后就根据事件类型往DeltaFIFO里面更新数据 [src]，最后会更新最新资源版本。 每次向apiserver发起watch请求，如果大概8分钟内都没有任何事件，则apiserver会主动断开连接，断开连接则会关闭watch对象的channel [src]，Reflector监听channel结束，然后会再次构建watch对象并发起watch请求。 ListAndWatch()会被Run()调用。Run()里面把ListAndWatch()包裹在了一个重试函数wait.Until()里面，ListAndWatch()正常情况下是死循环，一旦ListAndWatch()发送错误就会返回，wait.Until()在指定时间后又会重新执行ListAndWatch() [src]。这一步也叫所谓的ReList。再一次list资源时会尝试传入一个上次list到或最新watch到的资源版本，但并不保证可以成功list，比如watch到的Pod的资源版本和PodList的资源版本没有任何关联，Pod的更新不代表PodList的更新，这里只是尝试一下而已，如果list失败了就把url参数resourceVersion置为空 [src]，这样就能拉最新的列表。 类型检查 Reflector是反射器的意思，Reflector确实做了反射，它把ListerWatcher得到的对象反射出具体对象，然后与成员期望对象expectedType进行比较 [src]，若类型不符则不放入DeltaFIFO里。 ListerWatcher ListerWatcher分别继承了Lister和Watcher的接口，而ListWatch结构体则实现了ListerWatcher接口， 123456789101112131415161718192021222324type Lister interface &#123; List(options metav1.ListOptions) (runtime.Object, error)&#125;type Watcher interface &#123; Watch(options metav1.ListOptions) (watch.Interface, error)&#125;type ListerWatcher interface &#123; Lister Watcher&#125;// 结构体type ListWatch struct &#123; ListFunc ListFunc WatchFunc WatchFunc&#125;// 仅调用成员函数变量func (lw *ListWatch) List(options metav1.ListOptions) (runtime.Object, error) &#123; return lw.ListFunc(options)&#125;func (lw *ListWatch) Watch(options metav1.ListOptions) (watch.Interface, error) &#123; return lw.WatchFunc(options)&#125; ListWatch可以通过cache.NewListWatchFromClient()构建 [src]， 1podListWatcher := cache.NewListWatchFromClient(clientset.CoreV1().RESTClient(), \"pods\", v1.NamespaceDefault, fields.Everything()) 但核心是通过cache.NewFilteredListWatchFromClient()构建的 [src]，所谓的ListFunc和WatchFunc，内部其实是发起http请求，请求是由RestClient发起的。 注意的是cache.NewListWatchFromClient()这个接口只是cache包里面提供的接口，但到了后面讲到SharedIndexInformerFactory的时候，内部资源接口都被封装好了，其内部不会用这个。 RestClient和restclient.Request RestClient继承了http.Client，下面是其结构体，保留部分重要参数， 1234567891011type RESTClient struct &#123; base *url.URL versionedAPIPath string createBackoffMgr func() BackoffManager rateLimiter flowcontrol.RateLimiter Client *http.Client // ...&#125; base：APIServer的地址，比如https://192.168.1.2:6443 versionedAPIPath：资源访问路径，比如/apis/apps/v1 createBackoffMgr：退避管理器的构造函数，这部分在workqueuq那里再详细讲 rateLimiter：限速器，这部分在workqueuq那里再详细讲 Client：标准库http客户端 RESTClient其实特别简单，最终还是会传入restclient.Request结构体体里面， 12345type Request struct &#123; c *RESTClient pathPrefix string // ...&#125; pathPrefix是base与versionedAPIPath的结合，比如https://192.168.1.2:6443/apis/apps/v1 [src]。 Request通过函数式编程来构造最终的请求url，参考NewFilteredListWatchFromClient里面的实现： 123c.Get().Namespace(namespace).Resource(resource).VersionedParams(&amp;options, metav1.ParameterCodec).Do(context.TODO()).Get()c.Get().Namespace(namespace).Resource(resource).VersionedParams(&amp;options, metav1.ParameterCodec).Watch(context.TODO()) RESTClient.Get()之后就返回一个restclient.Request。对于Get请求，在Do()内部会发起http请求，然后返回一个Result的结构体，[src]，后面的Get()方法不是发起请求而是把得到的对象返回。 对于Watch，在Watch()里面，还是发起http请求，只不过它是一个长连接，可以持续从http.Response读取数据 [src]，这时会构造一个StreamWatcher对象 [src]，它通过NewStreamWatcher()方法构造，方法内部会开启一个协程无限循环并阻塞读取数据 [src]，然后把读取的数据放入channel，提供给外部读取。 额外提一句，StreamWatcher的定义不在client-go里，而是在apimachinery这个仓库里，有兴趣的话可以了解一下这个仓库。 Controller Controller的结构如下： 1234567891011121314type controller struct &#123; config Config reflector *Reflector // ..&#125;type Config struct &#123; Queue ListerWatcher Process ProcessFunc ObjectType runtime.Object FullResyncPeriod time.Duration // ...&#125; 你会发现它持有了DeltaFIFO (Queue), ListerWatcher, Reflector，可以说是把功能都整合进来了。它的功能特别简单，就是在Run函数里不断地调用DeltaFIFO的Pop并把对象传进Process函数里面。这一点在DeltaFIFO章节有提过，就不多说了。 上面提到的Informer和IndexInformer本质上就是一个Controller [src]。 SharedIndexInformer SharedIndexInformer在IndexInformer基础上又加了一个Shared，我的理解是他把Controller和indexer整合在一起，并且可以传入多个handler，所以就叫SharedIndexInformer。 使用如下，只返回一个SharedIndexInformer。 123456789s := cache.NewSharedIndexInformer(podListWatcher, &amp;v1.Pod&#123;&#125;, 0, cache.Indexers&#123;&#125;)s.AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123;&#125;, UpdateFunc: func(old interface&#123;&#125;, new interface&#123;&#125;) &#123;&#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123;&#125;,&#125;)s.Run(stopCh) 结构如下： 123456type sharedIndexInformer struct &#123; indexer Indexer controller Controller processor *sharedProcessor // ...&#125; 主要接口有以下，我保留了一些重点的函数， 12345678type SharedInformer interface &#123; AddEventHandler(handler ResourceEventHandler) GetStore() Store GetController() Controller Run(stopCh &lt;-chan struct&#123;&#125;) HasSynced() bool // ...&#125; 其中AddEventHandler()说明可以添加多个handler，每个handler会被一个ProcessListener结构包裹 [src]，所有ProcessListener最终会放入成员变量processor里 [src]。如果SharedInformer已经Run()起来后，再给它新增handler，则会把indexer的所有对象都给该handler处理一次 [src]。 在Run()函数的工作就是构造controller并调用其Run()而已，controller的关键函数Process传进的是HandleDeltas() [src]。该函数会调用processor的distribute()函数，把对象分发给所有ProcessListener，并调用相关处理函数 [src]。 ProcessListener里面有个比较有意思的缓冲实现，当外部加入对象是会放进addCh，当外部处理时会从nextCh取，那么理论上只要弄一个channel就好了呀？但问题在于nextCh和用户自定义的handler是同一协程下执行的，用户可能写了效率很差的代码，导致从nextCh读取速度比写入速度慢，进而导致nextCh空间占满，最终导致阻塞。 12345type processorListener struct &#123; nextCh chan interface&#123;&#125; addCh chan interface&#123;&#125; pendingNotifications buffer.RingGrowing // ... ProcessListener使用了一个pendingNotifications无限缓冲区，先把addCh得到的对象写入pendingNotifications，再把对象从pendingNotifications取出存入nextCh，源码里把这部分转换写得非常精妙，可以参考一下 [src]。 SharedInformerFactory SharedInformerFactory可以说是k8s资源访问的完全集合体，它能访问所有官方的API资源。可以先看看SharedInformerFactory的接口 [src]： 123456789101112131415161718192021222324type SharedInformerFactory interface &#123; internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh &lt;-chan struct&#123;&#125;) map[reflect.Type]bool Admissionregistration() admissionregistration.Interface Internal() apiserverinternal.Interface Apps() apps.Interface Autoscaling() autoscaling.Interface Batch() batch.Interface Certificates() certificates.Interface Coordination() coordination.Interface Core() core.Interface Discovery() discovery.Interface Events() events.Interface Extensions() extensions.Interface Flowcontrol() flowcontrol.Interface Networking() networking.Interface Node() node.Interface Policy() policy.Interface Rbac() rbac.Interface Scheduling() scheduling.Interface Storage() storage.Interface&#125; Apps, Core, Batch之类的方法能明显知道它可以访问对应的API Group的资源。 还有一个同名接口 [src]： 12345// client-go/informers/internalinterfaces/factory_interfaces.gotype SharedInformerFactory interface &#123; Start(stopCh &lt;-chan struct&#123;&#125;) InformerFor(obj runtime.Object, newFunc NewInformerFunc) cache.SharedIndexInformer&#125; Start() 很明显就是用来启动的 InformerFor() 可以看出是用来构造SharedIndexInformer的 看一下结构体，这里只保留重要的参数： 12345678type sharedInformerFactory struct &#123; client kubernetes.Interface defaultResync time.Duration informers map[reflect.Type]cache.SharedIndexInformer startedInformers map[reflect.Type]bool // ...&#125; client: kubeernetes的clientset，它是一个包装了各种RestClient的结合 [src]，client-go里面把所有官方API Group及其资源类型都做了RestClient包装，省去了自己从头构造RestClient。 defaultResync: resync的时间间隔 informers: 资源类型 到 其Informer的映射 startedInformers: 记录informer是否已经启动的Map 构造函数特别简单，只要传入clientset即可和一个resync的时间间隔， 12kubeClient, err := kubernetes.NewForConfig(cfg)kubeInformerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second*30) 其实构造一个SharedInformerFactory基本没有做什么工作，只有当你尝试获取某种资源的时候，才会开始构造Informer，比如我们需要一个访问deployment的Informer，就这么调用， 12deploymentInformer := kubeInformerFactory.Apps().V1().Deployments()informer := deploymentInformer.Informer() 看一下Deployments()干了什么 [src]， 123func (v *version) Deployments() DeploymentInformer &#123; return &amp;deploymentInformer&#123;factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions&#125;&#125; 里面在构造一个DeploymentInformer结构体，看一下它的接口： 1234type DeploymentInformer interface &#123; Informer() cache.SharedIndexInformer Lister() v1.DeploymentLister&#125; 你只能调用两个方法，Informer()获得一个SharedIndexInformer，Lister()获得一个DeploymentLister，DeploymentLister大概能猜出是一个通过indexer获取数据的接口，不纠结，关键在于Informer()，看一下实现方法 [src]： 123func (f *deploymentInformer) Informer() cache.SharedIndexInformer &#123; return f.factory.InformerFor(&amp;appsv1.Deployment&#123;&#125;, f.defaultInformer)&#125; 可以看到，其调用了factory的InformerFor方法，并传入了资源类型和一个构造函数defaultInformer()，defaultInformer()内部就是调用了cache.NewSharedIndexInformer()而已 [src]，这个前面有说过，就不多解释了。关键是factory的InformerFor做了什么。 InformerFor的实现有点长，我就不贴代码了，主要就是先判断一下成员变量informers里面有没有这个资源对应的Informer存在，如果不存在，就利用传进来的构造函数构造一个并存起来 [src]，最后返回构造出来的informer。 这时候就都串起来了，SharedInformerFactory在你调用到对应资源时，会先检查一下自己的缓存里是否已经创建了该资源对应的informer，如果创建了就直接返回informer；如果没有创建则用预先定义好的构造函数创建一个并存起来，也会返回这个informer。 这里之所以把整个流程捋一遍，是为了明白SharedInformerFactory是如何做资源访问集合的，了解这一点后你就能明白官方的代码生成工具code-generator生成的东西是干什么用的。看到这里就知道了生成的generated下的三个目录clientset, informers, listers的代码分别在哪里被使用。 SharedInformerFactory还有一个Start()接口，内部实现就是遍历成员informers列表，然后为每一个informer开一个协程去执行它的Run()方法 [src]。 Workqueue 和 RateLimiting 如果有看过官方的例子，就会发现用一个Workqueue把从Informer获取到的对象缓存起来是常规做法，然后会开若干线程去消费Workqueue队列。 1234567891011121314deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; key, _ := cache.MetaNamespaceKeyFunc(obj) workqueue.Add(key) &#125;, UpdateFunc: func(old, new interface&#123;&#125;) &#123; key, _ := cache.MetaNamespaceKeyFunc(new) controller.handleObject(key) &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; key, _ := cache.MetaNamespaceKeyFunc(obj) workqueue.Add(key) &#125;, &#125;) 这里就简单讲讲Workqueue加上RateLimiting（限速器）的实现。底层队列的类型实现可以看这里 [src]，Kubernetes很喜欢用数组来当队列。而带有限速器的实现可以看这里 [src]。 插入时会调用Add(item)。还有一个插入接口是AddRateLimited(item)，主要用于延迟对象的插入，在controller里面主要是用来延迟多次插入的对象。主循环可能处理某个对象失败了，可能是因为闪断等不可抗因素，为了不影响处理后面的对象，这时会再插到队列最后，但这可能会导致不断插入又不断取出的死循环，所以对于再次插入的对象会给予一定延迟，延迟时间随插入次数不断增大。 要记录对象的延迟就需要Map缓存，所以需要一个额外的接口去清理缓存，需要用接口Forget(item)。 获取数据时会调用Get()，队列内部还一个正在处理的processing缓存，每当你Get出这个对象时，就会把对象插入processing缓存中，如果要清理缓存，必须调用Done(item)接口。你不能往队列插入一个已经存在于processing缓存的对象。 因为队列会被多线程消费，可能导致同一个对象重复加入队列又同时被多个线程处理，所以在Add(item)时，如果发现一个对象已经存在队里中，需要用一个脏数据缓存dirty先存起来，在Done(item)函数中，把对象从队列移除后，会把该对象从dirty缓存取出再加入队列。 构造 构造方法有两种，一种是不指定名称，一种是指定名称 [src]， 12workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"Foos\") AddRateLimited 内部调用了AddAfter，即延迟多少时间后再插入队列。 123func (q *rateLimitingType) AddRateLimited(item interface&#123;&#125;) &#123; q.DelayingInterface.AddAfter(item, q.rateLimiter.When(item))&#125; 123456789101112131415161718192021// AddAfter adds the given item to the work queue after the given delayfunc (q *delayingType) AddAfter(item interface&#123;&#125;, duration time.Duration) &#123; // don't add if we're already shutting down if q.ShuttingDown() &#123; return &#125; q.metrics.retry() // immediately add things with no delay if duration &lt;= 0 &#123; q.Add(item) return &#125; select &#123; case &lt;-q.stopCh: // unblock if ShutDown() is called case q.waitingForAddCh &lt;- &amp;waitFor&#123;data: item, readyAt: q.clock.Now().Add(duration)&#125;: &#125;&#125; 这里数据会插入一个channel中，会有一个单独的协程监听waitingForAddCh，取出对象后插入一个优先队列里面，一个无限循环尝试从优先队列里面取出对象然后加入队列，和计时器的实现类似。这部分代码可以在func (q *delayingType) waitingLoop()看到， 关键在于rateLimiter.When如何计算时间， 1234567891011121314151617181920func (r *ItemExponentialFailureRateLimiter) When(item interface&#123;&#125;) time.Duration &#123; r.failuresLock.Lock() defer r.failuresLock.Unlock() exp := r.failures[item] r.failures[item] = r.failures[item] + 1 // The backoff is capped such that 'calculated' value never overflows. backoff := float64(r.baseDelay.Nanoseconds()) * math.Pow(2, float64(exp)) if backoff &gt; math.MaxInt64 &#123; return r.maxDelay &#125; calculated := time.Duration(backoff) if calculated &gt; r.maxDelay &#123; return r.maxDelay &#125; return calculated&#125; 可以发现，rateLimiter会有Map缓存failures，对象第一次进来时获取空对象exp=0，以后每次进来exp+1，延迟时间的公式是delay = base * 2 ^ exp。 Discovery Discovery的作用其实用来发现集群中的一些版本、API组和资源。 1234567type DiscoveryInterface interface &#123; RESTClient() restclient.Interface ServerGroupsInterface ServerResourcesInterface ServerVersionInterface OpenAPISchemaInterface&#125; ServerGroupsInterface：获取所有API组metav1.APIGroupList ServerResourcesInterface：用于获取某个组的资源列表metav1.APIResourceList ServerVersionInterface：用于获取集群版本 OpenAPISchemaInterface：用于获取所有API的语法文档 如果想知道，OpenAPISchemaInterface获取的是什么东西，可以尝试把apiserver代理到8080端口，然后访问下面的url，看看test.log里面输出了什么。 12kubectl proxy --port=8080curl &quot;http://localhost:8080/openapi/v2&quot; | jq . &gt; test.log 看看具体实现： 1234type DiscoveryClient struct &#123; restClient restclient.Interface LegacyPrefix string&#125; restClient：http客户端 LegacyPrefix：核心API的前缀，默认就是/api。k8s默认核心api的前缀是/api/{version}，而非核心的则是/apis/{group}/{version} [src] 具体方法就是实现了DiscoveryInterface的方法，其实就是url的拼接，然后访问apiserver获取数据 [src]。构造可以调用NewDiscoveryClient() [src]。 还有一个带缓存的接口： 12345type CachedDiscoveryInterface interface &#123; DiscoveryInterface Fresh() bool Invalidate()&#125; Fresh()：用于判断是否需要重新获取数据 Invalidate()：设置需要重新获取数据 Fresh()这个函数名看起来是动词，像是用来重新拉取数据的接口，但在官方的实现里面这个只是一个返回布尔值的判断。 可以参考memCacheClient的实现 [src]。内部有一个变量cacheValid用来记录数据是否合法，当你通过DiscoveryInterface的接口获取数据时，若发现cacheValid=false，则重新拉取数据，拉完后把cacheValid设为true。而Invalidate()函数则负责把cacheValid设为false和清理缓存。 RESTMapper RESTMapper这个接口会时常用到，虽然不在client-go包里，而是在apimachinery包里，但还是拿出来说一下： 1234567891011121314type RESTMapper interface &#123; KindFor(resource schema.GroupVersionResource) (schema.GroupVersionKind, error) KindsFor(resource schema.GroupVersionResource) ([]schema.GroupVersionKind, error) ResourceFor(input schema.GroupVersionResource) (schema.GroupVersionResource, error) ResourcesFor(input schema.GroupVersionResource) ([]schema.GroupVersionResource, error) RESTMapping(gk schema.GroupKind, versions ...string) (*RESTMapping, error) RESTMappings(gk schema.GroupKind, versions ...string) ([]*RESTMapping, error) // ...&#125; KindFor: 通过一个可能残缺的GroupVersionResource获取一个具体的GroupVersionKind，如果有多个就error KindsFor：通过一个可能残缺的GroupVersionResource获取可能的GroupVersionKind列表 ResourceFor和ResourcesFor和上面相似。 RESTMapping：指定group, kind和version，获取一个对应的RESTMapping RESTMappings：指定group, kind和version，获取一个对应的RESTMapping列表 既然它叫REST，可以想象它的实现可以通过RESTClient从apiserver获取数据来组装这些接口，但官方的从来没有这么去实现，DefaultRESTMapper是需要通过Add()和AddSpecific()从外部手动添加的。而DeferredDiscoveryRESTMapper则是传进一个Discovery，从Discovery获取数据。 RESTMapping的构造可以参考controller-manager源码 [src]。podautoscaler就是通过RESTMapper和配置里指定的Group和Kind来找到其指向的资源实例的 [src]。","link":"/post/k8s-client-go/"}],"tags":[{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"算法设计","slug":"算法设计","link":"/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"强化学习","slug":"强化学习","link":"/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"随记","slug":"随记","link":"/tags/%E9%9A%8F%E8%AE%B0/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"I/O","slug":"I-O","link":"/tags/I-O/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"}],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"算法设计","slug":"算法设计","link":"/categories/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"},{"name":"操作系统","slug":"操作系统","link":"/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"强化学习","slug":"强化学习","link":"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"随记","slug":"随记","link":"/categories/%E9%9A%8F%E8%AE%B0/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"消息队列","slug":"消息队列","link":"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"}]}