{"pages":[],"posts":[{"title":"CentOS6升级gcc4.9","text":"前段时间在CentOS6集群上跑TensorFlow的时候遇到了glibc和gcc版本过低的问题。本篇讲升级gcc。 问题 如果你的gcc版本过低，在跑一些比较新的软件的时候，可能会报这样的错误： 1libstdc++.so.6: version CXXABI_1.3.7’ not found 那么你就需要更新gcc了，因为CXXABI_x是包含在gcc的libstdc++中的。 查看你的gcc版本 1gcc -v 查看libstdc++版本 1find / -name &quot;libstdc++.so.*&quot; 安装必要依赖包 1yum install gcc gcc-c++ automake autoconf libtool make 下载和解压 12wget ftp://gcc.gnu.org/pub/gcc/releases/gcc-4.9.2/gcc-4.9.2.tar.bz2tar xf gcc-4.9.2.tar.bz2 如果直接从官网下载的话，因为国内网络问题，会慢到怀疑人生。这里提供一个网盘地址。 链接：https://pan.baidu.com/s/1X3cv2eUBh1ViUWOeUQtrCg 密码：ya2y 下载内置依赖 12cd gcc-4.9.2./contrib/download_prerequisites 同样的，下载速度依然龟速。依赖文件包含在上面的网盘地址里了，你可以一并下载下来，再上传到服务器。 打开download_prerequisites这个脚本，发现里面的工作就是下载了几个依赖包到gcc目录外，然后解压，并构建软连接到当前gcc目录。 既然如此，我们就照猫画虎的执行就好了。 12345678tar xf ../mpfr-2.4.2.tar.bz2ln -sf mpfr-2.4.2 mpfrtar xf ../gmp-4.3.2.tar.bz2ln -sf gmp-4.3.2 gmptar xf ../mpc-0.8.1.tar.gzln -sf mpc-0.8.1 mpc 编译安装 1./configure --prefix=/usr --enable-languages=c,c++ --enable--long-long --enable-threads=posix --disable-checking --disable-multilib -j 4 表示开4个线程去编译，一般编译时间大概是20分钟左右。请确保你的系统剩余空间大于6G。如果期间出了错误，一般是缺少依赖和必要库，把依赖安装完后再次执行即可。 12make -j4make install 安装完后，去检查版本是否为4.9.2。 发现 12345678910111213141516Libraries have been installed in: /usr/lib/../lib64If you ever happen to want to link against installed librariesin a given directory, LIBDIR, you must either use libtool, andspecify the full pathname of the library, or use the `-LLIBDIR&apos;flag during linking and do at least one of the following: - add LIBDIR to the `LD_LIBRARY_PATH&apos; environment variable during execution - add LIBDIR to the `LD_RUN_PATH&apos; environment variable during linking - use the `-Wl,-rpath -Wl,LIBDIR&apos; linker flag - have your system administrator add LIBDIR to `/etc/ld.so.conf&apos;See any operating system documentation about shared libraries formore information, such as the ld(1) and ld.so(8) manual pages. 安装完后，会打印这样一段话。这段话的大概意思是linux不会自动去你自己指定的安装目录寻找gcc，你需要在环境变量中设置gcc的安装目录，比如： 1LD_LIBRARY_PATH=/your_gcc_path/ 但是，我们不需要这么干，因为我们安装的时候，路径配置到了gcc的缺省路径上了。","link":"/2018/06/30/centos-upgrade-gcc/"},{"title":"CentOS6升级glibc2.17","text":"前段时间在CentOS6集群上跑TensorFlow的时候遇到了glibc和gcc版本过低的问题。本篇先讲升级glibc。 注意，glibc是linux的核心底层库，一旦升级失败，系统基本就完蛋了。虽然这个教程我有九成把握能升级成功，但是为了避免突发情况，建议你在实际环境上升级之前，先跑一台虚拟机去模拟升级。 检测你的glibc版本 1strings /lib64/libc.so.6 | grep GLIBC 或 1ldd --version 安装必要依赖 1yum install -y wget gcc glibc kernel-devel 方法一：rpm无痛升级 可以直接用社区制作好的rpm包来轻松升级。缺点是社区维护的版本不高，目前centos6最高维护到2.17，你想装2.18的话就没有了。 下载rpm包 1234wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-2.17-55.el6.x86_64.rpmwget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-common-2.17-55.el6.x86_64.rpmwget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-devel-2.17-55.el6.x86_64.rpmwget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-headers-2.17-55.el6.x86_64.rpm ### 安装rpm包 1234rpm -Uvh glibc-2.17-55.el6.x86_64.rpm \\glibc-common-2.17-55.el6.x86_64.rpm \\glibc-devel-2.17-55.el6.x86_64.rpm \\glibc-headers-2.17-55.el6.x86_64.rpm 安装完后查看一下当前glibc的版本，如果最高到2.17的话，说明安装成功。 方法二：源码安装glibc 源码安装的缺点就是有些麻烦，要花时间下载以及编译安装。优点就是你想要的所有版本都能安装上。 下载和解压 1234cd /usr/local/srcwget https://mirrors.tuna.tsinghua.edu.cn/gnu/glibc/glibc-2.17.tar.gztar -xf glibc-2.17.tar.gz 配置编译参数 1234mkdir glibc-buildcd glibc-build../glibc-2.17/configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin 编译与安装 12makemake install 安装完后查看一下当前glibc的版本，如果最高到2.17的话，说明安装成功。 问题 你会注意到，配置安装目录的路径是/usr，这么做的目的是在安装的时候直接覆盖旧版的glibc。 网上的一些博客的教程会在配置安装目录的时候，把路径配置到/usr之外的地方，比如 1../configure --prefix=/opt/glibc-2.17 然后把旧的软连接/lib64/libc.so.6删掉，再创建新的软连接。 事实上，这种做法是有风险的，一旦你把软连接删掉，系统在那一瞬间，一些核心的命令和功能就暂时废掉了。一开始我也这么做的，当我马上再创建新的软连接时就报了各种错。为了实验可行性，还导致我重装了好几次系统。","link":"/2018/06/30/centos-upgrade-glibc/"},{"title":"博客折腾记录","text":"hexo博客的一些坑和技巧写在这里。而对于常规操作可以看官方文档。 使用pandoc作为markdown渲染引擎 默认的 hexo-renderer-marked 与 mathjax 和 katex 有很多冲突的问题，所以建议换用 hexo-renderer-pandoc。 确保你已经安装了 pandoc 。然后npm安装即可。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-pandoc --save 有些主题可能会有js代码检查依赖，把对 hexo-renderer-marked 的检查去掉即可。 使用自定义域名 在github项目的 settings 中设置 custom domain 为域名设置DNS，设置以下A记录： 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 设置CNAME记录，www.YOURDOMAIN.com 指向 xxx.github.io 在source目录下添加文件CNAME，一行一个写下 www.YOURDOMAIN.com 和 YOURDOMAIN.com","link":"/2018/01/01/blog-log/"},{"title":"卷积序列模型","text":"使用长短时期记忆（LSTM）用于序列模型取得了很好的效果，在2017年由Facebook提出了使用卷积神经网络构建Seq2Seq模型 [1]。循环神经网络通过窗口移动方式输入数据进行训练，当句子有 \\(\\large n\\) 个窗口时，获得对应的特征表示的时间复杂度为 \\(\\large \\mathcal{O}(n)\\)。而使用卷积神经网络进行并行化计算，当卷积核宽度为 \\(\\large k\\) ，其时间复杂度为 \\(\\large \\mathcal{O}(\\frac{n}{k})\\)。 Encoder结构 我们采用一维卷积对序列进行处理，假设输入序列长度为 \\(\\large T\\)，卷积核宽度为 \\(\\large f\\)，边缘为 \\(\\large p\\)，则输出宽度为 \\(\\large T + 2p - f + 1\\)。 源句子的最大序列长度为 \\(\\large T\\), 目标句子的最大序列长度为 \\(\\large T&#39;\\)，批次大小为 \\(\\large B\\)，词向量大小为 \\(\\large E\\)， 源语言词汇数量为 \\(\\large V\\)，目标语言词汇数量为 \\(\\large V&#39;\\)，第一层卷积的通道数为 \\(\\large C\\)。 整个句子的词向量为 \\(\\large X \\leftarrow emb \\in \\mathbb{R}^{T \\times B \\times E}\\) 输入卷积层之前，要先通过一个线性层将维度规范到 $$\\(\\large X \\in \\mathbb{R}^{T \\times B \\times C}\\)，同时对维度做一个转置 \\(\\large X \\in \\mathbb{R}^{B \\times C \\times T}\\) 卷积块带有残差结构 (residual)，所以计算下一个卷积块之前保留一份残差 \\(\\large R \\leftarrow X\\) 在卷积计算时，我们不希望边缘向量产生影响，所以构造一个遮罩矩阵，将 \\(\\large \\langle pad \\rangle\\) 位置值设为0， \\[ mask \\in \\mathbb{R}^{T \\times B} \\begin{cases} 0 &amp; if \\; x = pad \\\\ 1 &amp; otherwise \\end{cases} \\] 转置和扩充维度 \\(\\large mask \\in \\mathbb{R}^{B \\times C \\times T}\\)，把边缘值的通道设为0，\\(\\large X \\leftarrow X \\circ mask\\) 然后把 \\(\\large X\\) 输入到一维卷积中 \\(\\large X \\overset{conv}{\\rightarrow} X \\in \\mathbb{R}^{B \\times T \\times 2C}\\)，这里指定输出通道是输入通道的2倍，而采用Same Convolution，所以序列长度不变 我们使用门控线性单元（Gated Linear Units）对卷积块的输出进行计算 [2]，输出后通道数减半， \\[ \\begin{align} &amp; X \\overset{split}{\\rightarrow}A,B \\in \\mathbb{R}^{B \\times C \\times T} \\\\ &amp; X = A \\circ \\sigma(B) \\end{align} \\] 加上残差，\\(\\large X \\leftarrow X + R\\)；更新残差 \\(\\large R \\leftarrow X\\) 把 \\(\\large X\\) 输入到下一卷积块，即下一层 从最后一个卷积块输出后，再经过线性层得到 \\(\\large O \\in \\mathbb{R}^{B \\times T \\times E}\\) 除了 \\(\\large O\\) 外，还要输出一个向量用于注意力计算 \\(\\large U \\leftarrow O + emb^T\\) 卷积块的计算过程如图所示：（图是我自己画的，如有错误请指出） Decoder结构 Decdoer前面的结构基本与encoder一致，不同处有三个， decoder的卷积层边缘 padding 设置为 kernel_size - 1。当输出时，我们去掉序列最后的 padding 个，以保持输入输出序列相同。这么做是为了确保当前信息不会受未来信息的影响 上述 (5) 计算完GLU后，还要输入到注意力层进行计算，注意力层输出后接上 (6) 上述 (8) 后再经过一个线性层得到对所有词汇的得分矩阵 \\(\\large X \\in \\mathbb{R}^{T&#39; \\times B \\times V&#39;}\\) 注意力层结构 注意力层接收decoder的隐藏状态，目标序列词向量，encoder的输出。 保留一个残差 \\(\\large R \\leftarrow X\\) ，结合decoder的隐藏状态和目标序列词向量，\\(\\large X \\leftarrow W^TX + emb_t ,\\; X \\in \\mathbb{R}^{B \\times T&#39; \\times E}\\) 再与encoder的输出计算注意力得分矩阵 \\(\\large A \\leftarrow X * O,\\; A \\in \\mathbb{R}^{B \\times T&#39; \\times T}\\) 我们不需要 \\(\\large \\langle pad \\rangle\\) 的产生注意，所以把 \\(\\large \\langle pad \\rangle\\) 位置的得分都设置为负无穷 通过softmax对 \\(\\large A\\) 计算注意力分布，得到对齐矩阵 \\(\\large A \\leftarrow softmax(A)\\) 论文中提到，对于注意力的输出，我们还需要计算一个conditional vector，\\(\\large Co \\leftarrow A * U,\\;Co \\in \\mathbb{R}^{B \\times T&#39; \\times E}\\) 最后加上残差后输出 \\(\\large X \\leftarrow W^TX + R,\\; X \\in \\mathbb{R}^{B \\times C \\times T&#39;}\\) 不同于RNN中所有时间步共享一个注意力层，这里的每一个卷积层后面都是一个独立的注意力层，当你有10层卷积层，那么就有10个独立的注意力层。 位置向量（Position Embeddings ） 为了让卷积网络在处理序列时有一种空间感，我们要对词向量加上一个位置向量 \\(\\large emb \\leftarrow emb + pos\\_emb\\)。其中 \\(\\large pos\\_emb\\) 表示对于词汇在该句子的索引编号。 初始化策略 为了抑制加上残差导致数值持续变大而导致高方差，所以每次加上残差或者加上词向量后，都乘以 \\(\\large \\sqrt{0.5}\\) 。 在获得词向量后和输入卷积块之前进行 \\(\\large p=0.1\\) 的dropout正则化 对于卷积块的参数初始化，我们指定其正态分布 \\(\\large \\mathcal{N}(0, \\sqrt{4p/C})\\)，其中乘以 \\(\\large p\\) 是为了抵消dropout时乘以的 \\(\\large 1/p\\)。 线性层初始化参数符合 \\(\\large \\mathcal{N}(0, \\sqrt{(1-p)/N})\\) 词向量和位置向量在 \\(\\large [0, 0.1]\\) 之间均匀分布，并且 \\(\\large \\langle pad \\rangle\\) 对应的词向量设为0 生成（Generation） 在训练时，我们可以一次性把整个目标句子输入到CNN中并行计算，不用像RNN中一步一步的输入，理论上的训练速度会有所提升，实际跑起来后会因为其中大量的注意力层会把训练速度拖慢。 在decoder生成预测序列时，我们需要以递进式的输入到CNN，需要输入 \\(\\large \\frac{T(T-1)}{2}\\) 次，而RNN逐个输入也就 \\(\\large T\\) 次，所以生成速度上，CNN明显要慢。论文中提出的解决方案是把前面序列的卷积参数保留下来，不用重复计算，但并没有详细讲要怎么做，实现起来貌似难度挺大的。 参考文献 [1] Convolutional Sequence to Sequence Learning. 2017 [2] Language Modeling with Gated Convolutional Networks. 2016","link":"/2019/01/27/cnn-seq/"},{"title":"深度确定性策略梯度","text":"虽然DQN的表现很好，但它有一个致命的缺点是无法学习连续动作空间。回想一下，DQN 是根据greddy策略找到最大Q值对应的行为，即 \\(\\large a = \\mathop{argmax}\\limits_{a \\in \\mathcal{A}} Q(s)\\)，也就说每一个行为对应一个Q值，当动作是高维度的或者是连续的，那么DQN的计算代价就很大，甚至无法进行学习。这时使用策略梯度就能很好解决这个问题。我们把环境动作设计成连续的，用动作特征表示一个动作，使用策略梯度，我们不需要计算每一个动作的Q值，而是可以通过参数化近似的办法直接输出一个动作特征，而不是从某个值映射到某个动作。DDPG (Deep Deterministic Policy Gradient)算法很好的弥补了DQN的缺陷。当然，并不是DDPG只能解决连续动作问题，只要把动作特征映射到某一个动作上就能应对非连续动作空间问题了。 算法 DDPG使用演员-评论家模型（actor-critic），设计了4个参数化函数。其中actor包含策略函数 \\(\\large \\mu\\)，目标策略函数 \\(\\large \\mu&#39;\\)，而critic包含Q函数 \\(\\large Q\\)，目标Q函数 \\(\\large Q&#39;\\)。 我们的目标是把 \\(\\large \\mu\\) 参数训练好，因为届时我们获取动作特征的方法是， \\[ a = \\mu(s) \\] 在训练过程中，critic的 \\(\\large Q\\) 的作用是对actor策略 \\(\\large \\mu\\) 进行评估， \\[ \\begin{aligned} a &amp;= \\mu(s) \\\\ q &amp;= Q(s, a) \\end{aligned} \\] 通过 \\(\\large Q(s, a)\\) 可以一个评价当前actor在状态 \\(\\large s\\) 执行动作 \\(\\large a\\) 的分数。 上面提到的参数中，目标参数 \\(\\large \\mu&#39;\\) 和 \\(\\large Q&#39;\\) 作为监督者，以此来计算 \\(\\large \\mu\\) 和 \\(\\large Q\\) 函数的参数梯度。 每一个episode，我们先从经验池 \\(\\large R\\) 中抽样 \\(\\large N\\) 个转移， \\[ (s_i, a_i, r_i, s_{i+1})_i^N \\in R \\] 对于 \\(\\large Q\\) 的损失函数，我们定义为： \\[ \\begin{aligned} &amp; a_{i+1} = \\mu&#39;(s_{i+1}) \\\\ &amp; y_i = r_i + \\gamma Q&#39;(s_{i+1}, a_{i+1}) \\\\ &amp; J = \\frac{1}{N} \\sum_i (y_i-Q(s_i,a))^2 \\end{aligned} \\] 对于 \\(\\large \\mu\\) 的损函数，论文中直接给的是梯度公式。而我参考了其他人的实现，发现他们定义了一个相对简单的损失函数。 \\[ \\begin{aligned} &amp; a_i = \\mu(s_i) \\\\ &amp; J = -\\frac{1}{N} \\sum_i Q(s_i,a_i) \\end{aligned} \\] 大概理解就是，我们设法把 \\(\\large Q(s_i,a_i)\\) 最大化，即通过梯度下降，最小化其负数。 更新完梯度后，需要进行参数软更新（soft-update）： \\[ \\begin{aligned} w^{Q&#39;} &amp;= \\tau \\; w^{Q} + (1-\\tau)w^{Q&#39;} \\\\ w^{\\mu&#39;} &amp;= \\tau \\; w^{\\mu} + (1-\\tau)w^{\\mu&#39;} \\\\ \\tau &amp;\\ll 1 \\end{aligned} \\] 每次更新target参数时，我们不会完全更新，而是保留多一些过往的记忆。 论文中提到，为了让的动作更加丰富，使个体勇于探索，我们要在动作特征上加入噪音。 \\[ \\mu(s) = \\mu(s) + \\epsilon \\; \\mathcal{N} \\] 其中 \\(\\large \\mathcal{N}\\) 使用Ornstein-Uhlenbeck process算法生成噪音。其中 \\(\\large \\epsilon\\) 是一个动态的衰减系数，论文中没有提到，实际使用时如果不加可能导致训练过程非常震荡。 实验细节 论文后面附加部分提供了算法参数的一些细节。 梯度更新使用Adam，actor和critic的学习率分别为1e-4和1e-3。 \\(\\gamma\\) 为0.99 \\(\\large \\tau\\) 为0.001 参数化近似使用神经网络，actor和critic使用三层神经网络，分别是400，300和300个神经元。actor的输出层使用tanh。critic第一层输入状态特征，第一层的输出与动作特征一起输入第二层。 参数初始化，前两层使用fanin_init初始化，最后一层使用区间为[-3e-3, 3e-3]的归一化分布。 Ornstein-Uhlenbeck process的参数中，\\(\\large \\theta\\) 为0.15，\\(\\large \\sigma\\) 为0.2 还有一个就是噪音衰减的问题，这个问题论文中没有提到，但我自己测试时去发现了这个问题不容忽视。\\(\\large \\epsilon\\) 初始值为1，每完成一个episode就减去 \\(\\large d\\)，这个 \\(\\large d\\) 的很难选，假设是0.01，那么100个episode后 \\(\\large \\epsilon\\) 就衰减到0，之后就无法加入噪音了。如果你的模型训练难度较大，要训练几百上千个episode，你必须确保能够持续加入噪音，同时兼顾到训练后期减少噪音的加入。 代码实现 我用pytorch实现了DDPG算法，详见Github 测试 以下是我自己跑的测试结果，测试过程表明某些参数的调节是非常重要的。 MountainCarContinuous-v0 环境细节看这里，小车到达终点就能获得100的奖励。这个环境中小车很容易就找到前往终点的路，所以噪声的衰减可以大一些，设置 \\(\\large d=0.01\\)。 可以看到，小车通过探索很快就找到了终点，并且奖励也接近最优，但因为仍然在往动作中加入噪声，所以小车在尝试有没有更好的路径，经过一番挣扎后，小车发现别的路径都行不通，而此时噪声已经几乎没有了，所以不再进行探索，又回到了之前的获得高奖励的路径上。 如果我们设置 \\(\\large d=0.001\\)，表现如下： 可以发现，噪声衰减调小了，使得小车进行持续的探索。上图经历的episode较少，虽然最终可能找到优秀的路径，但时间代价会很大。原本很快就能解决问题的，没有必要进行长时间的探索。 Pendulum-v0 这个是一个未解决环境，即没有一个奖励区间能说明已经解决。环境细节可以参考这里。 因为环境动作多样化，我们可以进行更多的探索，所以把噪声衰减调节慢一点，设置 \\(\\large d=0.001\\)。 奖励虽然在增长，但一直在负数，后期收敛极慢。而曲线仍然在震荡是因为我们的噪声还没衰减到0。 参考文献 Continuous control with deep reinforcement learning","link":"/2018/09/18/ddpg/"},{"title":"『知晓天空之蓝的人啊』：追梦者的重新出发","text":"随着毕业的钟声敲响，学生们纷纷离开校园跃入人海，是坚守家乡，抑或去往城市，人生有路，各自精彩。当经历社会的磨难，他们将变成怎样的人呢？又是否后悔当初的决定？『知晓天空之蓝的人啊』讲述的正是一群中年人与年轻人对人生之路的选择与思考的故事。 本作是超和平Buster三部曲的最后一部，由长井龙雪负责导演，冈田麿里负责脚本，田中将贺负责人设和作画监督，他们在此之前还创作了『未闻花名』和『心灵想要大声呐喊』。我尝试分别从画面，音乐，和主题三个方面解构该作品。 画面 本作一大特点是非常写实的背景，诸多画面是基于埼玉县秩父市的实景进行复刻，而且几乎到以假乱真的程度，会让你感觉这个故事真实地发生在秩父市中。想看实景对比的话可以去看软软冰的圣地巡礼视频BV1aA411n7mx。要说缺点的话，可能就是精度不高的人物，与高精度的背景会产生一些视觉不平衡。 实景(左)与作画对比图1 实景(左)与作画对比图2 然后是一些演出较佳的画面。比如69分44秒左右的这个镜头，利用秋天枯萎的落叶与稀疏的树枝表现姐姐的失落，妹妹在近处，姐姐在远处的景深镜头，更是给观众一种在现场看着她们的感觉，加深了观众的忧伤。88分40秒左右，葵将慎之拉出房子，在慎之飞出时，给了一个葵的视角，可以看到慎之映在了蓝色的天空上，然后与屋内飞出的吉他重叠在一起，暗示慎之与断弦的吉他一般挣脱了束缚。 优秀的演出 音乐 本作音乐由横山克负责，横山克的音乐风格总是厚重而感人。当音乐响起时，画面，角色情感以及观众的心都会逐渐变重，并慢慢沉下来，随着音乐沉入极点。 71分24秒左右，葵偶然发现了茜的笔记本，并阅读笔记时，画面由暗到明，与音乐由轻到重，同步转换，而音乐在葵勇敢地做出行动时及时收住，在这个葵理解姐姐茜的关键点，恰当地渲染了感情，又绝不拖沓。 51分19秒左右，葵和慎之有一段谈话，背景音乐宛如葵的内心一般，迷茫和犹豫，在慎之弹她额头开始，似乎是下定决心撮合姐姐和慎之介，音乐也随之激昂起来。 优秀的背景音乐插入 主题 本作的一大主题是现实与梦想。秩父市是一个小地方，毕业后的慎之想前往大城市东京寻找自己的音乐梦想，而茜却选择了留下来。岁月如梭，转眼13年，当慎之介回到故乡时，整个人变了副模样，原本跋扈的红色头发没了，演奏时的眼神没了光，嘴角也不再上扬，像是被现实和岁月磨平了棱角。虽然确实在音乐上取得了成绩，但离他当初的梦想高度还是差了一大截。 而茜这边，一开始可能会让人认为是为了照顾葵才放弃梦想选择留下，包括葵自己也是这么认为的，所以才拼命想离开，让姐姐摆脱束缚，但从葵找到姐姐的“攻略笔记”后，就揭示了姐姐茜的真正梦想——照顾葵并抚养她长大。从一开始，姐姐就坚持做葵喜欢的昆布饭团而不是慎之喜欢的金枪鱼蛋黄酱饭团，姐姐的梦想从来就没变过，正如毕业纪念册上姐姐的留言：井底之蛙，不知大海之宽阔，却知晓天空之蓝。 慎之与慎之介 慎之介是幸运的，他还留下了另一个自己，被困住的慎之。年轻且充满斗志的慎之用话语与行动唤醒了麻木的慎之介，就像向枯槁的身躯重新注入了热血，枯萎的精神被注入新的灵魂，历尽千帆，仍是少年，青春不再，但不妨重新出发。 冈妈用故乡象征初心，用大城市象征梦想。也许冈妈想传达的是：故乡不是囚笼，而是绿洲，当梦想受到阻碍，你可以在这里憩息片刻，然后鼓起勇气重新上路吧，少年。 最后，以片尾曲『葵』的歌词结束本文： きっと仆たちが想像した未来は 我们所想像的未来肯定 幼い顷见つけた石ころみたいに丸っこくて 就像小时候 找到的石头一样圆 変な伤迹なんかもなくてさ 没有奇怪的伤痕 平和っていう汉字の通りなんだって思っていた 就像是「和平」这个字一样意思 肝心な凡人は梦を追って岛を出た 珍重而平凡的人为了追逐梦想而离开了小岛 胸に梦って书いて飞び出したあの大海原へ 在心中怀藏着梦想 飞奔向茫茫大海 変に泣いたって空気が浊るからさ 莫名地哭泣让空气混浊了起来 じゃあねって言う またねって言う 说了拜拜 又说了再见 石ころを空に投げた 将石头抛向空中 巨大な力で溃されそうな 被巨大的力量击溃的 孤独には その毒には 那孤独里 那毒药中 独特の世界を呼び起こす 有着呼唤起独特世界 魔法があるよ 的魔法唷 サヨナラ いつかの少年の影よ 再会了 曾几何时那少年的身影 また会おうな まだただいま 会再次见面 会再次归来 言える场所はとっておくぜ 让我保留着能说出口的地方 この大空の青さを瞳のパレットに 将广阔蓝天映入瞳孔的调色盘里 潜らせて 包み込んで 深深隐入 包覆其中 涙さえも味方に 连眼泪都深藏起来 きっと仆たちが创造した未来は 我们所创造的未来肯定 ガラスの靴や魔女が飞び交う絵本のような物语 就像是玻璃鞋般或是魔女飞舞般的童话故事 変な理由もなくそう信じていたんだ 没理由的就这样深深相信着 ヘイ ベイビー！なんておどけて笑い 嘿 Baby！那是什么荒谬地笑了 汗かいて大人になった 流汗之后 成为了大人 知らない间に溃されそうな 貌似在不知不觉中压倒了 幸せや野に芽吹く花が 在幸福的田野里发芽的花苞 どうか最期まで咲きますように 请让花朵盛开到最后 全てを光らせ 让一切闪耀光芒 サヨナラ あの日の少年の梦よ 再会了 那一天少年的梦想 また会おうか まだただいま 会再次见面 会再次归来 言えるくらいの余裕はあるだろ？ 你还能够从容地说出口吗？ この大空の青さを心のオアシスにして 将广阔蓝天变成心灵的绿洲 泳いで 潜り込んで 游向绿洲 深潜其中 また帰っておいで 还会再次归来 だから今は 所以现在 サヨナラ 少年の影よ 再会了 少年的身影 また会おうな またただいま 会再次见面 会再次归来 言える场所はとっておくぜ 让我保留着能说出口的地方 この大空の青さを瞳のパレットに 将广阔蓝天映入瞳孔的调色盘里 潜らせて 包み込んで 深深隐入 包覆其中 涙さえも味方に 连眼泪都深藏起来","link":"/2020/08/30/her-blue-sky/"},{"title":"双深度Q网络","text":"本文主要讲一下 DQN 和 DDQN，以及它们的实验对比。 深度Q网络 (DQN) 回顾一下深度Q网络的过程。 从经验池 \\(\\large R\\) 中获取 \\(\\large n\\) 个转移，\\(\\large (s_i, a_i, r_{i+1}, t_i, s_{i+1})\\)，其中 \\(\\large t_i=1\\) 表示 \\(\\large s_i\\) 是结束状态 计算状态 \\(\\large s_i\\) 的预测Q值，\\(\\large y_{pred}^i = Q(s_i)\\)，执行动作 \\(\\large a_i\\) 时的值为 \\(\\large y_{pred}^{a_i}\\) 计算状态 \\(\\large s_{i+1}\\) 的目标Q值，\\(y_{target}^i=Q&#39;(s_i)\\)，取最优动作对应的值，即最大Q值 \\(\\large y_{target}^{max}\\) 计算预期Q值 \\(\\large y_{expected}^i = r_{i+1} + (1-t_i) * \\gamma * y_{target}^{max}\\) \\(\\large n\\) 个转移的均方差损失函数 \\(\\large loss=\\frac{1}{n}\\sum_i (y_{pred}^{a_i} - y_{expected}^i)\\) 使用梯度下降最小化损失函数，更新 \\(\\large Q\\) 网络的梯度值，注意这里不对目标网络 \\(\\large Q&#39;\\) 更新 相隔一定步数后，更新目标网络的参数 \\(\\large Q&#39; = Q\\) 其中，最重要的部分是对于期望Q值的计算，我们把这部分提取出来， \\[ Y_t^{DQN}=R_{t+1} + \\gamma \\max_{a} Q&#39;(S_{t+1}, a; w^-) \\] 使用经验池进行经验重新（experience replay）早在1992年就已经提出，而使用目标网络是在2015年由Mnih等人提出。 双深度Q网络（Double DQN） 不同于DQN，我们的期望Q值的计算为， \\[ Y_t^{DDQN} = R_{t+1} + \\gamma \\max_{a} Q&#39;(S_{t+1}, \\mathop{argmax}\\limits_a Q(S_{t+1}, a; w); w^-) \\] 我们的目标网络输入的不再是指定的 \\(\\large a\\)，而是原本的Q网络的最大Q值对应的动作。 深度Q网络的过度估计 论文中提出，DQN存在普遍的过度估计（overestimations）问题。 假设预测Q值为 \\(\\large Q_t(s, a)\\)，最优价值为 \\(\\large V_*(s)\\) ，每个状态有 \\(\\large m\\) 个动作，则平均误差为 \\[ \\frac{1}{m}\\sum_a(Q_t(s, a) - V_*(s))^2 = C \\] 从误差公式上看，环境的噪声，近似函数等因素都是Q函数参数学习的一部分，意味着任何这些因素都可能导致估值偏差。 对于最大值的估计， \\[ \\max_aQ_t(s,a) - V_*(s) \\ge \\sqrt{\\frac{C}{m+1}} \\] 随着 \\(\\large m\\) 的增大，下界越来越小，而实验结果的表现是随着 \\(\\large m\\) 的增大，估计误差会越来越大，即 \\(\\large Q_t(s,a)\\) 越来越大，产生了所谓的过分乐观（Overoptimism）。 过度乐观的实验 论文中选取的真值函数和近似函数分别为： True Value \\(\\large Q_*(s,a)\\) Approx Function \\(\\large Q_t(s,a)\\) \\(\\large sin(s)\\) \\(\\large W^TX\\) (d=6) \\(\\large 2exp(-s^2)\\) \\(\\large W^TX\\) (d=6) \\(\\large 2exp(-s^2)\\) \\(\\large W^TX\\) (d=9) 测试结果如下，其中每一行是一种真值函数， 第一行第一幅图，描绘了其中一个动作，紫色是真值函数，绿色是拟合函数，可以看出6维的线性回归可以很好的拟合真值。 第一行第二幅图，描绘了10个动作的拟合函数随着的Q值曲线，用黑色描绘的是对应的最大Q值，你会发现最大值普遍大于真值。 第一行第三幅图，描绘了最大Q值与真值的误差，其中橙色曲线是DQN，蓝色曲线是DDQN。明显的，DQN的平均误差要远大于DDQN。 第三行第三幅图，你会发现9维的线性回归的拟合效果远不如6维（第二行第三幅图）。所以说明，有时候复杂的近似函数不一定取得好的效果。 实际效果 在atari游戏上运行对比。 红色是DQN，蓝色是DDQN，其中横线表示无偏估计值。会发现DQN的估计值普遍大于DDQN，而且DQN的估计值偏离很严重。 从上图发现，使用DQN的估计值（浅色区域）非常的不稳地，所以过高的估计值会影响学习效率。 总结 注意一个问题，上面我们一直都在讨论估计值的问题，DDQN能够降低估计值，使得他更接近真值。在实际训练中得到的好处就是训练过程不会太过动荡，也就提高了训练的效率。强化学习与深度学习不同，深度学习会设法得到一个高的准确度。而强化学习中无论是DQN还是DDQN，一直训练下去都能收敛到一个值上，算法的区别更多体现在训练的过程和效率上。 代码实现 我用pytorch分别实现了DQN，DDQN以及用DDQN算法运行atari游戏。详见Github。 测试结果 CartPole-v0 针对CartPole-v0问题，小车上面放着一个杆子，我们的任务是左右移动车子尽量使得杆子在车子上屹立得更久，每一步动作奖励为1，屹立时间越长得到的奖励越多，默认200步结束一个episode，gym认为一个最近100个episode的平均奖励大于195就认为解决。 我分别是用DQN和DDQN训练它，结果如下（蓝色是DQN，橙色是DDQN） PongNoFrameskip-v4 Atari Pong 游戏就是两方在玩类似于乒乓球的游戏，当对方不能打回来，你就得分。目前，这是未解决问题，即没有一个奖励区间能说明解决该问题。 我用DDQN训练2百万帧，情况如下： 训练该环境时遇到了一些问题，一开始发现奖励曲线一直在-20左右徘徊，没有增长的趋势。后来我仔细对比了一下参数，发现Adam的学习率应该使用0.0001，而不是0.001。这也证明了调参的重要性，一个小数点就可能导致模型出现严重的偏差。 参考文献 Deep Reinforcement Learning with Double Q-learning Playing Atari with Deep Reinforcement Learning","link":"/2018/08/30/ddqn/"},{"title":"Transformer双向编码器","text":"BERT，完整描述是使用双向编码器的Transformer (Bidirectional Encoder Representations from Transformers)， 是Google在2018年发布的一个神经网络模型 [1]。该模型与以往大多数NLP模型不同，它本身便是一个用于迁移训练的模型。 对于迁移训练，在计算机视觉中迁移训练已经是个常态了，一般都是在经过ImageNet数据训练的预训练模型上进行微调。如今，研究人员在处理NLP问题上也希望能够通过预训练来提高模型能力。而BERT的该论文主要阐述如何在BERT模型上进行预训练，然后基于预训练模型，对于不同任务(task)进行微调。 预训练与微调方法 早于BERT，OpenAI GPT提出了在transformer上进行预训练和微调的方法 [2]，不过与BERT不同的是，GPT采用的是单向transformer以及使用无监督数据来学习语言模型。BERT采用的方法是基于无监督数据采用有监督方法进行预训练，详细方法后面会讲到。 模型结构 bert-1.png 上图是BERT与几个相似模型的对比，只有BERT在所有层级上都加入了左右上下文的信息。 输入表示 bert-2.png 输入向量由三层组合 1. Token Embeddings 即词向量。[CLS] 表示开始标志，[SEP] 用于分割多个句子的标志 2. Segment Embeddings 段落向量。上图中的\\(\\large E_A, E_B\\) 表示两个学习过的句子A和B。为什么这么做后面会讲到。 3. Position Embeddings 位置向量。这里的位置向量不使用余弦函数，而是需要进行训练。 预训练任务 下面会介绍几个用于预训练的任务。 Masked LM BERT 最重要的特点就是双向，能够同时掌握上下文的信息。但是这里就出现了一个问题，convS2S和transformer都在致力于使用masked去清除“过去”的影响，BERT论文中提到这么做是因为在双向多层结构中词汇会在第二层以后看到“自己”，所以要清除这种干扰。 而为了使得BERT这样的双向结构能够进行良好的训练，作者提出了使用Masked LM方法 [3]，该方法就是对于输入的句子，随机遮住(mask)掉一些词汇(token)，然后让模型去预测这个词汇。明显，token被mask掉以后就不会出现自己干扰自己的情况。mask的时候使用[MASK]标记即可。 但是这么做会产生一个弊端，这个[MASK]标记在微调(fine-tuning)中是不会出现的，为了减轻该影响，作者提供了方案： 1. 80% 的情况下，替换为[MASK]，比如 my dog is hairy -&gt; my dog is [MASK] 2. 10% 的情况下，替换为随机词，比如 my dog is hairy -&gt; my dog is apple 3. 10% 的情况下，不替换 下一句预测 就是将句子A和句子B通过[SEP]拼接起来，问你句子B是不是句子A的下一句。 举两个例子： Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] Label = IsNext Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label = NotNext 微调 提取最后一层中 [CLS]标记所在隐藏状态 \\(\\large C \\in \\mathbb{R}^H\\)，以及你自己加上去的权重 \\(\\large W \\in \\mathbb{R}^{K \\times H}\\)，\\(\\large K\\) 即你的分类数。通过Softmax进行预测： \\[ \\large P = softmax(CW^T) \\] 对于超参数的调节，视不同任务而定，作者建议的参数有： Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Number of epochs: 3, 4 为了方便理解，作者提供对于不同任务的微调的图示： bert-3.png bert-4.png 实验结果 bert-5.png 从上图可以看出，BERT在各项任务上的表现基本是毫无敌手。(以上指标均为GLUE [4]) 总结 BERT的实验结果表明预训练模型的优越性，尽管BERT本身有些瑕疵，但论文本身阐述了诸多预训练的方法和理论，并且表现出强劲的实力，可以说是NLP史上的一座重要的里程碑了。 尽管BERT能够适用于诸多任务，但也有一些任务无法胜任，比如机器翻译…… 参考文献 [1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018 [2] Improving Language Understandingby Generative Pre-Training. 2018 [3] “Cloze Procedure”: A New Tool for Measuring Readability. 1953 [4] GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. 2018","link":"/2019/03/28/bi-transformer/"},{"title":"GloVe","text":"GloVe（Global Vector），是一种结合全局矩阵分解和本地上下文窗口的方法。LSA（latent semantic analysis）虽然能够有效的统计信息，但在词汇类比任务中表现很差。而Skip-gram虽然在词汇类比任务中表现很好，但依赖于窗口的移动，而不能有效统计全局的计数。论文作者认为，全局计数的对数加上双线性回归方法会非常合适。 算法 首先，我们需要对整个语料库建立一个统计全局词频的矩阵，一般称为共现矩阵（Co-ocurrence Matrix）。 假设，语料库有三句话： \\(I \\hspace{0.15cm} enjoy \\hspace{0.15cm} flying\\) \\(I \\hspace{0.15cm} like \\hspace{0.15cm} NLP\\) \\(I \\hspace{0.15cm} like \\hspace{0.15cm} deep \\hspace{0.15cm} learning\\) 那他们的共现矩阵就是： \\(\\large X_{ij}\\) 表示单词 \\(\\large i\\) 在单词 \\(\\large j\\) 上下文出现的次数。你会发现，共现矩阵是对称的（symmetric），因为他同时取左右两边的单词，若只选取左边或右边的任一边，则是非对称的（asymmetric）。 构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系： \\[ u_i^Tv_j + b_i + \\widetilde b_j \\approx log(X_{ij}) \\tag{1} \\] 这里的 \\(\\large u_i, v_j\\) 与Skip-Grams模型中的意义不同，它仅表示的是单词 \\(\\large i,j\\) 之间的联系，没有中心与周围的区分。（对于公式(1)，论文中做了相关推导，因为推导过程有些复杂，我把这部分放到文章后面。） 他的代价函数是： \\[ J = \\sum_{i,j=1}^V f(X_{ij})(u_i^Tv_j + b_i + \\widetilde b_j - log(X_{ij})) \\] 其中 \\(\\large log(X_{ij}) \\rightarrow log(1 + X_{ij})\\)，这么做的目的是防止出现 \\(\\large log0\\) 的情况。 \\(\\large f(X_{ij})\\) 是一个加权函数（weighting function），有些单词对经常同时出现，那么会导致 \\(\\large X_{ij}\\) 会很大，应该衰减它们；而有些单词对从不一起出现，这些单词对就不应该对代价函数产生任何贡献，它们没必要去计算损失值，这时使用 \\(\\large f(0) = 0\\) 来避免计算。 我们可以采用分段函数： \\[ f(x) = \\begin{cases} (\\frac{x}{x_{max}})^\\alpha &amp; if \\; x &lt; x_{max} \\\\ 1 &amp; otherwise \\end{cases} \\] 函数图像如下： 作者实验 \\(\\large \\alpha\\) 取值 0.75，\\(\\large x_{max}\\) 取值 100。 \\(\\large u_i, v_j\\) 的初始化可以不同，也可以相同。当训练得足够之后，\\(\\large u_i, v_j\\) 与 \\(\\large X_{ij}\\) 相似，而 \\(\\large X_{ij}\\) 是对称的，所以 \\(\\large u_i, v_j\\) 也是对称或接近对称。因为都是学习得到的参数，在我们看来没有本质区别。当初始化不同时，相当于加入了噪音，你可以选择把 \\(\\large e = u + v\\) 作为最终向量，这样有助于提高鲁棒性。 实验结果 论文中展示了glove与word2vec等方法的对比，但其实大家都旗鼓相当。 下图是glove在不同参数的情况下的表现： （图a 展示了向量维度对准确度的影响。图b和图c分别使用对称上下文和非对称上下文在不同窗口大小下的表现。以上所有测试都使用6B (billion) 的语料库） 可以发现最好的参数是300维度，窗口大小为5。再往上增加虽然能提高准确度，但也提高了计算代价，没有必要为了提升一点准确度而产生巨大的计算代价。 公式推导 这里对上面公式(1)进行推导。 论文中举了一个简单的例子： 试问单词 \\(\\large k\\) 应该是哪个单词？我们可以通过 \\(\\large P(k \\vert ice)/P(k \\vert stream)\\) 来判断。当 \\(\\large P_{ik}/P_{jk}\\) 很大的时候，说明 \\(\\large i\\) 与 \\(\\large k\\) 更接近一些。 然后，作者就想到，我们可以用参数去近似这个比值。 \\[ F(w_i, w_j, w_k) = \\frac{P_{ik}}{P_{jk}} \\] \\(\\large F\\) 这里暂且设为未知函数，因为它包含了三个参数，所以我们需要削减一下参数，因为参数是线性的，所以自然可以构造成相减的形式。 \\[ F(w_i - w_j, w_k) = \\frac{P_{ik}}{P_{jk}} \\] 然后我们用内积来表现两个单词向量的近似程度， \\[ F((w_i - w_j)^Tw_k) = \\frac{F(w_i^Tw_k)}{F(w_j^Tw_k)} \\] 提取其中一个单词的 \\(\\large F\\)， \\[ F(w_i^Tw_k) = P_{ik} = \\frac{X_{ik}}{X_i} \\] 我们设 \\(\\large F\\) 为指数函数，则， \\[ w_i^Tw_k = log(P_{ik}) = log(X_{ik}) - log({X_i}) \\] 然后，我们用偏置项来替代 \\(\\large log(X_i)\\)，即 \\[ w_i^Tw_k + b_i + b_k = log(X_{ik}) \\] 就这样，这个公式就被奇异地构造出来了。 代码实现 我自己用python实现了一下，但跑起来后发现这没什么意义。GloVe与Skip-gram不同，他构造共现矩阵需要花费大量时间和内存，这部分极度需要计算性能，用python跑半天都跑不完。 论文作者是使用纯C语言实现的，提供了不少预训练的模型，以及词向量的评估脚本，详见Github。 参考文献 GloVe: Global Vectors for Word Representation - Stanford NLP","link":"/2018/11/20/glove/"},{"title":"无锁环形缓冲","text":"无锁环形缓冲(Lock-free RingBuffer)，又称无锁管道，是一个在不使用锁的情况下允许多线程访问的缓冲区，是在竞争条件下维护数据的最高性能选择之一。该方法早在1994年的 Implementing Lock-Free Queues 论文中就开始被研究，该文尝试实现了无锁链表，而本文将尝试实现无锁的环形数组。 构成 一个RingBuffer主要包含如下字段： buf 缓冲数组，每个元素作为结构体node，包含了pos字段表示元素位置，data字段表示用户插入的数据。 cap 容量，整数 mask 容量减一，整数 queue 插入数据的指针，整数 dequeue 获取数据的指针，整数 disposed 是否停止阻塞，布尔 方法有如下： Put(v) 插入数据 Get() 获取数据 Cap() 获取容量 Len() 数据数量 Dispose() 中断阻塞 算法过程 下面用图来分析插入和获取数据的过程。 一开始，一个容量为4的缓存区里面每个node都自带从0开始递增的pos，queue和dequeue指针都是0。 当插入数据时，会在queue指向的位置插入数据，该位置pos加一；然后queue指针会移动指向下一个node，即值加一。 同理，我们再依次插入两个数据，这时queue为3，指向第4个node。 当获取数据时，会获取dequeue指向的位置的数据，然后该位置的pos=dequeue+cap，dequeue移动到下一个node，即加一。之所以pos=dequeue+cap是为了表示该位置已经空出，以便于插入时能通过queue判断出这一情况。 我们继续插入数据，这次会把数据插入到第4个node中，然后queue移动。 当获取数据后，第一个node就空了出来，意味着我们可以把数据插入到第一个node中。 当插入数据时，会作判断，必须是pos-queue=0时，我们才认为可以在该位置插入该数据，如果pos-queue&lt;0，意味着该位置的数据还没被获取，你不能去覆盖它，一般遇到这种情况会在一个循环中阻塞，直到该位置的数据被Get()或者被Dispose()。实际实现时，pos, queue等属性可能会作为无符号整型，pos-queue&lt;0的判断则应该变为pos-queue&gt;0。 若想获得当前缓冲区中有多少数据还没被获取，即Len()函数，则只要queue-dequeue即可。若想中断阻塞或将管道改为非阻塞，则只需要将disposed字段置为true，循环会自行打断。程序实现可以参考我写的这一版rtd/ringbuf.h。 无锁原因 之所以环形缓冲能实现无锁，是因为采用了双指针和CAS。双指针分别指向插入数据位置和获取数据位置，读写时互不干扰，唯一会发生干扰的情况就是当缓冲区满了，两者指向同一位置，必须等到该位置数据被Get之后，即其中一个指针被移开，数值发生了变化后，才能继续插入，而又这里仅仅涉及数值上的比较和更改，这部分完全可以简化为CAS原子操作。 Golang里面的channel用的也是类似方法，但也并非完全无锁，作为标准库必须最大程度确保安全，侧面说明了目前无锁队列的研究并没有完全结束。最理想的情况应该是基于链表实现的无锁队列，因为链表可以实现动态扩容，而环形数组会遇到装满后而必须阻塞的情况。无锁环形数组比较著名的应用有Disruptor，而无锁链表尽管多年来出了不少论文，但在工业上被使用的情况还是很少。 参考 Implementing Lock-Free Queues go-datastructures/queue/ring.go bounded-mpmc-queue Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms","link":"/2020/07/19/ring-buffer/"},{"title":"Kafka 分析","text":"分布式消息队列的优势是很好地对计算机系统进行了解耦，不同系统之间通过消息队列来交换消息。Kafka是分布式消息队列的代表作之一，很好的兼顾了性能和一致性，但代价就是使用起来更加复杂。我花了点时间阅读了Kafka 0.10版本的源码，本文不对繁杂的源码细节和实现做描述，只关注架构和思路。 1 主题 (topic) 主题是消息队列用来区分数据的逻辑概念，可以把不同类型的数据分开，让消费者只获取某个主题的数据。 2 分区 (partition) 知道主题之后，最直观的想法是，一个主题一条队列，这样就能做到只获取某一个主题的数据，这个想法是对的。但分布式的呢，可以想到一个主题多条队列，一条队列放在节点上，这样消费者就能充分发挥分布式IO效率获取该主题的数据。但是这样还不行，万一个节点挂了，存在该节点上的所有队列就丢失了。所以，每一条队列还要有副本，存放在不同的节点上，当某个节点挂了，消费者就可以从别的节点上的副本中找数据。而这条队列，就称为“分区”。 如上所说，每个分区的数据会维持多个副本分散在不同的节点上，其中有一个leader和若干个follower。写入数据时先写到leader中，随后leader将数据拷贝到followers上。如果某个follower挂了不会有影响，因为获取数据时优先从leader获取，kafka就再找一个节点作为该分区的新follower，然后让leader把数据拷贝过去。当leader挂了，就从followers上选出一个新leader。 3 生产者 生产者负责将用户输入的数据存入分区中。 3.1 发送方式 生产者客户端通过类似于 send(data, callback) 的函数发送输入数据，先经过自定义的拦截器做一些处理，下一步将数据序列化，通过哈希函数计算出该存到哪个分区上，然后将数据放入分区对应的缓冲区中。每个缓冲区可以存放多个batch，每个batch有一个内存块，内存块序列存储用户的输入数据。每次请求以batch为单位。输入数据先经过压缩器的压缩，然后插入到内存块中，插入后会记录一个元数据。如果输入数据超出内存块大小就新建一个batch，默认大小是16Mb。 元数据主要存储时间戳，crc，数据所在分区，在batch中的偏移offset，数据大小等参数，这个元数据不会发送出去，会作为用户回调函数的参数，用户可以获得这些参数，具体用途由用户自己决定。 每一个输入数据对应有一个thunk，我们把回调对象和元数据存入该对象中。当该batch发送成功并响应时，会遍历该batch下的所有thunk，调用每个thunk的回调方法。如果没有收到响应就重新发送。 我们开启一个IO线程，线程不断从缓冲区中获取batch。IO线程维护若干个客户端对象，把batch分给其中一个客户端，由它负责发起请求，发起请求可以是多线程的。每发起一个请求后就会把该请求缓存到一个等待响应队列中，当响应成功，就把请求从队列中去掉，我们也可以通过该队列判断客户端的负载情况。 3.2 集群元数据 客户端对象内部维护一个集群元数据，由此知道每个分区应该发送到哪个节点上，这些数据需要从zookeeper服务中获取集群元数据信息，那什么时候获取元数据呢？一个是定期请求，比如3分钟一次，另一个是发送数据时发生异常。一旦发起获取元数据请求，其他线程中的请求需要阻塞，直到元数据成功获取后再唤醒它们。 4 消费者 消费者负责从分区中获取数据。采用主动拉取的方式，通过类似于 poll() 的函数单线程轮询服务端并获得数据。生产者只需要把数据扔进分区中即可，而消费者不单纯把数据从分区中拿出来，它要考虑数据的一致性问题。 比如生产者生成了一个订单，扔进了分区。而消费者从该分区中拿出来，万一在处理过程中消费者挂掉了，那这个订单算怎么回事？你必须确保这个订单能够被正常处理。 4.1 一致性 总体来说，确保数据的一致性有两个方面，一个是确保数据不丢失，另一个是确保数据不重复。 这就是所谓的传递保证(Delivery guarantee semantic)： At most once： 消息可能会丢， 但绝不会重复传递。 At least once： 消息绝不会丢， 但可能会重复传递。 Exactly once： 每条消息只会被传递一次。 确保数据不丢失这个问题，我们可以通过偏移和提交来解决。高可用的消息队列一定会把数据存储起来，而不是单纯的将数据从一个地方传到另一个地方。因为我们使用了分区，所以可以由指定一个消费者去消费某一个分区的数据。而消费者根据偏移每次获取一份数据，处理完后就提交这个偏移。万一中途挂了，因为偏移没有被提交，所以仍然认为这个数据没有被处理。 对于重复的问题，生产者为了确保数据已安全放入分区，需要等一个响应。当没有等到响应时怎么办呢？如果数据发送过程出错，那就重新发送即可。但也有可能数据已经放入分区了，但响应包丢失了，这时生产者重新发送就会造成数据重复。一个比较好的解决办法是，每个消息一个全局唯一ID，如果消费者发现ID重复，就忽略掉。 4.2 分区分配策略 每个消费者会被分配到一个或多个分区，但可能随时加入新的消费者，或移除旧的消费者，不管如何，你要确保所有分区都有消费者在消费。也就是说，当加入新消费者或移除旧消费者，你要为所有消费者重新分配新的分区，这一行为称为再平衡(Rebalance)。 我们把对监听同一个主题的消费者看成是一个组(Consumer Group)，我们需要把该主题下的所有分区分配给该组消费者。 4.2.1 Rebalance 在讲怎么分配之前，我们需要先知道Rebalance的过程。 一般操作是，服务端维护了名为GroupCoordinator的中心控制台，当新开启一个消费者，消费者会获取一个GroupCoordinator，消费者会发送消息到中心控制台。中心控制台根据该消费者监听的主题从对应的组里选出一个Leader，然后给该组所有成员发送消息，告诉它们现在有新成员加入要重新分配任务，都把手动上的事情停一停。期间，只有Leader能知道该组所有成员的信息，Leader计算出分区分配结果，然后发给中心控制台，中心控制台再发给该组的所有成员，成员们拿到分区信息后就能开始工作了。 一个消费者同时监听多个主题的情况也是同样适用的，相当于一个消费者同时属于多个组而已。 4.2.2 分配方法 分配方法是可以在启动消费者时自定义的，但使用内置方法也是可以的。 方法一，RangeAssignor：针对每个Topic，n=分区数/消费者数量， m=分区数%消费者数量，前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个Partition。 方法二，RoundRobinAssignor：将所有Topic的Partition按照字典序排列， 然后对每个Consumer进行轮询分配。 举个例子，有C0、C1两个消费者和t0、t1两个Topic，每个Topic有三个分区编号都是0～2。使用RangeAssignor的分配结果是：C0: [t0p0, t0p1, t1p0, t1p1]， C1: [t0p2, t1p2]；使用RoundRobinAssignor的分配结果是：C0: [t0p0, t0p2, t1p1]、C1: [t0p1, t1p0, t1p2]。 4.3 心跳 服务端是多节点的，GroupCoordinator可能处于不同节点上，当节点挂掉，节点上的GroupCoordinator也就挂掉了。所以要确保在节点挂掉后，消费者能找到新的GroupCoordinator。这时就需要心跳协议，消费者定时向GroupCoordinator发送心跳，如果响应超时则认为该GroupCoordinator挂掉了，消费者需要再次获取一个GroupCoordinator。 反过来，如果GroupCoordinator长时间没有收到消费者的心跳，它也会认为消费者挂掉了，就会触发重新分配(Rebalance)。 5 服务端 服务端负责接收和存储生产者发送的消息，以及将数据提供给消费者。因为是多节点的，我们把每个节点称为Broker。 5.1 存储 在分布式领域中，我们一般把消息或数据看成日志，以日志的方式存储。一个Topic可以有多个分区，每个分区使用偏移量offest来标识一份数据。 物理存储结构是以&lt;topic_name&gt;-&lt;partition_id&gt;的目录存储，目录下保护一个分区的日志文件.log，日志文件可以分成多个，以该文件中第一份数据的偏移量作为文件名，比如0003458.log。 实际存储时，并不是来一条消息就马上写入文件中，而是尝试将多条消息合并压缩后再写入文件，这样可以节省存储空间。压缩后的结构将变为内层消息和外层消息，内层消息是真实的单条消息，外层消息是内层消息的集合。外层消息的偏移量以最后一条内层消息为准，同时维护一个内部偏移量。 为了提高查询效率，每一个日志文件附带一个索引文件0003458.index。索引文件存储消息的位置和偏移量的对应关系，已知偏移量，可以通过索引文件快速知道该消息在log文件中的位置。为了节省空间，索引文件使用了稀疏存储的方式。 通过对索引文件记录的二分查找，可以快速找到消息所在位置的附近，然后顺序查找一下即可。 一个log和它对应的index文件在程序中看作一个LogSegment。一个分区的所有LogSegment以跳表形式存储。 5.2 副本 每个分区维护多个副本，并且会从其副本集合中选出一个副本作为Leader副本，所有的读写请求都由选举出的Leader副本处理。剩余的其他副本都作为Follower副本，Follower副本会从Leader副本处获取消息并更新到自己的Log中。 一般情况下，同一分区的多个副本会被均匀地分配到集群中的不同Broker上，当Leader副本的所在的Broker出现故障后，可以重新选举新的Leader副本继续对外提供服务。 涉及到选举，那么就必须关心谁有资格参与选举。副本的同步涉及到多个节点，从Leader发往Followers，总有一些Followers同步速度跟不上，只有跟上Leader同步进度的Followers才有资格参与选举。所有副本统称为AR(Assigned Repllicas)，保持同步的副本称为ISR(In-Sync Replicas)，同步滞后的副本称为OSR(Out-Sync Relipcas)，能够参与选举的必须是该分区处于ISR的节点。 5.3 Broker Leader 因为存在多个Broker，为了方便管理，需要选出一个Leader来管理其他所有Broker。 不使用Leader的话，每个Broker都各自从ZooKeeper中获取元数据，而ZooKeeper本身的数据也在不断更新，这种情况下，不同Broker发起请求的时间差会导致可能获得到不同的数据，出现“脑裂”，“羊群效应”等问题。 使用Leader后，只有Leader能与ZooKeeper交互，Followers发生了什么问题统一向Leader请求指示，比如某分区Leader副本故障，或新增了消费者等。当然，要是Leader挂了，同样要从Followers中选举出新的Leader。","link":"/2020/07/04/kafka/"},{"title":"词性标注的简单综述","text":"词性标注(Part-of-Speech Tagging, 简称POS tagging)是将句子中的每个词做一些标记，如动词，名词，副词，形容词等。词性很有用，因为它们揭示了一个单词及其相邻词的很多信息。知道一个单词是名词还是动词可以告诉我们可能的相邻单词(名词前面有限定词和形容词，动词前面有名词)和句法结构单词(名词通常是名词短语的一部分)。一个单词的词性甚至可以在语音识别或合成中发挥作用，因为有些单词不同词性时的读音是不同的。在本综述中，将讨论词性标注的相关算法，比如早期的隐马尔可夫模型 (Hidden Markov Model, HMM)和随机条件域 (Conditional Random Fields, CRF)，以及近几年的神经网络。 1 介绍 词性标注的研究始于20世纪60年代初。词性标注是自然语言处理的重要工具。它是许多NLP应用程序中最简单的统计模型之一。词性标注是信息提取、归纳、检索、机器翻译、语音转换的初始步骤。在上世纪80年代末，人们使用基于隐马尔可夫模型已经使词性标注已经最高达到了95%的准确度。而最近几年，由于神经网络的完善和推广，有些模型可以达到97%的准确度。 2 早期算法 早期解决词性标注问题以隐马尔可夫模型算法为主。 2.1 HMM HMMs和随机语法被广泛应用于文本和语音处理的各种问题，包括主题分割、词性标注、信息提取和句法消歧。 在计算语言学和计算机科学中，隐马尔可夫模型(Hidden Markov models, HMMs)和随机语法被广泛应用于文本和语音处理的各种问题，包括主题分割、词性标注、信息提取和句法消歧。 2.1.1 Hidden Markov Model 马尔可夫链是一个模型，它告诉我们随机变量序列的概率。这些集合可以是表示任何东西的单词、标记或符号，例如天气。马尔可夫链有一个重要的假设，如果我们想在序列中预测未来，重要的是当前状态，当前状态之前的所有状态对未来都没有影响。就好像要预测明天的天气，你可以检查今天的天气，但是你不允许查看昨天的天气。 而马尔可夫假设(Markov Assumption)表示为， \\[ P(q_i=a|q_1 ...q_{i-1} ) = P(q_i=a|q_{i-1} ) \\] 规定，离开给定状态的弧的值之和必须为1。一种马尔可夫链，用于为单词序列\\(w_1...w_n\\)分配一个概率，它表示一个双语言模型，每条边表示概率\\(p(w_i|w_j)\\)。 \\(Q=q_1q_2...q_N\\) 表示N个状态的集合。 \\(A=a_{12}a_{12}...a_{n1}...a_{nm}\\) 一个转移概率矩阵。\\(a_{ij}\\) 表示状态\\(i\\)到状态\\(j\\)的概率。 \\(\\pi=\\pi_1\\pi_2...\\pi_N\\) 一个开始概率分布。 隐马尔可夫模型，即我们不直接观测状态。例如，我们通常不会在文本中观察词性标记。相反，我们看到单词，必须从单词序列中推断出标记。我们将这些标记称为隐藏标记，因为它们没有被观察到。 \\(Q=q_1q_2...q_N\\) 表示N个状态的集合。 \\(A=a_{12}a_{12}...a_{n1}...a_{nm}\\) 一个转移概率矩阵。\\(a_{ij}\\) 表示状态\\(i\\)到状态\\(j\\)的概率。 \\(O=o_1o_2...o_N\\) 序列\\(T\\)的观测。 \\(B=b_i(o_t)\\) 表示由状态\\(i\\)产生的观测值\\(o_t\\)的概率 \\(\\pi=\\pi_1\\pi_2...\\pi_N\\) 一个开始概率分布。 第二个假设：输出观测\\(o_i\\)的概率只取决于产生观测\\(q_i\\)的状态，而不取决于任何其他状态或任何其他观测: \\[ P(o_i|q_1...q_{i-1}) = P(o_i|q_{i-1}) \\] 2.1.2 HMM 标记器 对我们的语料库构建一个词性转移概率矩阵\\(A\\)，包含\\(P(t_i|t_{i-1})\\)，表示给定前一个标记得到当前标记的概率，比如像will这样的情态动词后面很可能跟一个基本形式的动词。 而构建这样的词性转移概率矩阵，需要统计语料库中的词性转移次数， \\[ P(t_i|t_{i-1}) = \\frac{C(t_{i-1},t_i)}{C(t_{i-1})} \\] 比如，MD在WSJ语料库中出现的次数是13124，而MD之后出现will的次数为4046， \\[ P(will|MD) = \\frac{C(MD,will)}{C(MD)} = \\frac{4046}{13124} = 0.31 \\] 2.1.3 HMM 解码器 解码器的定义是：输入一个HMM \\(\\lambda=(A, B)\\) 和观测\\(O=o_1o_2...o_N\\)，找到最有可能的状态序列 \\(Q=q_1q_2...q_N\\)。 n个单词的序列\\(w^n\\)，标记序列\\(t^n\\)，则最终序列结果\\(\\hat{t}^n\\)由： \\[ \\hat{t}^n = \\mathop{argmax}_{t^n}P(t^n|w^n) \\] 有贝叶斯定理可得： \\[ \\hat{t}^n = \\mathop{argmax}_{t^n}\\frac{P(w^n|t^n)P(t^n)}{P(w^n)} \\] 为了简化公式，去掉分母， \\[ \\hat{t}^n = \\mathop{argmax}_{t^n}P(w^n|t^n)P(t^n) \\] 根据第一个假设可知，一个标记只取决于前一个标记。 \\[ P(t^n) \\approx \\prod_{i=1}^n P(t_i|t_{i-1}) \\] 根据第二个假设可知，一个单词(观测)依赖于自身的标记(状态)以及相邻单词及其标记，可得 \\[ P(w^n|t^n) \\approx \\prod_{i=1}^n P(w_i|t_i) \\] 最终可得， \\[ \\hat{t}^n = \\mathop{argmax}_{t^n}P(t^n|w^n) \\approx \\mathop{argmax}_{t^n} \\prod_{i=1}^n P(w_i|t_i)P(t_i|t_{i-1}) \\] 2.1.4 The Viterbi Algorithm 维特比算法建立一个概率矩阵或格子(lattice)，这个格子，的每一列表示一个观测，而每一行表示一个状态。 对于格子中的每一个单元格 \\(v_t(j)\\) 表示HMM \\(\\lambda\\) 经过了\\(t\\)个观测来到状态\\(j\\)中。 \\[ v_t(j)=\\mathop{max}_{q_1...q_{t-1}}P(q_1...q_{t-1},o_1,o_2...o_t,q_t=j|\\lambda) \\] 其中\\(q_1...q_{t-1}\\)表示该单元格经过的所有状态，我们需要像动态规划那样最大化这个状态序列，以便于最终取得最优值。 可得迭代公式， \\[ v_t(j)=\\mathop{max}^N_{i=1}v_{t-1}(i)a_{ij}b_j(o_t) \\] 2.2 CRF HMMs和随机语法是生成模型，将联合概率分配给成对的观察和标签序列。为了定义观测序列和标号序列的联合概率，生成模型需要枚举所有可能的观测序列。特别是，表示多个相互作用的特征或观测值的长期依赖关系是不现实的，因为这类模型的推理问题是难以解决的。 条件随机域 (conditional random fields, CRF) (Lafferty, McCallum &amp; Pereira, 2001)是一个单一的指数模型，可以计算得到给定观测序列的整个标签序列的联合概率。 我们用\\(z = {z_1,..., z_n}\\)表示一个通用的输入序列，\\(z_i\\) 表示第\\(i\\)个单词的向量。\\(y={y_1,...,y_n}\\) 表示\\(z\\)的标签。\\(\\mathcal{Y}(z)\\) 表示\\(z\\)的标签序列集。序列的条件随机域定义为一个条件概率模型 \\(p(y|z;W,b)\\)。给定\\(z\\)与所有可能的\\(y\\)组成： \\[ p(y | z ; W, b)=\\frac{\\prod_{i=1}^{n} \\psi_{i}\\left(y_{i-1}, y_{i}, z\\right)}{\\sum_{y^{\\prime} \\in \\mathcal{Y}(z)} \\prod_{i=1}^{n} \\psi_{i}\\left(y_{i-1}^{\\prime}, y_{i}^{\\prime}, z\\right)} \\] 其中，\\(\\psi_{i}\\left(y_{i-1}, y_{i}, z\\right) = \\exp \\left(W_{y^{\\prime}, y}^{T} z_{i}+b_{y^{\\prime}, y} r\\right)\\) ，而 \\(W_{y^{\\prime}, y}^{T}\\) 与 \\(b_{y^{\\prime}, y}\\) 分布为权重向量和对应标签对的偏移。 对于CRF训练，我们使用最大条件似然估计： \\[ L(W, {b})=\\sum_{i} \\log p(y | {z} ; {W}, {b}) \\] 只要最大化 \\(L(W, {b})\\) 即可。 而解码过程就是搜索具有最高条件概率的标签序列 \\(y^*\\)： \\[ {y}^{*}=\\underset{y \\in \\mathcal{Y}({z})}{\\operatorname{argmax}}\\; p({y} | {z} ; {W}, {b}) \\] 3 神经机器学习 早期算法HMM，CRF等严重依赖手工制作的特性和特定于任务的资源。这种特定于任务的知识开发成本很高，使得序列标记模型难以适应新任务或新领域。近年来，作为输入分布式词表示的非线性神经网络，也称为词嵌入，在NLP问题中得到了广泛的应用，并取得了很大的成功。最近，递归神经网络(RNN) (Goller and Kuchler, 1996) 及其变体，如长短时记忆(LSTM) (Hochreiter and Schmidhuber, 1997)和门控循环神经单元 (GRU) (Cho et al., 2014)在序列数据建模方面取得了巨大的成功。 3.1 LSTM 递归神经网络(RNNs)是一个强大的连接主义模型家族，它通过图中的周期来捕捉时间动态。虽然在理论上，RNNs能够捕获远程依赖关系。但在实践中，由于梯度消失/爆炸问题，基本上失败了。 LSTMs (Hochreiter and Schmidhuber, 1997) 是RNNs的变体，用于处理这些梯度消失问题。基本上，LSTM单元由三个乘法门组成，它们控制信息的比例，以便遗忘和传递到下一个时间步骤。 形式上，LSTM单元在 \\(t\\) 时间步更新的公式为: \\[ \\begin{aligned} \\mathbf{i}_{t} &amp;=\\sigma\\left(\\boldsymbol{W}_{i} \\mathbf{h}_{t-1}+\\boldsymbol{U}_{i \\mathbf{x}_{t}}+\\boldsymbol{b}_{i}\\right) \\\\ \\mathbf{f}_{t} &amp;=\\sigma\\left(\\boldsymbol{W}_{f} \\mathbf{h}_{t-1}+\\boldsymbol{U}_{f} \\mathbf{x}_{t}+\\boldsymbol{b}_{f}\\right) \\\\ \\tilde{\\mathbf{c}}_{t} &amp;=\\tanh \\left(\\boldsymbol{W}_{c-1} \\mathbf{h}_{t-1}+\\boldsymbol{U}_{c} \\mathbf{x}_{t}+\\boldsymbol{b}_{c}\\right) \\\\ \\mathbf{c}_{t} &amp;=\\mathbf{f}_{t} \\odot \\mathbf{c}_{t-1}+\\mathbf{i}_{t} \\odot \\tilde{\\mathbf{c}}_{t} \\\\ \\mathbf{o}_{t} &amp;=\\sigma\\left(\\boldsymbol{W}_{o} \\mathbf{h}_{t-1}+\\boldsymbol{U}_{o} \\mathbf{x}_{t}+\\boldsymbol{b}_{o}\\right) \\\\ \\mathbf{h}_{t} &amp;=\\mathbf{o}_{t} \\odot \\tanh \\left(\\mathbf{c}_{t}\\right) \\end{aligned} \\] 其中 \\(\\sigma\\) 表示 sigmoid 函数，\\(\\odot\\) 表示点积，\\(x_t\\) 是在 \\(t\\) 时间步的输入向量，\\(h_t\\) 是隐藏状态向量，包含了当前及前面所有时间步的信息。\\(U_i, U_f, U_c, U_o\\) 表示不同门的输入权重矩阵。而\\(W_i, W_f, W_c, W_o\\)表示不同门的隐藏状态权重矩阵，\\(b_i, b_f, b_c, b_o\\) 表示偏移向量。 3.1.1 双向 LSTM (BLSTM) 对于许多序列标记任务，访问过去(左)和未来都是有益的。然而，单向LSTM的隐藏状态\\(h_t\\)只从过去获取信息，对未来一无所知。一个优雅的解决方案是双向LSTM，其有效性已经被之前的工作证明 (Chiu et al., 2016)。其通过向前和向后传播的两个隐藏状态，分别捕获过去和未来的信息，然后将这两个隐藏状态连接起来，形成最终的输出。 3.2 用于字符级表示的CNN 卷积神经网络(CNN)是一种从单词字符中提取形态学信息(如单词的前缀或后缀)并将其编码成神经表示的有效方法 (Ma, &amp; Hovy, 2016)。Figure 1 展现了用CNN提取给定单词的字符级表示。 Figure 1: 卷积神经网络用于提取单词的字符级表示。 虚线箭头表示在向CNN输入字符嵌入之前应用了一个dropout层 用CNN计算出每个单词的字符级表示，然后字符级向量表示连接词级向量组成最终的输入向量。 4 训练与评估 词嵌入。 常使用斯坦福大学公开的GloVe 嵌入式系统，该系统训练了来自维基百科和网络文本的60亿个单词 (Pennington et al., 2014)。 数据集。 常使用宾夕法尼亚大学的《华尔街日报》部分 Treebank (PTB) (Marcus et al., 1993)，其中包含45个不同的POS标签。并分为训练集和测试集。而准确度标准采用F1分数。 Mocel Acc. Bi-LSTM (Plank et al., 2016) 97.22 Feed Forward (Vaswani et a. 2016) 97.4 NCRF++ (Yang and Zhang, 2018) 97.49 LSTM-CNNs-CRF (Ma and Hovy, 2016) 97.55 Adversarial Bi-LSTM (Yasunaga et al., 2018) 97.59 Meta BiLSTM (Bohnet et al., 2018) 97.96 Table 1: 近几年的各种模型的测试结果。 5 目前与未来研究 过去以及目前，很多模型都字符级或词级的词嵌入来改善模型性能，还有一些是通过优化语言模型来助力提高词性标注的准确性。未来，一种可能是会通过预训练的强大语言模型或词嵌入来继续提高词性标注性能；二是提出一种新型的网络模型来产生突破性的成果。 参考 Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Goller, C., &amp; Kuchler, A. (1996, June). Learning task-dependent distributed representations by backpropagation through structure. In Proceedings of International Conference on Neural Networks (ICNN'96) (Vol. 1, pp. 347-352). IEEE. Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780. Cho, K., Van Merriënboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259. Chiu, J. P., &amp; Nichols, E. (2016). Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics, 4, 357-370. Ma, X., &amp; Hovy, E. (2016). End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354. Pennington, J., Socher, R., &amp; Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543). Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: The Penn Treebank. Plank, B., Søgaard, A., &amp; Goldberg, Y. (2016). Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss. arXiv preprint arXiv:1604.05529. Vaswani, A., Bisk, Y., Sagae, K., &amp; Musa, R. (2016). Supertagging with lstms. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 232-237). Yang, J., &amp; Zhang, Y. (2018). Ncrf++: An open-source neural sequence labeling toolkit. arXiv preprint arXiv:1806.05626. Yasunaga, M., Kasai, J., &amp; Radev, D. (2017). Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint arXiv:1711.04903. Bohnet, B., McDonald, R., Simoes, G., Andor, D., Pitler, E., &amp; Maynez, J. (2018). Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings. arXiv preprint arXiv:1805.08237.","link":"/2019/11/23/pos-review/"},{"title":"Linux Select 源码分析","text":"先说明一些关于Linux进程间通信的基本知识：Linux的进程间通信是基于文件的，也可以称之为设备，因为所有设备其实都是文件。描述符其实指的是文件描述符，与文件系统中的一个文件对应，每当你创建出一个描述符，就创建出一个设备驱动进程，该进程一直监听着该文件的变化。 核心 select 源码可以参考这里。 首先select不会一直轮询，轮询一次发现没有可处理的事件，进程就会挂起。那为何select中为何能够感知到某一个描述符发生了变化，其实是文件IO事件触发回调函数唤醒了挂起的select进程。而一旦被唤醒后，则记录一个triggered参数，使得select进程以后不再挂起。 当调用select函数，会创建一个实体缓存队列(poll_wqueues)，尔后会调用 poll_initwait(poll_wqueues) 去初始化该缓存队列，缓存队列将缓存一些实体(poll_table_entry)。（队列大小是有限的，可以通过 poll_table_page 结构体扩容，这个不是重点，这里不展开讲。） 12345678910111213141516171819202122232425262728293031323334353637// poll 表格typedef struct poll_table_struct &#123; poll_queue_proc _qproc; __poll_t _key;&#125; poll_table;// 实体缓存队列struct poll_wqueues &#123; poll_table pt; struct poll_table_page* table; struct task_struct* polling_task; //保存当前调用select的用户进程struct task_struct结构体 int triggered; // 当前用户进程被唤醒后置成1，以免该进程接着进睡眠 int error; // 错误码 int inline_index; // 数组inline_entries的引用下标 struct poll_table_entry inline_entries[N_INLINE_POLL_ENTRIES]; // 实体数组，后面会讲&#125;;// select 的核心函数int do_select(int n, fd_set_bits *fds, struct timespec *end_time)&#123; struct poll_wqueues table; // ... poll_initwait(&amp;table); // ...&#125;void poll_initwait(struct poll_wqueues *pwq)&#123; init_poll_funcptr(&amp;pwq-&gt;pt, __pollwait); // ...&#125;static inline void init_poll_funcptr(poll_table *pt, poll_queue_proc qproc)&#123; pt-&gt;qproc = qproc; pt-&gt;key = ~0UL; /* all events enabled */&#125; 每个设备驱动进程都有一个等待队列，等待队列存放一个等待项(wait_queue_entry_t)，首次发生事件时，会调用 pollwait 往队列放入一个等待项，等待项保存了回调函数，由等待项可会获得一个实体(entry)，实体存放一个指针函数与监听事件掩码key，监听事件掩码的不同二进制位存储是否监听对应事件，由此可知道用户想监听哪些事件。当驱动程序发送IO事件，就会扫描等待队列中的实体，检测是否注册了对应事件，并回调函数，该回调函数是一个唤醒函数(pollwake)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 等待项struct wait_queue_entry &#123; unsigned int flags; void *private; wait_queue_func_t func; // 回调函数 struct list_head entry;&#125;;// 实体struct poll_table_entry &#123; struct file *filp; // 指向特定fd对应的file结构体; unsigned long key; // 等待特定fd对应硬件设备的事件掩码，如POLLIN、 POLLOUT、POLLERR; wait_queue_entry wait; // 需要放入等待队列的等待项 wait_queue_head_t *wait_address; // 设备驱动程序中特定事件的等待队列头&#125;;// 新增一个等待项// pollwait() -&gt; __pollwait()static void __pollwait(struct file *filp, wait_queue_head_t *wait_address, poll_table *p) &#123; struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt); struct poll_table_entry *entry = poll_get_entry(pwq); if (!entry) return; get_file(filp); entry-&gt;filp = filp; // 保存对应的file结构体 entry-&gt;wait_address = wait_address; // 保存来自设备驱动程序的等待队列头 entry-&gt;key = p-&gt;key; // 保存对该fd关心的事件掩码 init_waitqueue_func_entry(&amp;entry-&gt;wait, pollwake);// 初始化等待队列项，pollwake是唤醒该等待队列项时候调用的函数 entry-&gt;wait.private = pwq; // 将poll_wqueues作为该等待队列项的私有数据，后面使用 add_wait_queue(wait_address, &amp;entry-&gt;wait);// 将该等待队列项添加到从驱动程序中传递过来的等待队列头中去。&#125;// 唤醒static int pollwake(wait_queue_entry *wait, unsigned mode, int sync, void *key)&#123; struct poll_table_entry *entry; entry = container_of(wait, struct poll_table_entry, wait);// 取得poll_table_entry结构体指针 if (key &amp;&amp; !((unsigned long)key &amp; entry-&gt;key))/*这里的条件判断至关重要，避免应用进程被误唤醒，什么意思？*/ return 0; return __pollwake(wait, mode, sync, key);&#125;static int __pollwake(wait_queue_t *wait, unsigned mode, int sync, void *key)&#123; struct poll_wqueues *pwq = wait-&gt;private; DECLARE_WAITQUEUE(dummy_wait, pwq-&gt;polling_task); smp_wmb(); pwq-&gt;triggered = 1; // select()用户进程只要有被唤醒过，就不可能再次进入睡眠，因为这个标志在睡眠的时候有用 return default_wake_function(&amp;dummy_wait, mode, sync, key); // 默认通用的唤醒函数&#125; 调用结构 123456789101112select() &#123; sys_select(); &#125;sys_select()&#123; // 将用户态参数拷贝到内核 core_sys_select();&#125;core_sys_select()&#123; // 填充 fd_set_bits do_select();&#125;do_select() &#123; // 轮询&#125; do select do_select 是上面我所说的一切工作的外部函数。 接口： 1static int do_select(int n, fd_set_bits *fds, struct timespec64 *end_time) - n 即n个描述符 - fds 类型fd_set_bits是一个结构体，包含了可读，可写，异常三个状态的监听数组，以及三个对应的状态输出数组。 1234typedef struct &#123; unsigned long *in, *out, *ex; unsigned long *res_in, *res_out, *res_ex;&#125; fd_set_bits; - end_time 结束时间 源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160static int do_select(int n, fd_set_bits *fds, struct timespec64 *end_time)&#123; ktime_t expire, *to = NULL; // 实体缓存队列 struct poll_wqueues table; // poll 表格 poll_table* wait; int retval, i, timed_out = 0; u64 slack = 0; __poll_t busy_flag = net_busy_loop_on() ? POLL_BUSY_LOOP : 0; unsigned long busy_start = 0; rcu_read_lock(); // 等一下按位移动查询，所以先计算需要位移的数量 // retval = n * BITS_PER_LONG // BITS_PER_LONG = in | out | exp retval = max_select_fd(n, fds); rcu_read_unlock(); if (retval &lt; 0) return retval; // 注意，这里给了n n = retval; // 初始化等待队列 poll_initwait(&amp;table); // 将初始化后的 poll 表格拿出来，后面传入vfs_poll用于构造等待项 wait = &amp;table.pt; if (end_time &amp;&amp; !end_time-&gt;tv_sec &amp;&amp; !end_time-&gt;tv_nsec) &#123; wait-&gt;_qproc = NULL; timed_out = 1; &#125; if (end_time &amp;&amp; !timed_out) slack = select_estimate_accuracy(end_time); retval = 0; for (;;) &#123; // 轮询 unsigned long *rinp, *routp, *rexp, *inp, *outp, *exp; bool can_busy_loop = false; inp = fds-&gt;in; outp = fds-&gt;out; exp = fds-&gt;ex; rinp = fds-&gt;res_in; routp = fds-&gt;res_out; rexp = fds-&gt;res_ex; // 在描述符数组上按位移动 for (i = 0; i &lt; n; ++rinp, ++routp, ++rexp) &#123; unsigned long in, out, ex, all_bits, bit = 1, j; unsigned long res_in = 0, res_out = 0, res_ex = 0; __poll_t mask; in = *inp++; out = *outp++; ex = *exp++; // 当前描述符监听了哪些事件 all_bits = in | out | ex; if (all_bits == 0) &#123; // 没有监听事件，到下一个描述符 i += BITS_PER_LONG; continue; &#125; for (j = 0; j &lt; BITS_PER_LONG; ++j, ++i, bit &lt;&lt;= 1) &#123; struct fd f; if (i &gt;= n) break; // bit 每次左移一位看看监听哪些事件 if (!(bit &amp; all_bits)) continue; // 找到一个监听的事件，则获取描述符的文件对象 f = fdget(i); if (f.file) &#123; wait_key_set(wait, in, out, bit, busy_flag); // 调用设备驱动程序的poll // 得到事件掩码 // 除了返回掩码外，内部将调用 pollwait() 构造等待项，并放入等待队列 mask = vfs_poll(f.file, wait); fdput(f); // 通过事件掩码查看到底发生了哪些事件 if ((mask &amp; POLLIN_SET) &amp;&amp; (in &amp; bit)) &#123; res_in |= bit; retval++; wait-&gt;_qproc = NULL; &#125; if ((mask &amp; POLLOUT_SET) &amp;&amp; (out &amp; bit)) &#123; res_out |= bit; retval++; wait-&gt;_qproc = NULL; &#125; if ((mask &amp; POLLEX_SET) &amp;&amp; (ex &amp; bit)) &#123; res_ex |= bit; retval++; wait-&gt;_qproc = NULL; &#125; /* got something, stop busy polling */ if (retval) &#123; can_busy_loop = false; busy_flag = 0; /* * only remember a returned * POLL_BUSY_LOOP if we asked for it */ &#125; else if (busy_flag &amp; mask) can_busy_loop = true; &#125; &#125; if (res_in) *rinp = res_in; if (res_out) *routp = res_out; if (res_ex) *rexp = res_ex; cond_resched(); &#125; wait-&gt;_qproc = NULL; // 发现事件 或 超时 或 当前进程有信号要处理 则打断轮询 if (retval || timed_out || signal_pending(current)) break; if (table.error) &#123; retval = table.error; break; &#125; /* only if found POLL_BUSY_LOOP sockets &amp;&amp; not out of time */ if (can_busy_loop &amp;&amp; !need_resched()) &#123; if (!busy_start) &#123; busy_start = busy_loop_current_time(); continue; &#125; if (!busy_loop_timeout(busy_start)) continue; &#125; busy_flag = 0; /* * If this is the first loop and we have a timeout * given, then we convert to ktime_t and set the to * pointer to the expiry value. */ if (end_time &amp;&amp; !to) &#123; expire = timespec64_to_ktime(*end_time); to = &amp;expire; &#125; // 挂起当前进程，直到被唤醒或timeout if (!poll_schedule_timeout(&amp;table, TASK_INTERRUPTIBLE, to, slack)) timed_out = 1; &#125; // 释放实体缓存队列的内存 poll_freewait(&amp;table); return retval;&#125; 总结过程 select调用的基本步骤就是， 调用 select select 轮询一次，找出需要监听事件的描述符，把等待项(监听掩码+回调函数)放到描述符对应的驱动程序的等待队列里边，并返回一个结果掩码，比对所有结果掩码，如果没有发生事件，则挂起。（一般第一次轮询都是挂起的，否则就是第一次轮询之前就发生了事件） 发生IO事件 select进程被唤醒，第二次轮询，找出发生事件的描述符及其具体事件，在fd事件数组上对应位置上做标记。但可能发生的IO事件并非用户要求监听的事件，这时候就不再挂起，而是一直轮询，等待下一次IO事件 清除缓存 返回一个发生事件描述符的数量 用户调用 FD_ISSET() 找出哪个描述符发生事件，处理一些事情 处理完后，又循环回到第1步 参考 https://my.oschina.net/fileoptions/blog/911091 https://elixir.bootlin.com/linux/latest/source/fs/select.c","link":"/2019/09/25/linux-select/"},{"title":"Red black tree implementation","text":"Red black tree is a data structure for searching data and can self-balance after modification. I will try to implement it and log in this article. You can find the source code in here. Node Evey Node have five member properties. color_ represent the color of node, red or black. 123456789101112enum RbTreeColor &#123; rb_red = false, rb_black = true &#125;;template &lt;typename T&gt;struct RBNode &#123; typedef RBNode&lt;T&gt;* SelfPtr; typedef const RBNode&lt;T&gt;* ConstSelfPtr; RbTreeColor color_; SelfPtr parent_; SelfPtr left_; SelfPtr right_; T value_; Iterator function increment() find the last node that is larger than current node, corresponding operator++. function decrement() find the last node that is smaller than current node, corresponding operator--. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061template &lt;typename T, typename Ref, typename Ptr&gt;struct RBTreeIterator &#123; // ... typedef RBTreeIterator&lt;T, Ref, Ptr&gt; Self; typedef RBNode&lt;T&gt;* NodePtr; NodePtr node_; explicit RBTreeIterator() = default; explicit RBTreeIterator(NodePtr x): node_(x) &#123;&#125; void increment() &#123; if(node_-&gt;right_ != 0) &#123; node_ = node_-&gt;right_; while(node_-&gt;left_ != 0) &#123; node_ = node_-&gt;left_; &#125; &#125; else &#123; NodePtr y = node_-&gt;parent_; while (node_ == y-&gt;right_) &#123; node_ = y; y = y-&gt;parent_; &#125; if(node_-&gt;right_ != y) &#123; node_ = y; &#125; &#125; &#125; void decrement() &#123; if(node_-&gt;color_ == rb_red &amp;&amp; node_-&gt;parent_-&gt;parent_ == node_) &#123; node_ = node_-&gt;right_; &#125; else if (node_-&gt;left_ != 0) &#123; NodePtr y = node_-&gt;left_; while (y-&gt;right_ != 0)&#123; y = y-&gt;right_; &#125; node_ = y; &#125; else &#123; NodePtr y = node_-&gt;parent_; while (node_ == y-&gt;left_) &#123; node_ = y; y = y-&gt;parent_; &#125; node_ = y; &#125; &#125; reference operator*() const &#123; return node_-&gt;value_; &#125; pointer operator-&gt;() const &#123; return &amp;(node_-&gt;value_); &#125; Self&amp; operator++() &#123; increment(); return *this; &#125; Self&amp; operator--() &#123; decrement(); return *this; &#125; // ... Tree Property header rb-1.jpg parent: link to root if exist left: also call leftmost, link to the most left node of tree right: also call leftmost, link to the most right node of tree color: default is red root rb-2.jpg parent: link to header left: link to nullptr if no node right: link to nullptr if no node color: always is black Multi Node See a tree with multi nodes. The children of leaf are nullptr. rb-3.jpg Rotation Left Rotation When the right sub-tree is taller, we need to rotate left to shorten it. 1234567891011121314151617181920212223242526/* * x y * / \\ / \\ * u y =&gt; x b * / \\ / \\ * a b u a * */inline void _rotate_left(NodePtr x, NodePtr&amp; root) &#123; NodePtr y = x-&gt;right_; x-&gt;right_ = y-&gt;left_; if(y-&gt;left_ != 0) &#123; y-&gt;left_-&gt;parent_ = x; &#125; y-&gt;parent_ = x-&gt;parent_; if(x == root) &#123; root = y; &#125; else if(x-&gt;parent_-&gt;right_ == x) &#123; x-&gt;parent_-&gt;right_ = y; &#125; else &#123; x-&gt;parent_-&gt;left_ = y; &#125; y-&gt;left_ = x; x-&gt;parent_ = y;&#125; Right Rotation When the left sub-tree is taller, we need to rotate right to shorten it. 12345678910111213141516171819202122232425/* * x y * / \\ / \\ * y u =&gt; a x * / \\ / \\ * a b b u */inline void _rotate_right(NodePtr x, NodePtr&amp; root) &#123; NodePtr y = x-&gt;left_; x-&gt;left_ = y-&gt;right_; if(y-&gt;right_ != 0) &#123; y-&gt;right_-&gt;parent_ = x; &#125; y-&gt;parent_ = x-&gt;parent_; if(x == root) &#123; root = y; &#125; else if(x-&gt;parent_-&gt;right_ == x) &#123; x-&gt;parent_-&gt;right_ = y; &#125; else &#123; x-&gt;parent_-&gt;left_ = y; &#125; y-&gt;right_ = x; x-&gt;parent_ = y;&#125; Rule and Balance The color of node is eithor red or black. The root node always is black. If a node is red, its children must be black. Every path of every sub-tree have the same number of black nodes. As the rule 4, we can ensure the different of the height of the tallest sub-tree and that of the shortest sub-tree cannot over 2 times. The color is the measure of the balance for red black tree, as the height difference for the AVL tree. Insertion We insert a not-allow-replaced key with three considerations: insert to the left of leftmost insert to the right of y if just bigger than y insert to the left of y if just bigger than decrement(y) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061pair&lt;iterator, bool&gt; insertUnique(const Val &amp;v) &#123; NodePtr y = header_; NodePtr x = root(); bool comp = true; while(x != nullptr) &#123; y = x; comp = keyCompare_(KeyOfValue()(v), key(x)); x = comp ? left(x) : right(x); &#125; // while end, y will be the parent of the node to insert iterator j = iterator(y); if (comp) &#123; // comp is true, insert to left if (j == begin()) &#123; return pair&lt;iterator, bool&gt;(_insert(x, y, v), true); &#125; else &#123; // if j is not the leftmost --j; // need to --j, because KeyOfValue(v) probably equal to --j, // it need to be bigger than --j as the following comparsion. &#125; &#125; if (keyCompare_(key(j.node_), KeyOfValue()(v))) &#123; return pair&lt;iterator, bool&gt;(_insert(x, y, v), true); &#125; return pair&lt;iterator, bool&gt;(j, false);&#125;// x is the position need to be inserted// y is the parent of x// v is the new valueiterator _insert(NodePtr x, NodePtr y, const Val&amp; v) &#123; NodePtr z; if(y == header_ || x != nullptr || keyCompare_(KeyOfValue()(v), key(y))) &#123; // insert to left of y z = createNode(v); left(y) = z; if(y == header_) &#123; root() = z; rightmost() = z; &#125; else if (y == leftmost()) &#123; leftmost() = z; &#125; &#125; else &#123; z = createNode(v); right(y) = z; if(y == rightmost()) &#123; rightmost() = z; &#125; &#125; parent(z) = y; left(z) = nullptr; right(z) = nullptr; // rebalance the tree _rebalance(z, root()); ++nodeCount_; return iterator(z);&#125; Inside and Outside For convenient, we define the inside and outside as shown blow. If the cur node direction is not equal to the parent direction, we call it inside, or else we call it outside. 12345678910111213/* * v * / \\ * a u * / \\ a is a left node * b c b is outside, c is inside * * v * / \\ * u a * / \\ a is a right node * c b b is outside, c is inside */ ReBalance Initially, x is new inserted node. x is red default. When its parent is red, which break the rule 3. If its parent is black, it do not breaks any rules, which does not need to rebalance. You need to consider four situations: - First, we need to think about the parent of x is either a left or a right, which determines the rotation direction. - And then, consider the uncle of x is either red or black, which determines how to change the color. In the process of percolation and rebalance, the parent of x initially is red, when percolate up, we change its color. How to change is determines to its sibling (uncle of x), which we need to make the color as same as its sibling. If uncle is red, the grandparent must be black, all of you need to do is to change the color of parent and uncle to black, and grandparent's color to red, for following the rule 3. But these probably break the rules in higher position, so we need to percolate up to the grandparent. After percolate up, if you find the parent is black, you can stop the rebalance, because the number of black have not changed and followed the rule 3 as above step. But if the parent is red, two continuous red node break the rule 3 again. And its uncle must be black or null. Looking at the following steps. If uncle is black or null, If cur node is in outside, change parent's color to black and grandpa's color to red. These will make the number of black nodes in its parent sub-tree increase 1 and greater than that of the uncle sub-tree, so we do a rotation to make the parent move to the grandpa position to make anthor direction sub-tree have the same number of black nodes. The rotation direction is opposite to its direction to its parent. If cur node is in inside, we need to do a rotate firstly (the rotation direction is same to its direction to its parent) to make the cur node becomes the parent node, and the original parent node becomes the cur node in the outside, as same as the situation above, so do the same thing as above. (total two rotations) Finally, making the root's color to black, these action do not break any rule. 12345678910111213141516171819202122232425262728293031323334353637383940414243inline void _rebalance(NodePtr x, NodePtr&amp; root) &#123; x-&gt;color_ = rb_red; while(x != root &amp;&amp; x-&gt;parent_-&gt;color_ == rb_red) &#123; if(x-&gt;parent_ == x-&gt;parent_-&gt;parent_-&gt;left_) &#123; // parent is a left node NodePtr y = x-&gt;parent_-&gt;parent_-&gt;right_; // consider uncle node if(y &amp;&amp; y-&gt;color_ == rb_red) &#123; // uncle is red // parent and uncle change to black x-&gt;parent_-&gt;color_ = rb_black; y-&gt;color_ = rb_black; // grandparent change to red x-&gt;parent_-&gt;parent_-&gt;color_ = rb_red; x = x-&gt;parent_-&gt;parent_; // go up &#125; else &#123; // uncle is black or is null if(x == x-&gt;parent_-&gt;right_) &#123; // x is inside x = x-&gt;parent_; _rotate_left(x, root); // rotate left &#125; // parent's color become as same as uncle's x-&gt;parent_-&gt;color_ = rb_black; x-&gt;parent_-&gt;parent_-&gt;color_ = rb_red; _rotate_right(x-&gt;parent_-&gt;parent_, root); // rotate right &#125; &#125; else &#123; // parent is a right node NodePtr y = x-&gt;parent_-&gt;parent_-&gt;left_; // uncle if(y &amp;&amp; y-&gt;color_ == rb_red) &#123; // uncle is red // parent and uncle change to black x-&gt;parent_-&gt;color_ = rb_black; y-&gt;color_ = rb_black; x-&gt;parent_-&gt;parent_-&gt;color_ = rb_red; &#125; else &#123; // uncle is black or is null if(x == x-&gt;parent_-&gt;left_) &#123; // x is inside x = x-&gt;parent_; _rotate_right(x, root); // rotate left &#125; x-&gt;parent_-&gt;color_ = rb_black; x-&gt;parent_-&gt;parent_-&gt;color_ = rb_red; _rotate_left(x-&gt;parent_-&gt;parent_, root); &#125; &#125; &#125; // ensure root is black root-&gt;color_ = rb_black;&#125;","link":"/2020/08/02/rb-tree/"},{"title":"循环神经机器翻译","text":"编码和解码 我们把源句子（source sentence）表示为 \\(\\large (x_1, x_2, \\dots, x_n) \\in x\\), 目标句子（target sentence）表示为 \\(\\large (y_1, y_2, \\dots, y_n) \\in y\\)。 使用编码网路（encoder）对源句子进行编码，使用解码网络（decoder）对源句子的编码进行解码，解码出预测句子。同时，作为监督，把目标句子作为解码网络的输入。 本文主要讲使用循环神经网络构建encoder-decoder模型。 源句子的最大序列长度为 \\(\\large T\\), 目标句子的最大序列长度为 \\(\\large T&#39;\\)，批次大小为 \\(\\large B\\)，词向量大小为 \\(\\large E\\)，RNN的隐藏层大小为 \\(\\large H\\), 源语言词汇数量为 \\(\\large V\\)，目标语言词汇数量为 \\(\\large V&#39;\\)。 一个batch即一个句子，把句子填充到长度为 \\(\\large T\\)， 依次输入一个词，经过词向量 \\(\\large X \\in \\mathbb{R}^{T \\times B \\times E}\\)。 encoder 一次性输入整个源句子 \\(\\large X\\) 和隐藏状态 \\(\\large h \\in \\mathbb{R}^{T \\times B \\times H}\\) encoder 输出 \\(\\large X \\in \\mathbb{R}^{T \\times B \\times H}\\) 和 \\(\\large h \\in \\mathbb{R}^{T \\times B \\times H}\\) decoder 把时间步拆开，每一时间步的输入为上一时间步的隐藏状态和预测词，除了预测词，也可以用目标句子中的词，一个时间步输入数据为 \\(\\large X_i \\in \\mathbb{R}^{1 \\times B \\times E}\\) 和上一时间步的隐藏状态 \\(\\large h \\in \\mathbb{R}^{1 \\times B \\times H}\\) decoder 一个时间步的输出为 $O ^{1 B H} $ 和隐藏状态 \\(\\large h\\)。然后再下一个词向量 \\(\\large X\\) 和 \\(\\large h\\) 输入到下一时间步，执行第4步，直到句子末尾。 我们把decoder每一时间步的输出收集起来 \\(\\large Y \\in \\mathbb{R}^{T&#39; \\times B \\times E}\\) decoder的输出要用于softmax函数，所以每一时间步的输出通过一个线性层 $W ^{H V'} $，则最后的输出 $Y ^{T' B V'} $ 为了方便计算，把decoder的输出压缩成二维 $Y ^{(T' * B) V'} $ ，并使用softmax交叉熵计算损失值 反向传播并梯度更新 seq2seq网络结构如图所示： 因为我们是按批次计算，一个批次中的句子有长有短，为了统一长度，会对所有句子填充 \\(\\large \\langle pad \\rangle\\)。而句子尾部会填充 \\(\\large \\langle eos \\rangle\\) 表示结束，目标句子会在句首加一个\\(\\large \\langle sos \\rangle\\)，通过这个起始标志输出第一个词汇。 目标句子包含 \\(\\large \\langle eos \\rangle\\) , \\(\\large \\langle sos \\rangle\\) 和 \\(\\large \\langle pad \\rangle\\) 的序列长度是 \\(\\large T&#39;\\)， \\[ \\large \\langle sos \\rangle, I, love, you,\\large \\langle eos \\rangle, \\large \\langle pad \\rangle, \\dots, \\large \\langle pad \\rangle \\] 但因为decoder以 \\(\\large \\langle sos \\rangle\\) 为起点，预测输出的是不包含 \\(\\large \\langle sos \\rangle\\) 的，所以计算交叉熵时目标句子应该去掉 \\(\\large \\langle sos \\rangle\\) ，长度变为 \\(\\large T&#39;-1\\)， 但是decoder输出长度为 \\(\\large T&#39;\\)，长度不一致。这时注意到，句子末尾是一堆无意义的 \\(\\large \\langle pad \\rangle\\) ，预测出来的同样是无意义的，所以我们可以把decoder输入的最后一个去掉，这样序列长度就一致了。 注意力机制（Attention Mechanism） 注意力机制有两个作用： 其一，是区分哪部分是重要的。当你听到句子“the ball is on the ﬁeld”，你不会认为这 6 个单词都一样重要。你首先会注意到单词“ball”，“on” 和 “ﬁeld”，因为这些单词你是觉得最“重要”的。类似，Bahdanau 等人注意到使用 RNN 的最终状态作为 Seq2Seq 模型的单个“上下文向量”的缺点：一般对于输入的不同部分具有不同的重要程度。再者，此外，输出的不同部分甚至可以考虑输入的不同部分“重要”。 其二，是具有更长时间的记忆力。你会注意到，当你通过编码网络输出特征向量后，解码阶段就只是根据这个向量来解码，而不再有其他信息。如果我们可以持续从编码网络获取信息，那么我们的预测不是更准确一些吗？就好像人类翻译员，他们肯定也不是只看一眼原句，就能翻译出来，而是对照着原句一个一个翻译。 注意力机制的基本方法就是在解码阶段获取编码阶段的信息。 全局注意力（Global Attention） 所谓全局注意力，即把encoder的所有输出的信息参加到decoder每一时间步的输出的计算中。简单的说就是，当我们预测每一个词时，我们会参考源句子中每一个词，每个源词汇都会得到一个注意力得分（score）。 为了计算注意力，我们把decoder的每一时间步拆开计算。 假设当前时间步的输出为 \\(\\large y \\in \\mathbb{R}^{1 \\times B \\times H}\\)，而encoder的输出为 $O ^{T B H} $。则得分函数可以有以下三种， \\[ score(y, O) = \\begin{cases} y^TO \\\\ (y^TW)^TO \\\\ v^T tanh(W[y;O]) \\end{cases} \\] （一般用第二种） 为了得到得分矩阵，我们计算时会进行转置，\\(\\large y \\in \\mathbb{R}^{B \\times 1 \\times H}\\)， $O ^{B H T} $，矩阵相乘后得到注意力得分 \\(\\large a \\in \\mathbb{R}^{B \\times 1 \\times T}\\)，一般我们还会对它用softmax计算。 我们需要得到一个上下文向量 \\(\\large a \\times O \\rightarrow context \\in \\mathbb{R}^{B \\times 1 \\times H}\\) 把上下文向量与decoder时间步的输出相连，并经过线性层输出注意力向量 \\(\\large concat(context, y)^T W \\rightarrow attn \\in \\mathbb{R}^{B \\times 1 \\times H}\\) 把中间维度去掉后，经过线性层输出 $y ^{B V'} $ 当decoder的 \\(\\large T&#39;\\) 个时间步算完后，我们就能完整的输出序列 \\(\\large Y \\in \\mathbb{R}^{T&#39; \\times B \\times V&#39;}\\) ，以及所有的注意力得分 \\(\\large a\\) 组成了对齐矩阵（alignment matrix）\\(\\large align \\in \\mathbb{R}^{B \\times T&#39; \\times T}\\) 对齐矩阵如图所示： 这个表是将源句子中的单词映射到目标句子中的相应单词，越深色表示相关性越大。 全局注意力网络结构： Input-Feeding Luong et al. (2015) 中提出了Input-Feeding模型 [1]，将注意力向量 \\(\\large attn\\) 和目标词向量连接后作为decoder RNN的输入，即每一时间步的输入为 \\(\\large y \\in \\mathbb{R}^{B \\times 1 \\times (E+H)}\\)。 Teacher Forcing 所谓Teacher Forcing就是对每一时间步的输入使用目标句子中正确的词汇，而不是上一时间步输出的预测词。 Scheduled Sampling Scheduled Sampling 直译“计划抽样” [2]，是介于Teacher Forcing和Without Teacher Forcing之间的方法。我们设置一个使用Teacher Forcing的概率分布，对于每一时间步，我们使用该概率分布来绝对是否使用Teacher Forcing。若使用，我们就输入目标句子中的词汇，否则输入上一时间步的预测词。 论文中的方案是每一时间步就抛硬币选择一次，但实际训练中为了统一，一般是一个batch选择一次。 评估 我们对机器翻译的评估通常会用BLEU算法 [3]。它是一种比较参考翻译（Reference），即人工翻译，与机器翻译的得分算法。 我们对参考句子和预测句子取n元组（n-grams）, 机器翻译结果为 \\(\\large \\hat y\\), 得分为： \\[ P_n = \\frac{\\sum_{ngrams \\in \\hat y}Count_{ref}(ngrams)}{\\sum_{ngrams \\in \\hat y}Count_{MT}(ngrams)} \\] 而BLEU得分的定义为，我们将1至n元组的得分求和再求均值，然后用指数函数放大， \\[ BLEU = \\beta \\; exp({\\frac{1}{n}\\sum_{i=1}^n P_i}) \\] 其中 \\(\\large \\beta\\) 是惩罚因子, 也叫简短惩罚（brevity penalty）。如果你输出了一个非常短的翻译，那么它会更容易得到一个高精确度。因为输出的大部分词可能都出现在参考之中，这时就要对得分进行惩罚。 假设，参考翻译的长度为 \\(\\large r\\), 若有多个句子就取平均值。机器翻译句子长度为 \\(\\large m\\)。 \\[ \\beta = \\begin{cases} 1 &amp; if \\, m &gt; r \\\\ exp(1-\\frac{r}{m}) &amp; otherwise \\end{cases} \\] 如果机器翻译句子大于参考翻译句子长度，我们就不进行惩罚；若小于，\\(\\large 1-\\frac{r}{m}\\) 为负数, 而指数函数会得到一个小数，使得bleu得分降低。 束搜索 （Beam Search） 你也许会直觉的认为，解码器就是直接将每个时间步的输出通过softmax找到最可能的那个词汇即可，这也叫贪心查找（Greedy Search）。但在这只是考虑到一个词的最优选择，而不是整体最优选择。 如图，一句话可能会有成多种翻译版本 如果按照局部最优的选择，那可能会输出第一句。如果按照整体最优，那可能是第三句。而实际上，我们应该考虑整体最优。 \\[ \\mathop{argmax}_{y_1,y_2,...,y_T}\\,P(y_1,y_2,...,y_T|x) \\] 目前比较好的方法是束搜索（Beam search）[4]。 定义一个束宽 \\(\\large b\\)，每个序列得分函数为 \\(\\large F(w)\\)。 第一个时间步输出时，我们会从词汇表 \\(\\large V\\) 中找 \\(\\large b\\) 个得分 \\(\\large F(w)\\) 最大的词汇，加入到对应的序列当中。假设 \\(\\large b = 3\\)，那么就有3个候选序列 \\(\\large s_1(w_1), s_2(w_2), s_3(w_3)\\) 对于第一个时间步的每一个候选，依次输入到第二个时间步中，找得分最大的 \\(\\large b\\) 个词汇分别加入到该候选序列中，由一个候选序列变成 \\(\\large b​\\) 个候选序列 \\[ s(w_1,w_4), s(w_1, w_5), s(w_1, w_6) \\] 总共可以得到 \\(\\large b^2\\) 个候选序列 \\(\\large s_1^1, s_1^2, s_1^3, s_2^1, s_2^2, \\dots\\) 。假设序列长度为 \\(\\large L\\) ，一个序列的总得分为每个词的得分之和 $F(s) = _i^ L F(w_i) $，则该序列的平均得分为 \\(\\large score = F(s) / L\\)。我们保留 \\(\\large b\\) 个平均得分最高的序列，进入下一时间步。 往后重复类似于第2步的操作，当其中一个序列的最后一个词为 \\(\\large \\langle eos \\rangle\\) 时，该序列就停止搜索，直到所有序列都停止或者到达指定的最大长度为止。 最后从 \\(\\large b\\) 个序列选取平均得分最高的序列。 对于得分函数，我们一般使用 \\[ F=log(softmax(Y)) \\] 束搜索一般用在测试阶段。 卷积机器翻译 使用循环神经网络训练的感觉就是慢。使用卷积网络进行机器翻译可以实现并行化，大大提高训练效率。这部分我打算放到另一篇文章中讲。 参考文献 [1] Effective Approaches to Attention-based Neural Machine Translation. 2015 [2] Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. 2015 [3] BLEU: a Method for Automatic Evaluation of Machine Translation. 2002 [4] Sequence-to-Sequence Learning as Beam-Search Optimization. 2016","link":"/2018/12/31/rnn-nmt/"},{"title":"Raft 简述","text":"Raft 是一种分布式一致性算法，详细可见论文。主要作用是确保分布式系统的一致性问题。考虑主从模式的集群下，服务器挂掉或网络故障等情况发生时如何确保数据安全。在raft中，主节点称为leader，从节点称为follower。 下面我尝试以简洁流畅的语言描述raft算法的各个重要部分，某些地方会结合etcd/raft的源码说明。 节点感知 leader和follower需要相互知道的对方存在，一般采用发心跳信息的方式。在raft中，一定是leader主动发心跳信息给follower，然后follower回应。 日志复制 日志复制也叫日志同步或日志分发，之所以叫日志复制是因为总是leader单方面将日志复制到followers上。raft尽可能保证leader和followers的日志进度相同，自动进行着日志的同步。每条日志有一个 log index 参数，严格递增，可以通过该参数判断该follower的同步进度。 可能新增日志的速度快于同步速度，为了确保顺序，会等旧日志同步完后再同步新日志。实现方法可能是，leader与各个follower之间维护一个队列，leader往队列投入日志，followers从队列获取日志。 日志提交 当一个日志同步给超过半数的节点后，leader就可以提交该日志。所谓提交在不同分布式场景中可能有不同的具体表现。比如，在分布式存储中，leader把数据复制到其他节点后并非就真的存储完成，可能只是把数据放到了内存上，只有超过半数节点拥有这一数据才肯定该数据是安全的，然后leader执行提交，通知其他所有节点可以把该数据存储到磁盘上。 follower故障 如果leader给follower发送的心跳信息长时间没有回应，可能有两种情况，一是follower挂了，二是网络分区(网络连接失败)。如果是前者，则该follower是不可用的，如果是后者，可能过段时间网络恢复后又会重新连上来。但leader无法分辨是哪种情况，一般措施是依旧把follower维持在集群中，并不断发送消息，尽管不能得到回复。只要集群中有过半节点正常运行，集群就是可用状态。 领导选举 leader并非万无一失，也可能会发生故障而挂掉。需要选举出新的leader，每选举出一个leader，每个节点上维护的任期号term加一。 当某个follower持续一定时间没有收到leader的心跳信息，该follower就会认为leader挂掉了，这时他会把自己变成candidate，先把自己的任期号term加一，给自己投一票后广播信息请求其他节点给它投票，信息会携带自己的term和log index。 其他节点收到信息后，先看看自己能不能投，可以重复投给一个节点，但投过一个后就不能投给另一个。可以投票的话就先比较term，如果对方term更大，就投票给它，如果term相同且对方log index更大，同样投票给它，否则拒绝投票 [src]。拒绝投票意味着对方没有资格参与选举，因为它的term或log index落后于人。拒绝投票消息会携带当前term，对方收到消息后就会把自己的term更新为消息中的term，然后把自己变回follower [src]。 如果请求投票期间，candidate收到leader的心跳，也会把自己变回follower [src]。 当一个candidate受到半数以上的投票，它就把自己变成leader并向其他节点发信息宣告这一消息。当然，也可能选举失败，比如若干个candidate拿到相同的票数，并且没有多余的票数，超时之后将自己的term加一并继续新一轮选举。 若是前一个leader在分发某一个日志过程中挂掉了怎么办？如果这条日志还没来得及分发到任何一个节点上，leader就已经挂掉，那么这条日志必然随着leader的挂掉而丢失。如果成功分发到一个节点及以上，那么新leader一定是具有该日志的，否则log index落后一定不会成为leader。新leader发现该日志还没提交，就会分发该日志。与之前任期重复分发了也没关系，反正还没提交写入磁盘。 成员变更 当新加入节点或减少节点，需要将新的集群配置表分发到各个节点上，这一行为与其他日志分发并无不同。如果在分发过程中，leader挂掉了，那么可能发送一种情况是：具有旧集群配置表的节点们会选出一个leader，而新集群配置表的节点们也可能选举出一个leader，最终造成一个集群两个leader。 解决办法是每次只变更一个节点。事实证明，每次只变更一个节点不会产生分裂的情况，因为不会出现同时有两个超过半数以上子集群的存在。假设，我们有一个3节点的集群，然后添加一个节点，leader把变更日志分发到其他2个节点，这时，leader 出现了崩溃，重新选举。这个时候，会有2个结果： 新配置复制到了集群的大多数（大多数的值在这里必须大于2 （包括 leaer））。如果新配置复制到了大多数集群，那么新 leader 肯定使用的是新的配置。 新配置没有复制到集群的大多数。如果新配置没有复制到大多数集群，那么新 leader 肯定使用的是老的配置。 可以参考论文中的图： change-single-server 加入新节点 当加入新节点时，需要告诉leader，有新节点加入，但新节点不会马上加入集群的运作当中。leader会先向改节点同步数据，分多轮同步，直到新节点的数据追上集群的进度才加入集群。 既然日志同步是自动进行的，为什么还要等新节点的进度追上来后才加入集群？存在可能刚加入一个新节点，就有一个节点挂掉了，而新加入节点没有数据是不可用的，相当于同时又两个节点不可用，如果集群当前有4个节点，两个不可用，整个集群就不可用了。 删除节点和网络分区 一个节点被移除后，它自己可能不知道这个情况，它发现没有leader的心跳，自己就会发起选举。如果它的日志不是最新的，它总是无法获得投票，但是又没有leader，所以隔一段时间又再次发起投票，以此类推，老是发起选举会干扰到集群；如果它的日志是最新的，会导致集群节点给它投票，进而可能让一个被移除的节点成为leader。 网络分区类似，在节点通信故障后又重新连回集群后，该节点在多次发起选举后term可能变得很大，收到leader的心跳后发现自己的term比leader的更大，就回应一个消息 [src]，leader发现对方term比自己大，就把自己变成follower [src]，等待一段时间后触发新一轮选举。 为了防止被移除的节点成为leader，每个节点在投票之前需要检查自己在一定时间内有没有收到心跳，如果又收到心跳，说明leader还存活，没必要给别人投票。然而因为被移除节点无法知道自己已经被移除，所以还可以发起选举，虽然不会成为leader，但一直持续下去也不好，最好在移除节点后把该节点上的raft程序停掉。其实可以考虑加一种移除消息类型，当节点收到该消息后把自己停掉就好了。 解决网络分区term过大的问题的办法是PreVote，即预投票。Candidate先发起一次预投票，若是自己能获得大多数节点的投票就发起一次正式投票，如果不能则不能增加term。","link":"/2019/12/28/raft/"},{"title":"如何构建Raft分布式系统","text":"前文讲述了Raft算法的核心，本文提供将Raft算法应用于分布式系统的宏观指导，将不再赘述算法细节而是专注于用户层面和应用层面，描述一个Raft的分布式系统需要进行哪些工作。 组件 一个Raft节点，需要包含的组件有 Config 配置 Storage 日志存储器 Logger 打印程序运行时日志 Raft 算法核心 Node 一个节点的代称，是Raft的上层包装 提议和消息 我们把向Raft集群发起用户自定义操作的行为称为提议(Propose)，而一个提议称为(Proposal)。Raft内部沟通的信息称为消息(Message)。 对于一个Node，如果收到的是一个Message，则直接进入Raft的状态机中；如果向Leader发起一个Proposal，则需要先包裹成Message，然后再进入状态机。到了这一步再称为提议(Proposal)可能就不恰当了，我们可以称一个提议的数据实体为Entry。 快照 快照即把当前集群的状态保存下来。事实上，快照并不属于Raft算法的一部分，是由用户自行维护的。别以为很复杂，其实总共需要记录的数据就两个，term和commit index (applied index)。主要是为了再启动时能够回到关闭时的状态。 轮询 需要一个轮询时刻监听来自客户端的Proposal或Message。轮询需要做的事情大概如下： 接收到一个Proposal或Message，接收后就会进入状态机中处理。 更新时钟滴答，以便于适时发送心跳。 进入Ready阶段，发送Message，检查并处理提交后的提议。 以上操作需要顺序执行，且在单线程内完成。 而接收Proposal或Message的具体做法可以通过跨线程管道来处理，即CSP并发模型。这种多生产者，单消费者的模式在这种轮询场景下特别适用。 Ready 当我们把Message投入状态机后，经过状态机的一系列判断和转换，某些操作会产生一个新的Message，并存入内部日志中。要注意的是，Raft算法不应该包含网络传输，使用怎样的网络传输方式应该由外部决定，即用户需要自己从内部日志中获取该Message，然后再发送出去。 Raft会把需要发送的Message，和需要处理的Entry都包裹到一个名为Ready的结构中，它可以包含零个，一个或多个Message和Entry。在Ready阶段，主要对该结构进行处理。 在处理一个Ready时，你可能需要做以下几件事情： 持久化Entry。当本节点拿到新Entry时，需要手动持久化该Entry。注意是持久化Entry，而不仅是你所提议的数据，因为Entry还包含了term，index等信息。 保存快照。快照是对当前内存数据的一次集中存储，方便宕机后快速还原。 发送Message。根据状态机创建的Message中的目标节点ID，向其发送该Message。 处理提交后的Config Entry。提交后的Entry会放到缓存区中，在这里你需要尝试获取，如果有就进行处理。Config Entry是用于配置集群的指令，比如新增一个节点或删除一个节点。根据算法，必须在提议提交后才能正式加入新节点或删除节点，对于类似行为必须在此处进行操作。 处理提交后的Normal Entry。Normal Entry就是用户发起的提议。需要在提议提交后进行一些回调，比如，向客户端响应读操作的结果等。提议和Message一定会被顺序执行和提交，所以你可以使用链表等结构存储提议的回调函数或其他信息。另一个办法是在提议数据中包含一个唯一ID，凭此做对应的处理。 检查是否存储快照。一般记录每隔提交个entry就存储一次快照。 前进(Advance)。这里表示该Ready已经处理完成的意思，标记一下，然后就可以获取下一个Ready，不标记则永远返回同一个Ready。 滴答与心跳 Raft结构体只负责算法的实现，即定义了每隔多少个滴答(Tick)就发送心跳的行为，而程序运行时间和一个滴答的时间间隔则需要外部维护。比如一个滴答是100ms，当发现经过了100ms，就需要让Raft内部的滴答数加一，当滴答数到达3(假设)，Raft就自动发送心跳。 运行 讨论Raft集群运行过程中需要处理的问题。 配置 主要配置一些不会变动的信息，启动一个Raft节点主要需要配置以下信息： 节点ID 心跳的滴答数 重新选举的滴答数（没有收到心跳） 读操作的模式 是否开启预投票(Pre vote) 存储器 存储器是Raft节点加载和存储信息和数据的工具。使得Raft节点在启动能跟上集群的最新状态，以及重新启动时迅速回到上次关闭时的状态。 储存器需要存储和提供以下信息： Raft状态，包括commit index, leader, term, 成员ID等信息。 所有Entry。 快照(Snapshot)。 启动 当启动集群时，有一些需要注意的问题。 不要同时启动所有节点，而是先选定一个节点启动一段时间，至少该子节点自己成为了Leader，然后再启动其他节点。如果节点启动时间太近会导致难以选举出leader，因为大部分节点的term和index可能相同，大家都争着当候选人，但又无法拉开差距，进而不断发起选举。 时间滴答的时间间隔不能太小，如果太小会导致还没来得及响应投票选举出leader就发起了新一轮选举。 对于多节点启动，配置方式大概分为两种，一种是一开始就在成员配置表中写好所有节点，然后所有节点启动时都读取该配置，即静态配置；第二种方法是先启动一个节点，再启动其他节点，其他节点并没有成员配置表，而是走“新增节点”流程，即运行时配置。 新增节点 新增节点的过程大概如下： 新增节点向leader发送一个请求新增节点的指令，需要附带上自己的ID和RPC端口。 leader收到指令后通过网络库提取出该节点的host地址(IP地址)，和它发送过来的RPC端口号可以组成该节点程序的具体地址。 leader创建一个Conf Message，填入新节点的ID和地址，然后投入状态机。 当多数节点收到该Message后就会被提交，提交后所有成员将该节点加入成员记录中。 再启动 再次启动时需要读取本地存储中的快照，即term和commit index，以便于直接跳过已经提交的日志。 删除节点 删除节点过程则简单很多。向leader发起删除指定节点的提议，然后leader将该节点从记录中删除即可。 与对外服务沟通 一般集群服务都会运行一个对外服务程序与外部沟通，可以是TCP Server或HTTP Server。该服务要与Raft服务相互沟通，则可以使对外服务作为生产者，往管道中投入一个Proposal，使其进入Raft的轮询中。并构建一个反向的管道，在回调中向反向管道输入结果，而对外服务则接收结果。 读操作可以向任何节点请求，而写操作只能向Leader请求。一般，所有成员都运行同一份代码，收到提议的节点会通过ID标识并缓存提议和回调，虽然Entry提交后所有节点都会做出同样的响应操作，但只有具有该ID缓存的那个节点才会成功向客户端响应。 框架化 上面讲到的点很多都是通用的，不同的仅仅是业务逻辑。我们可以提炼出一个框架来处理。事实上，框架化是一个很个性化的事情，不同人的思路和实现方式都可能不同，下面仅提供一种思路。 首先，业务逻辑和消息队列交给用户自己维护，抽象为RaftService，而Raft的轮询和算法抽象为RaftServer。我们的目标是把RaftService传入RaftServer中即可完成Raft集群的一系列工作，同时我们也可以把RaftService传入对外服务中，起到沟通作用。 RaftService实现定义一系列接口，具体实现交给用户。我们可以提炼出的接口有： recv() -&gt; Message 从管道(或队列)获取一个Message。 send(Message) 向管道(或队列)投入一个Message。注意，该Message应该是框架定义的对象，而不是Raft里面的Message，因为可能还要包含其他操作。 update_context(Context) 更新上下文(Context)，上下文对象用于储存集群状态信息，该接口被框架定时调用并传入新的Context。 context() -&gt; Context 用户获取可以通过该接口获得集群状态信息。 propose(bytes, callback) -&gt; Something 向Raft发起一个提议，用户自行将需要提交的数据包装成Message，并放入管道中。callback是指提交之后的回调函数，由用户自行管理。一般有一个返回值，告诉上游是否提议成功，比如有些提议只能由leader发起。 propose_conf_change(ConfChange) -&gt; Something 用于发布配置操作。 on_committed(bytes) 提交后的处理，Raft集群提交一个Entry后调用。 其中提议传入bytes只是为了通用，如果你使用的语言有更强大的泛型系统，可以使用泛型取代。 实现 根据上面的理论，我用Rust实现了一个Raft服务框架，详见github。","link":"/2020/06/06/raft-build/"},{"title":"Transformer模型与自注意力","text":"Facebook激进地使用卷积网络处理NLP问题，意外地取得了很不错的效果。而 Google 一不做二不休，发布了一种新型的网络结构，transformer模型 [1]，该网络结构既不使用RNN，也不使用CNN，而且也获得不错的效果。 1. 模型结构 Encoder和Decoder都是从下往上的栈结构。 1.1 Encoder Encoder有6个相同的独立层，每层有2个子层。第一个子层是多头自注意力层（multi-head self-attention），第二个子层是前馈传播层。 1.2 Decoder Decoder同样有6个相同的独立层，每层有3个子层。第一个子层是遮罩了未来信息的多头自注意力层，第二个子层是联合encoder最后一层的输出的多头注意力层，第三个子层是前馈传播层。 下图是当只有1个独立层时候的网络结构 [1]： 1.3 注意力 注意力函数从 Query, Key, Value 映射到一个输出，这里的 Query, Key, Value 是什么可以先不管，先理解注意力模型的构造。 1.3.1 缩放的点积注意力 （Scaled Dot-Product Attention ） 假设 query, key, value 描述为矩阵 \\(\\large Q, K, V\\), query 和 key 的特征维度是 \\(\\large d_k\\)，而 value 的特征维度是 \\(\\large d_v\\)，那么缩放的点积注意力函数为， \\[ Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\] 1.3.2 多头注意力（Multi-Head Attention） 把 query, key, value 从一个大的特征维度映射出多个小特征维度，由 \\(\\large d_{model}\\) 映射到若干个 \\(\\large d_k\\) 或 \\(\\large d_v\\)，假设有 \\(\\large h\\) 个head，那么就有 \\(\\large d_k = d_v = d_{model}/h\\)，每一个 head 进行一次 Scaled Dot-Product Attention，最后再把所有head连接回 \\(\\large d_{model}\\) 维度。 注意力结构如下图： 那么 query, key, value 到底是什么呢？联系上面的模型结构图。 对于encoder来说，第一层输入的 query, key, value 三者皆是源句子的词向量，而后面的层的输入就是上一层的输出。 decoder的第一个子层和encoder相同，三者皆是目标句子的词向量，但是有一个Masked的标识，说明对于decoder来说，我们需要消除该词汇位置往后的参数的影响，消除方法是设置参数为负无穷。 decoder的第二个子层中，query 是第一个子层的输出，key 和 value 是encoder 最后一层的输出。 1.4 前馈传播网络 前馈传播网络包含两个线性层， 计算如下： \\[ FFN(x) = f_2(ReLU(f_1(x))) \\] 1.5 位置编码（Positional Encoding） 一个非RNN的模型无法很好的判断一个词汇在句子中的位置，解决办法是为词向量附加一个位置参数，使得模型能够学习和判断词汇的位置。论文中提出了一个使用余弦和正弦曲线的方法， \\[ PE(pos, 2i)=sin(pos/10000^{2i/d_{model}}) \\\\ PE(pos, 2i+1)=cos(pos/10000^{2i/d_{model}}) \\] 其中 \\(\\large pos\\) 为词在序列的位置，\\(\\large i\\) 为词向量的维度，最后的编码矩阵 \\(\\large PE \\in \\mathbb{R}^{T \\times E}\\)。 假设当前序列长 \\(\\large t\\) ，那么词向量为 \\(\\large emb + PE(t) \\rightarrow emb \\in \\mathbb{R}^{t \\times E}\\) 。 函数对于维度的数值是区分偶数和奇数的，以偶数为例，当位置为1, 5, 10时，位置向量的曲线如图所示， 可以发现，不同位置会进行不同的线性变换，模型会学习到不同位置的固有位置向量，当词向量附加了某一位置向量后，模型就能知道该词在句子中的所在位置。 2. 详细过程 前面大概讲了一下结构和概念，但实际实现模型时还是有很多细节需要弄懂。 源句子的最大序列长度为 \\(\\large T\\), 目标句子的最大序列长度为 \\(\\large T&#39;\\)，批次大小为 \\(\\large B\\)，词向量大小为 \\(\\large E\\)， 源语言词汇数量为 \\(\\large V\\)，目标语言词汇数量为 \\(\\large V&#39;\\)。 2.1 Encoder 计算整个句子的词向量为 \\(\\large dropout(s \\cdot emb(x) + emb\\_pos(x)) \\rightarrow X \\in \\mathbb{R}^{B \\times T \\times E}\\)，\\(\\large s\\) 为 \\(\\large \\sqrt{E}\\) 进入第一子层，记录残差 \\(\\large R \\leftarrow X\\)，标准化 \\(\\large X \\leftarrow Norm(X)\\) 把 \\(\\large X\\) 作为query, key, value，输入自注意力层 \\(\\large Atten(X, X, X) \\rightarrow X \\in \\mathbb{R}^{B \\times T \\times D}\\)，维度 \\(\\large D\\) 即 \\(\\large d_{model}\\) ，和词向量大小 \\(\\large E\\) 是一样的 dropout 正则化后加上残差 \\(\\large X \\leftarrow dropout(X) + R\\) 进入第二子层，记录残差 \\(\\large R \\leftarrow X\\)，标准化 \\(\\large X \\leftarrow Norm(X)\\) 输入到前馈传播网络，再加上残差 \\(\\large X \\leftarrow FFN(X) + R\\) 从步骤 (2) 到 (6) 就结束一个层了，多个层则做同样计算即可。 最终输出为 \\(\\large O \\in \\mathbb{R}^{B \\times T \\times E}\\) 2.2 Decoder Decoder 的第一个子层和第三个子层的步骤和encoder基本相同，这里列出第二子层的过程 记录残差 \\(\\large R \\leftarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times E}\\) [2]，标准化 \\(\\large X \\leftarrow Norm(X)\\) [3] 需要输入到另外一个自注意力层中 \\(\\large Atten(X, O, O) \\rightarrow X\\) dropout 正则化后加上残差 \\(\\large X \\leftarrow dropout(X) + R\\)，再记录残差 \\(\\large R \\leftarrow X\\) 输入到第三子层中 最后一层输出后，通过线性层映射出去 \\(\\large W^TX \\rightarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times V&#39;}\\) 计算softmax交叉熵，反向传播，更新梯度 2.3 Self-Attention \\(\\large d_k = d_v = D / h\\)，则query，key 和 value 分别对应的线性层为 \\(W_q \\in \\mathbb{R}^{D \\times (h\\cdot d)}\\)，\\(W_q \\in \\mathbb{R}^{D \\times (h\\cdot d)}\\) ，\\(W_q \\in \\mathbb{R}^{D \\times (h\\cdot d)}\\) 三个输入参数分别经过线性层后，并把维度转换为 \\(\\large Q, K, V \\in \\mathbb{R}^{B \\times h \\times T \\times d}\\) 矩阵乘法得到相似性得分矩阵 \\(\\large (s \\cdot Q) * K \\rightarrow A \\in \\mathbb{R}^{B \\times h \\times T&#39; \\times T}\\)，其中 \\(\\large s=1/\\sqrt{d}\\) 遮罩计算，\\(\\large A \\leftarrow masked(A)\\)。对于encoder的第一个子层和decoder的第二个子层，把源句子中的边缘位置得分设为负无穷，对于decoder的第一个子层，我们还需要把每一个位置的后面位置的得分设为负无穷。 计算softmax得到归一化的相似性矩阵，\\(\\large A \\leftarrow softmax(A)\\)，正则化 \\(\\large A \\leftarrow dropout(A)\\) 通过矩阵乘法与 value 加权求和，求得注意力得分 \\(\\large A * V \\rightarrow X \\in \\mathbb{R}^{B \\times h \\times T \\times d}\\) 连接 \\(\\large h\\) 个head \\(\\large X \\in \\mathbb{R}^{B \\times T \\times D}\\) 经过一个线性层后输出 \\(\\large W^TX \\rightarrow X \\in \\mathbb{R}^{B \\times T \\times D}\\) 前面提到我们在计算decoder第一子层的注意力时把每一个位置的后面位置的得分设为负无穷，目的是防止未来无用信息的干扰。我们构造该句子的方法是在去边缘的遮罩矩阵基础上，取该矩阵的上三角矩阵，即把相似性矩阵的边缘位置和上三角位置都设为负无穷。 2.4 Feed-Forward Network 构造两个线性层，\\(\\large W_1 \\in \\mathbb{R}^{D \\times F}\\) 和 \\(\\large W_2 \\in \\mathbb{R}^{F \\times D}\\) 。 记录残差 \\(\\large R \\leftarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times D}\\)，标准化 \\(\\large X \\leftarrow Norm(X)\\) 通过第一个线性层并使用ReLU激活函数 \\(\\large ReLu(W_1^TX) \\rightarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times F}\\) 正则化后在通过第二个线性层 \\(\\large W_2^T(dropout(X)) \\rightarrow X \\in \\mathbb{R}^{B \\times T&#39; \\times D}\\) 加上残差后输出 \\(\\large X \\leftarrow X + R\\) 上述过程与论文不同的是，我们把标准化 Layer Normalization 移到注意力层的前面。这么做的原因在于当我不做改动时，模型预测结果很糟糕，结果全是 padding 或 eos，个人感觉是因为残差连接后再标准化会把残差丢失，详细原因也不是很肯定。而tensor2tensor中的建议也是放在前面的，既然如此，我就放在前面了。 2.5 Optimizer 基于Adam，设 \\(\\large \\beta_1 =0.9\\)，\\(\\large \\beta_2=0.98\\) 和 \\(\\large \\epsilon = 10^{-9}\\) ，热更新步为 \\(\\large w=4000\\)，在第 \\(\\large s\\) 步时，则学习率更新规则为， \\[ lr = D^{-0.5} \\cdot min(s^{-0.5}, s \\cdot w^{-1.5}) \\] 学习率的变化如图所示， 4. 参数细节 词向量大小取 512 head的数量为 8 encoder和decoder都是 6 层 前馈传播网络的维度 \\(\\large F\\) 为 2048 dropout 为 0.1 5. 结果 从论文中公示的结果来看，Transformer 模型得到了相当不错的结果，略胜 ConvS2S 一筹。 6. 自注意力的有效性 自注意力（self-Attention）主要特点是解决了远程依赖问题（long-range dependencies）。信号传递的距离越远，信号就变得越弱，远程依赖就越弱。RNN需要通过时间步的计算传递，信号传递长度是 \\(\\large \\mathcal{O}(n)\\)，而自注意力是把整个序列的信号进行前一层和后一层的直接计算，所以只要 \\(\\large \\mathcal{O}(1)\\)。 参考文献 [1] Attention Is All You Need. 2017 [2] Deep Residual Learning for Image Recognition. 2015 [3] Layer Normalization. 2016","link":"/2019/02/27/transformer/"},{"title":"负采样的Skip-Gram模型","text":"Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定 Input Word 来预测上下文。而CBOW是给定上下文来预测。本篇文章仅讲解Skip-Gram模型。 数据获取 首先，每次都需要从语料库中抽样一个批次的数据，batch size 为 \\(\\large n\\) 我采用窗口移动的方式获取，中心词（target）为 \\(\\large w_c\\)，窗口（Skip-window）为 \\(\\large m\\)，以中心词为中心，两扇窗口分别涵盖 \\(\\large m\\) 个上下文词汇，则以中心词为中心的取词跨度（span）是 \\(\\large 2 * m + 1\\)。 \\[ w_{c-m}, \\,.. \\, ,w_{c-1}, w_{c}, w_{c+1}, \\, .. \\,, w_{c+m} \\] 有一个参数是取词数量（num-skips），表示每次移动窗口取多少个上下文词汇。以上面为例，假设num-skips为2，那么可能取到的数据是 \\(\\large w_c \\rightarrow w_{c+m},\\; w_c \\rightarrow w_{c+1}\\)，没有规定分别取前后多少词，每次在跨度内除中心词外随机取。 中心词每向后移动一次，则重复上面的操作，直到取满批次大小，最少移动 \\(\\large n/\\text{num-skips}\\) 次。 算法 假设批次大小是 \\(\\large n\\)，Skip-gram的目标函数为： \\[ \\mathcal{L}_{SG}=-\\frac{1}{n}\\sum_{i=1}^n\\sum_{|j|\\le c} log\\;p(w_{i+j}|w_i) \\] 概率函数 \\(\\large p\\) 是我们重点讨论的对象。 我们使用两个词向量去拟合，假设词向量的的维度是 \\(\\large E\\)，词汇表有 \\(\\large V\\) 个单词。 \\(\\large w_i\\): 词汇表第 \\(\\large i\\) 个单词 \\(\\large V \\in \\mathbb{R}^{V \\times E}\\): 输入词向量 \\(\\large v_i\\): \\(\\large V\\)的第 \\(\\large i\\) 行，单词 \\(\\large w_i\\) 的输入向量表示 \\(\\large U \\in \\mathbb{R}^{V \\times E}\\): 输出词词向量 \\(\\large u_i\\): \\(\\large U\\)的的第 \\(\\large i\\) 行，单词 \\(\\large w_i\\) 的输出向量表示 中心词的词向量由输入词向量获得，上下文词的词向量由输出词向量获得。中心词 \\(\\large c\\) 与之对应的上下文词汇 \\(\\large w\\) 的词向量的乘积再求和就该中心词的得分 \\(\\large z_c^w = \\sum^E_i u_w^i v_c^i\\)。 以前会使用softmax函数，但总所周知，softmax函数的计算代价是昂贵的。所以论文作者提出了负采样（ negative samples），称为负采样Skip-gram（SGNS）。 假设单词在上下文中，我们称为正样本 \\(\\large w \\in D^+\\)，若单词不在上下文中，我们称为负样本 \\(\\large \\hat w \\in D^-\\)。每计算一个正样本会携带计算 \\(\\large K\\) 个负样本, 这个 \\(\\large K\\) 不会很大，一般5~20个，数据集越大，这个值越小。 我们要最大化，下面这个函数 \\[ \\begin{aligned} &amp; z_c^w = \\sum^E_i u_w^i v_c^i \\\\ &amp; \\hat z_c^{\\hat w} = \\sum^E_i u_{\\hat w}^i v_c^i \\\\ &amp; \\max_{U,V}\\;log\\;\\sigma(z_c^w)+\\sum_{(\\hat w_k,c)\\in D^-}^K log\\;\\sigma(-\\hat z_c^{\\hat w_k}) \\end{aligned} \\] 即尽量使得正采样的得分变大，而负采样的得分变小。 那么我们可以得到目标函数为， \\[ J = -\\frac{1}{n}\\sum_{i=1}^n \\Big(log\\;\\sigma(z_{c_i}^{w_i})+\\sum_{(\\hat w_k,c_i)\\in D^-}^K log\\;\\sigma(\\hat z_{c_i}^{\\hat w_k})\\Big) \\] 然后使用随机梯度下降最小化目标函数即可。Kaji, et al. (2017) 建议最好使用原生的随机梯度下降，学习率 \\(\\large \\alpha\\) 为1.0，不需要使用Adam或AdaGrad。实验发现，使用Adam或AdaGrad等得到的效果不如SGD，而且还会降低计算效率。 一般的，我们会以输入向量 \\(\\large V\\) 作为最终的词向量。 噪音分布（noise distribution） 负样本的选取并不是完全随机的，其中定义了一个噪声分布去选取负样本 [1]。 我们定义一元组分布（unigram distribution）为单个单词数量与总单词数量的比值的概率分布，即 \\[ U(w_i) = \\frac{count(w_i)}{\\sum_i count(w_i)} \\] 而噪声分布定义为 \\[ P_n(w_i) = \\frac{U(w_i)^{\\frac{3}{4}}}{Z} \\] 其中 \\(\\large Z\\) 是一个扩大系数，一般为 \\(\\text{0.001}\\)。 噪音分布的用意是使频繁出现的词更容易被选为负样本，而不频繁出现的词不作为负样本。为什么呢？比如说，类似于\\(\\text{is, a, the}\\) 这样的高频词，他们没有实际意义，而且不是我们所关心的，所以应该更多的把他们作为负样本。 为什么要选择\\(\\large \\frac{3}{4}\\) 次幂呢？在google输入plot y = x^(3/4) and y = x，可以看到曲线的变化趋势 其中蓝色曲线是\\(\\large \\frac{3}{4}\\) 次幂的，越接近1，增量就越小，目的是适当减少超高频词数量。 \\(\\large P_n(w_i)\\) 在实际使用中就是词汇 \\(\\large w_i\\) 在总负样本池中出现的次数。 代码实现 我用pytorch实现了SGNS算法，详见Github 测试 经过测试使用了噪音分布后，负样本池的大小减少了非常多，而且 spearmans_rho 得分也提高了一点。 eval-data use-noise non-noise EN-MC-30 0.2513 0.2405 EN-MTurk-287 0.2578 0.2357 （词向量300d，训练20万步） 参考文献 [1] Distributed Representations of Words and Phrases and their Compositionality. 2013 [2] Incremental Skip-gram Model with Negative Sampling. 2017 [3] negative sampling tutorial. 2017","link":"/2018/10/08/skip-gram/"},{"title":"AVL Tree Implementation","text":"AVL tree is a self-balancing binary search tree. In an AVL tree, the heights of the two child subtrees of any node differ by at most one; if at any time they differ by more than one, rebalancing is done to restore this property. I will try to implement it and log in this article. You can find the source code in here. Node Evey Node have five member properties. balFactor_ is the balance factor that equal to -1, 0, 1 representing the left sub-tree 1 layer taller than right sub-tree, tall equally and converse, respectively. 123456789template &lt;typename T&gt;struct AVLNode &#123; typedef AVLNode&lt;T&gt;* NodePtr; T value_; NodePtr parent_; NodePtr left_; NodePtr right_; signed char balFactor_; Iterator function increment() find the last node that is larger than current node, corresponding operator++. function decrement() find the last node that is smaller than current node, corresponding operator--. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465template &lt;typename T&gt;struct AVLTreeIterator &#123; // ... typedef AVLTreeIterator&lt;T&gt; Self; typedef AVLNode&lt;T&gt;* NodePtr; NodePtr node_; explicit AVLTreeIterator(NodePtr x): node_(x) &#123;&#125; reference operator*() const &#123; return node_-&gt;value_; &#125; pointer operator-&gt;() const &#123; return &amp;(operator*()); &#125; void increment() &#123; if(node_-&gt;right_ != 0) &#123; node_ = node_-&gt;right_; while(node_-&gt;left_ != 0) &#123; node_ = node_-&gt;left_; &#125; &#125; else &#123; NodePtr y = node_-&gt;parent_; while (node_ == y-&gt;right_) &#123; node_ = y; y = y-&gt;parent_; &#125; if(node_-&gt;right_ != y) &#123; node_ = y; &#125; &#125; &#125; void decrement() &#123; if(node_-&gt;parent_-&gt;parent_ == node_ &amp;&amp; node_-&gt;balFactor_ == -2) &#123; // end() node node_ = node_-&gt;right_; &#125; else if (node_-&gt;left_ != 0) &#123; NodePtr y = node_-&gt;left_; while (y-&gt;right_ != 0)&#123; y = y-&gt;right_; &#125; node_ = y; &#125; else &#123; NodePtr y = node_-&gt;parent_; while (node_ == y-&gt;left_) &#123; node_ = y; y = y-&gt;parent_; &#125; node_ = y; &#125; &#125; Self&amp; operator++() &#123; increment(); return *this; &#125; Self&amp; operator--() &#123; decrement(); return *this; &#125; // ... Tree Property header Let us see a tree without value node, and the only node called header. avl-1.jpg parent: link to root if exist left: also call leftmost, link to the most left node of tree right: also call leftmost, link to the most right node of tree balFactor: always equal to -2 (special) root root is the top node of a tree. avl-2.jpg parent: link to header left: link to nullptr if no node right: link to nullptr if no node 1NodePtr&amp; root() const &#123; return header_-&gt;parent_; &#125; Multi Node See a tree with three nodes. The children of leaf are nullptr. avl-3.jpg Balance AVL Tree (Adelson-Velskii-Landis tree) is a kind of balanced binary search tree with the demand that the difference heigth of the left and right tree cannot over 1. avl-4.jpg the sub-tree or tree with a root node whose balance factor = -2 or = 2 is an unbalance tree. We can rebalance it by rotation. Rotation Left Rotation When the right sub-tree is taller, we need to rotate left to shorten it. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** / /* x y* / \\ / \\* a y =&gt; x c* / \\ / \\* [b] c a [b]*/void _rotateLeft(NodePtr x, NodePtr &amp;root) &#123; NodePtr y = x-&gt;right_; // change the child x-&gt;right_ = y-&gt;left_; y-&gt;left_ = x; // change the parent y-&gt;parent_ = x-&gt;parent_; x-&gt;parent_ = y; if(x-&gt;right_ != nullptr) &#123; // b x-&gt;right_-&gt;parent_ = x; &#125; if(x == root) &#123; root = y; &#125; else if(y-&gt;parent_-&gt;right_ == x) &#123; // x is the right son of his parent y-&gt;parent_-&gt;right_ = y; &#125; else &#123; y-&gt;parent_-&gt;left_ = y; &#125; if(y-&gt;balFactor_ == 1) &#123; /* * / / * x y * / \\ / \\ * a y =&gt; x c * / \\ / \\ / * b c a b d * / * d */ y-&gt;balFactor_ = 0; x-&gt;balFactor_ = 0; &#125; else &#123; /* * / / * x y * / \\ / \\ * a y =&gt; x c * / \\ / \\ * b c a b * \\ \\ * d d */ y-&gt;balFactor_ = -1; x-&gt;balFactor_ = 1; &#125;&#125; Right Rotation When the left sub-tree is taller, we need to rotate right to shorten it. 123456789101112131415161718192021222324252627282930313233343536/** / /* x y* / \\ =&gt; / \\* y a c x* / \\ / \\* c [b] [b] a*/void _rotateRight(NodePtr x, NodePtr &amp;root) &#123; NodePtr y = x-&gt;left_; x-&gt;left_ = y-&gt;right_; y-&gt;right_ = x; y-&gt;parent_ = x-&gt;parent_; x-&gt;parent_ = y; if(x-&gt;left_ != nullptr) &#123; // b x-&gt;left_-&gt;parent_ = x; &#125; if(x == root) &#123; root = y; &#125; else if(y-&gt;parent_-&gt;right_ == x) &#123; y-&gt;parent_-&gt;right_ = y; &#125; else &#123; y-&gt;parent_-&gt;left_ = y; &#125; if(y-&gt;balFactor_ == -1) &#123; y-&gt;balFactor_ = 0; x-&gt;balFactor_ = 0; &#125; else &#123; y-&gt;balFactor_ = 1; x-&gt;balFactor_ = -1; &#125;&#125; Left Right Rotation As for Double rotation, we need to look four layers of tree. When the top node of tree or sub-tree has 2 layers taller left sub-tree, and the left sub-tree have a taller right sub-tree, we need to do a left rotation to make the latter right sub-tree shorter firstly and then do a right rotation to make the former left sub-tree shorter shorter secondly 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** / /* a c* / \\ / \\* b g =&gt; / \\* / \\ b a* d c / \\ / \\* / \\ d e f g* e f*/void _rotateLeftRight(NodePtr a, NodePtr &amp;root) &#123; NodePtr b = a-&gt;left_; NodePtr c = b-&gt;right_; // a and b link to c' sons a-&gt;left_ = c-&gt;right_; b-&gt;right_ = c-&gt;left_; // c becomes the parent of a and b c-&gt;right_ = a; c-&gt;left_ = b; // c links to the parent of a and b c-&gt;parent_ = a-&gt;parent_; a-&gt;parent_ = b-&gt;parent_ = c; // check c' sons and link to their new parent if(a-&gt;left_) &#123; // f a-&gt;left_-&gt;parent_ = a; &#125; if(b-&gt;right_) &#123; // e b-&gt;right_-&gt;parent_ = b; &#125; // the parent of a and b link to c if(a == root) &#123; root = c; &#125; else if(c-&gt;parent_-&gt;left_ == a) &#123; c-&gt;parent_-&gt;left_ = c; &#125; else &#123; c-&gt;parent_-&gt;right_ = c; &#125; switch (c-&gt;balFactor_) &#123; case -1: /* * c * / * e */ a-&gt;balFactor_ = 1; b-&gt;balFactor_ = 0; break; case 0: a-&gt;balFactor_ = 0; b-&gt;balFactor_ = 0; break; case 1: /* * c * \\ * f */ a-&gt;balFactor_ = 0; b-&gt;balFactor_ = -1; break; default: assert(false); &#125; c-&gt;balFactor_ = 0;&#125; Right Left Rotation As for Double rotation, we need to look four layers of tree. When the top node of tree or sub-tree has 2 layers taller right sub-tree, and the right sub-tree have a taller left sub-tree, we need to do a right rotation to make the latter left sub-tree shorter firstly and then do a left rotation to make the former right sub-tree shorter shorter secondly. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** / /* a c* / \\ / \\* d b =&gt; / \\* / \\ a b* c g / \\ / \\* / \\ d e f g* e f*/void _rotateRightLeft(NodePtr a, NodePtr &amp;root) &#123; NodePtr b = a-&gt;right_; NodePtr c = b-&gt;left_; a-&gt;right_ = c-&gt;left_; b-&gt;left_ = c-&gt;right_; c-&gt;left_ = a; c-&gt;right_ = b; c-&gt;parent_ = a-&gt;parent_; a-&gt;parent_ = b-&gt;parent_ = c; if(a-&gt;right_) &#123; a-&gt;right_-&gt;parent_ = a; &#125; if(b-&gt;left_) &#123; b-&gt;left_-&gt;parent_ = b; &#125; if(a == root) &#123; root = c; &#125; else if(c-&gt;parent_-&gt;left_ == a) &#123; c-&gt;parent_-&gt;left_ = c; &#125; else &#123; c-&gt;parent_-&gt;right_ = c; &#125; switch(c-&gt;balFactor_) &#123; case -1: /* * c * / * e */ a-&gt;balFactor_ = 0; b-&gt;balFactor_ = 1; break; case 0: a-&gt;balFactor_ = 0; b-&gt;balFactor_ = 0; break; case 1: /* * c * \\ * f */ a-&gt;balFactor_ = -1; b-&gt;balFactor_ = 0; break; default: assert(false); &#125; c-&gt;balFactor_ = 0;&#125; Insertion When we insert a node to tree, we consider three section: Ensuring insert to the child of the leaf Considering the parent of new node is header, means that the new node is root, and that we need to add the relation between header and root, including the parent of them each other and the leftmost or rightmost. Rebalance. We consider the parent of x at the start, and percolate up the path from leaf to root. We at least do zero or once rotation for certein. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// [x] is the new node to insert// [p] is the parent of [x]void _insertAndRebalance(bool insertLeft, NodePtr x, NodePtr p) &#123; // construct the new node to insert x-&gt;parent_ = p; x-&gt;left_ = nullptr; x-&gt;right_ = nullptr; x-&gt;balFactor_ = 0; if(insertLeft) &#123; p-&gt;left_ = x; if(p == header_) &#123; // consider that p is header header_-&gt;parent_ = x; header_-&gt;right_ = x; &#125; else if(p == header_-&gt;left_) &#123; // consider that p is son of header header_-&gt;left_ = x; &#125; &#125; else &#123; p-&gt;right_ = x; if(p == header_-&gt;right_) &#123; header_-&gt;right_ = x; &#125; &#125; // rebalance while(x != root()) &#123; switch (x-&gt;parent_-&gt;balFactor_) &#123; case 0: // same tall right and left sub tree of x parent. // One of them will become taller after insert a node. x-&gt;parent_-&gt;balFactor_ = (x == x-&gt;parent_-&gt;left_) ? -1 : 1; x = x-&gt;parent_; // percolate up the path break; case 1: // right sub-tree is taller if(x == x-&gt;parent_-&gt;left_) &#123; // If inserted node in the left, become same tall. x-&gt;parent_-&gt;balFactor_ = 0; &#125; else &#123; // If inserted node in the right, become more taller probably. // For shortening it, we need to rotate it. if(x-&gt;balFactor_ == -1) &#123; // x have a taller left, do a right left rotation to shorten the left _rotateRightLeft(x-&gt;parent_, root()); &#125; else &#123; // just do a left rotation to shorten the right. _rotateLeft(x-&gt;parent_, root()); &#125; &#125; // adjust once is enough return; case -1: // -1 mean that x parent have a taller sub-left tree if(x == x-&gt;parent_-&gt;left_) &#123; // but insert a new node in sub-left tree // leading to become more taller (probably) // For shortening it, we need to rotate it. if(x-&gt;balFactor_ == 1) &#123; // If x have a taller sub-right tree, // doing a left right rotation can shorten the right. _rotateLeftRight(x-&gt;parent_, root()); &#125; else &#123; // Else, just do a right rotation to shorten the left. _rotateRight(x-&gt;parent_, root()); &#125; &#125; else &#123; x-&gt;parent_-&gt;balFactor_ = 0; &#125; return; default: assert(false); &#125; &#125;&#125; We insert a not-allow-replaced key with three considerations: insert to the left of leftmost insert to the right of y if just bigger than y insert to the left of y if just bigger than decrement(y) 123456789101112131415161718192021222324252627282930313233343536pair&lt;iterator, bool&gt; insertUnique(const Val&amp; v) &#123; NodePtr x = root(); NodePtr y = header_; bool comp = true; while(x != nullptr) &#123; y = x; comp = keyComp_(KeyOfValue()(v), key(x)); x = comp ? left(x) : right(x); &#125; iterator j = iterator(y); if(comp) &#123; // left if(j == begin()) &#123; // leftmost return pair&lt;iterator, bool&gt;(_insert(x, y, v), true); &#125; else &#123; --j; &#125; &#125; if(keyComp_(key(j.node_), KeyOfValue()(v))) &#123; return pair&lt;iterator, bool&gt;(_insert(x, y, v), true); &#125; return pair&lt;iterator, bool&gt;(j, false);&#125;// [x] is the child of [p],// always is nullptr passed from insertUnique() and insertEqual(),// do not need to consider, in theory.// [p] is the parent of [x]iterator _insert(NodePtr x, NodePtr p, const Val&amp; v) &#123; bool insertLeft = (x != nullptr || p == header_ || keyComp_(KeyOfValue()(v), key(p))); NodePtr z = createNode(v); _insertAndRebalance(insertLeft, z, p); ++nodeCount_; return iterator(z);&#125; Insertion of allowing equal key. 123456789iterator insertEqual(const Val&amp; v) &#123; NodePtr x = root(); NodePtr y = header_; while(x != nullptr) &#123; y = x; x = keyComp_(KeyOfValue()(v), key(x)) ? left(x) : right(x); &#125; return _insert(x, y, v);&#125; Deletion When we delete a node from a tree, we consider three sections: Find the successor to replace the position of deleted node. If the node to be deleting has only one child, the successor is the child. If it has two children, the successor is the last node that is larger than it. The replacement action including three steps: Disconnecting to its children and make the successor links to them Suturing the wound caused by successor moved Its parent link to the successor Rebalance. We will percolate up and rotate multi times probably until the balFactor of certain node is 0, we just change its balFactor to -1 or 1 (depending the deleted node in right or left). the balFactor of certain node after rotation becomes from 1 to -1 or from -1 to 1. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132// erase node [z] and return the deleted nodeNodePtr _eraseAndRebalance(NodePtr z) &#123; NodePtr y = z; NodePtr x = nullptr; NodePtr xParent = nullptr; // x is the child of y if(y-&gt;left_ == nullptr) &#123; x = y-&gt;right_; &#125; else if(y-&gt;right_ == nullptr) &#123; x = y-&gt;left_; &#125; else &#123; // y has two children // find the successor y = y-&gt;right_; while(y-&gt;left_) &#123; y = y-&gt;left_; &#125; x = y-&gt;right_; //y has no left child, so x is the successor &#125; if(y != z) &#123; // mean that z has two children // we make the successor y to replace z // z will disconnect to its children and y link to them z-&gt;left_-&gt;parent_ = y; y-&gt;left_ = z-&gt;left_; if(y != z-&gt;right_) &#123; // y will be moved, need to link its child to its parent xParent = y-&gt;parent_; if(x) &#123; x-&gt;parent_ = y-&gt;parent_; &#125; y-&gt;parent_-&gt;left_ = x; y-&gt;right_ = z-&gt;right_; z-&gt;right_-&gt;parent_ = y; &#125; else &#123; // y == z-&gt;right, mean y has no left child and x is the right of y xParent = y; &#125; // link z's parent to y if(root() == z) &#123; root() = y; &#125; else if(z-&gt;parent_-&gt;left_ == z) &#123; z-&gt;parent_-&gt;left_ = y; &#125; else &#123; z-&gt;parent_-&gt;right_ = y; &#125; y-&gt;parent_ = z-&gt;parent_; y-&gt;balFactor_ = z-&gt;balFactor_; &#125; else &#123; // mean that z has one child or none and y == z // x become the successor xParent = y-&gt;parent_; if(x) &#123; x-&gt;parent_ = y-&gt;parent_; &#125; if(root() == z) &#123; root() = x; &#125; else if (z-&gt;parent_-&gt;left_ == z) &#123; z-&gt;parent_-&gt;left_ = x; &#125; else &#123; z-&gt;parent_-&gt;right_ = x; &#125; if(leftmost() == z) &#123; if(z-&gt;right_ == nullptr) &#123; leftmost() = z-&gt;parent_; &#125; else &#123; leftmost() = Node::minimum(x); &#125; &#125; if(rightmost() == z) &#123; if(z-&gt;left_ == nullptr) &#123; rightmost() = z-&gt;parent_; &#125; else &#123; rightmost() = Node::maximum(x); &#125; &#125; &#125; // rebalance while (x != root()) &#123; switch (xParent-&gt;balFactor_) &#123; case 0: xParent-&gt;balFactor_ = (x == xParent-&gt;right_) ? -1 : 1; return z; case -1: // left sub-tree taller if(x == xParent-&gt;left_) &#123; // erase the node in the left, balance just right xParent-&gt;balFactor_ = 0; x = xParent; xParent = xParent-&gt;parent_; &#125; else &#123; // erase the node in the right, need to shorten the left NodePtr a = xParent-&gt;left_; if(a-&gt;balFactor_ == 1) &#123; // check whether need a double rotation _rotateLeftRight(a, root()); &#125; else &#123; _rotateRight(xParent, root()); &#125; // percolate up x = xParent-&gt;parent_; xParent = xParent-&gt;parent_-&gt;parent_; if(x-&gt;balFactor_ == 1) &#123; return z; &#125; &#125; break; case 1: // right sub-tree taller if(x == xParent-&gt;right_) &#123; xParent-&gt;balFactor_ = 0; x = xParent; xParent = xParent-&gt;parent_; &#125; else &#123; NodePtr a = xParent-&gt;right_; if(a-&gt;balFactor_ == -1) &#123; _rotateRightLeft(a, root()); &#125; else &#123; _rotateLeft(xParent, root()); &#125; x = xParent-&gt;parent_; xParent = xParent-&gt;parent_-&gt;parent_; if(x-&gt;balFactor_ == -1) &#123; return z; &#125; &#125; break; default: assert(false); &#125; &#125; return z;&#125; Nothing additional thing If need to delete a known node. 12345void erase(iterator pos) &#123; NodePtr y = _eraseAndRebalance(pos.node_); destroyNode(y); --nodeCount_;&#125; Deleting a key that probably exists in multi node, we need to find the range and delete them one by one. lowerBound() can find the last node that greater than or equal to a certein key. And upperBound() find the last node that greater than a certein key. So the deleted range is [lowerBound(), upperBound()). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748size_type erase(const Key&amp; k) &#123; pair&lt;iterator, iterator&gt; p = equalRange(k); size_type old = size(); erase(p.first, p.second); return old - size();&#125;void erase(iterator first, iterator last) &#123; if(first == begin() &amp;&amp; last == end()) &#123; clear(); &#125; else &#123; while (first != last) &#123; erase(first++); &#125; &#125;&#125;pair&lt;iterator, iterator&gt; equalRange(const Key&amp; k) const &#123; return pair&lt;iterator, iterator&gt;(lowerBound(k), upperBound(k));&#125;iterator lowerBound(const Key&amp; k) const &#123; NodePtr y = header_; NodePtr x = root(); while(x != nullptr) &#123; if(!keyComp_(key(x), k)) &#123; y = x; x = left(x); &#125; else &#123; x = right(x); &#125; &#125; return iterator(y);&#125;iterator upperBound(const Key&amp; k) const &#123; NodePtr y = header_; NodePtr x = root(); while(x != nullptr) &#123; if(keyComp_(k, key(x))) &#123; y = x; x = left(x); &#125; else &#123; x = right(x); &#125; &#125; return iterator(y);&#125; Find Using lowerBound, we can find the last node that greater than or equal to a certein key, if no, return the header node. And we only need the node with the same key. 1234567891011121314151617181920iterator lowerBound(const Key&amp; k) const &#123; NodePtr y = header_; NodePtr x = root(); while(x != nullptr) &#123; if(!keyComp_(key(x), k)) &#123; y = x; x = left(x); &#125; else &#123; x = right(x); &#125; &#125; return iterator(y);&#125;iterator find(const Key&amp; k) &#123; iterator j = lowerBound(k); // 1. j will be end() if cannot find // 2. key(j) probably less than k return (j == end() || keyComp_(k, key(j.node_))) ? end() : j;&#125;","link":"/2020/08/15/avl-tree/"}],"tags":[{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"强化学习","slug":"强化学习","link":"/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"漫评","slug":"漫评","link":"/tags/%E6%BC%AB%E8%AF%84/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"I/O","slug":"I-O","link":"/tags/I-O/"},{"name":"算法与设计","slug":"算法与设计","link":"/tags/%E7%AE%97%E6%B3%95%E4%B8%8E%E8%AE%BE%E8%AE%A1/"}],"categories":[{"name":"操作系统","slug":"操作系统","link":"/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"强化学习","slug":"强化学习","link":"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"漫评","slug":"漫评","link":"/categories/%E6%BC%AB%E8%AF%84/"},{"name":"算法与设计","slug":"算法与设计","link":"/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E8%AE%BE%E8%AE%A1/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"消息队列","slug":"消息队列","link":"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]}